[
{
	"uri": "http://localhost:1313/db-course/transacties/acid/",
	"title": "ACID",
	"tags": [],
	"description": "",
	"content": "ACID is een acronym dat we gebruiken binnen databases dat een lijst van voorwaarden omschrijft waar dat database systeem aan moet voldoen. De regels van ACID worden over het algemeen geïmplementeerd door het concept van Transacties. ACID omschrijft vier principes:\nAtomicity Consistency Isolation Durability Atomicity Transacties bestaan vaak uit meerdere statements. Atomicity verwacht dus dat al deze statements als één geheel worden beschouwd. Ofwel faalt alles, ofwel slaagt alles. Zo wordt er nooit slechts een deel van de changes bewaard.\nHet gevolg hiervan is dat de staat van een transactie niet gezien kan worden door andere gebruikers. Stel we hebben een typische bankoverschrijving die €10 gaat overschrijven van de rekening van Alice naar de rekening van Bob.\nsequenceDiagram\rNote over Application,DB: BEGIN TRANSACTION\rDB--\u003eDB: Rekening van Alice: €100\rDB--\u003eDB: Rekening van Bob: €150\rApplication-\u003e\u003eDB: UPDATE Account SET Amount -= 10 WHERE Owner = 'Alice'\rDB--\u003eDB: Rekening van Alice: €100\rDB--\u003eDB: Rekening van Bob: €150\rApplication-\u003e\u003eDB: UPDATE Account SET Amount += 10 WHERE Owner = 'Bob'\rNote over Application,DB: COMMIT TRANSACTION\rDB--\u003eDB: Rekening van Alice: €90\rDB--\u003eDB: Rekening van Bob: €160\rConsistency Consistency zorgt ervoor dat een database altijd in een consistente staat moet blijven. Met andere woorden, er moet altijd voldaan worden aan de constraints die op de database gedefinieerd zijn. Dit kan een referentiële constraint zijn, als er een rij wordt verwijderd die nog gebruikt wordt in een andere tabel waar onderliggend een referentiële constraint op ligt.\nNeem bovenstaand voorbeeld waarbij Alice geld overschrijft naar de rekening van Bob. Maar nu wil ze €150 overschrijven. We hebben op de Account tabel een CHECK CONSTRAINT gedefinieerd dat de Amount altijd groter of gelijk aan 0 moet zijn. Wat gebeurt er dan?\nsequenceDiagram\rNote over Application,DB: BEGIN TRANSACTION\rDB--\u003eDB: Rekening van Alice: €100\rDB--\u003eDB: Rekening van Bob: €150\rApplication-\u003e\u003eDB: UPDATE Account SET Amount -= 150 WHERE Owner = 'Alice'\rrect rgb(194, 24, 7)\rDB--\u003eDB: Error: Amount should be \u003e= 0\rend\rNote over Application,DB: ROLLBACK TRANSACTION\rDB--\u003eDB: Rekening van Alice: €100\rDB--\u003eDB: Rekening van Bob: €150\rDe transactie wordt teruggedraaid. Er werd geen geld afgehaald van de rekening van Alice en Bob heeft ook geen geld gekregen. We zitten opnieuw in een geldige consistente staat.\nIsolation Als je een query of transactie uitvoert op een database, ga je zelden als enige gebruiker actief zijn. Er zullen wellicht andere transacties uitgevoerd worden terwijl jij de jouwe uitvoert. Om ervoor te zorgen dat deze geen invloed hebben op elkaar worden er locks gelegd op een set van data. Daar bovenop worden isolation levels gedefinieerd die bepalen wat andere gebruikers mogen zien en lezen van andere transacties.\nDit komt meer detail aan bod in een volgend hoofdstuk.\nDurability Als we een transactie afronden en hij wordt gecommit dan is die nog steeds committed ook in het geval van een crash of stroomonderbreking. Veel DBMS providers lossen dit op door data weg te schrijven in non-volatile memory en door gebruik te maken van een transaction log in de write-ahead logging strategy.\nWrite-ahead logging is een strategie waarbij alle wijzigingen die op de database worden uitgevoerd, eerst worden vastgelegd in een transaction log voordat ze daadwerkelijk worden toegepast op de database. Dit logboek bevat een gedetailleerde lijst van alle acties die zijn uitgevoerd, zoals INSERT, UPDATE, en DELETE statements.\n"
},
{
	"uri": "http://localhost:1313/db-course/apis/basics/",
	"title": "API Basics",
	"tags": [],
	"description": "",
	"content": "Layered Application Tiers In software engineering worden applicaties logisch opgesplitst in verschillende \u0026ldquo;tiers\u0026rdquo;. Een typische 3-Tier webapplicatie bestaat uit 3 lagen: de laag die de gebruiker te zien krijgt\u0026mdash;de UI, bestaande uit HTML en CSS, de backend\u0026mdash;een server waar de requests naartoe worden gestuurd en die de aanvragen verwerkt, en een data laag die onze database voorstelt. Onderstaand schema vat dit samen (via Trevor N. Mudge):\nIn de praktijk varieert deze tier benadering van project tot project.\nTot nu toe in dit vak hebben we ons toegelegd op Tier 3: de data laag. Zonder frontend applicatie laag kan een gebruiker echter niet interageren met deze database; er is dus minstens één extra tier nodig.\nOm ons in het vak databases te kunnen focussen op de data en de integratie van de data met de software gaan wij ons toeleggen op Tier 2 + 3. Het web gedeelte valt weg en proberen zoveel mogelijk gebruik te maken van een commandline interface om te concepten uit te werken. In het vak Full Stack Web development duiken gaan we meer in op een mooie interface GUI programmeren. Een simpele 2-tier applicatie is ook wel een client-server applicatie genoemd. In ons geval is de server de database, die in principe op een andere machine kan gedeployed worden. Voor de oefeningen vereenvoudigen we dit systeem door gebruik te maken van een embedded database die in het lokaal geheugen kan draaien.\nWe teren dus op de volgende kennis:\nHet opstellen van Gradle projecten in Java (SES); Databases ontwerpen en koppelen (Databases); GUI interfaces ontwikkelen (FSWEB). Problemen met je JDK versie en Gradle versies? Raadpleeg de Gradle Compatibiility Matrix. Gradle 6.7 of hoger ondersteunt JDK15. Gradle 8.5 of hoger ondersteunt JDK21. Let op met syntax wijzigingen bij Gradle 7+! Je Gradle versie verhogen kan door de URL in gradle/gradlew.properties te wijzigen.\nGradle dependency of Git source control problemen? Grijp terug naar de cursus van SES.\nAPI solutions testen "
},
{
	"uri": "http://localhost:1313/db-course/bigdata/basics/",
	"title": "Big Data Basics",
	"tags": [],
	"description": "",
	"content": "De Big in Big Data mag je letterlijk nemen. IBM berekende onlangs dat wij allemaal 2.5 quintillion bytes aan data genereren. Elke minuut meer dan 350.000 tweets, 75.000 uren van Netflix video streams, meer dan 35.000 Apple store apps gedownload, enzovoort.\nDe term Big Data is al sinds eind de jaren negentig aan een opmars bezig. We kunnen \u0026ldquo;grote datasets\u0026rdquo; categoriseren afhankelijk van wat we noemen de vijf Vs:\nVolume\u0026mdash;Het gaat (uiteraard) over een alsmaar groeiend \u0026ldquo;groot volume\u0026rdquo; aan data; Velocity\u0026mdash;De snelheid waarmee de data in en uit systemem vloeit die altijd maar toeneemt; Variety\u0026mdash;De range aan data types breidt altijd maar uit (JSON, XML, RDBMS, noSQL, files, \u0026hellip;); Veracity\u0026mdash;Hoe waarheidsgetrouw is de data eigenlijk wel? Data vertoont alsmaar vaker inconsistenties/ambiguiteit; Value\u0026mdash;Wat heb je aan al die data zonder er iets waardevol mee te doen (zoals een crutiale business beslissing)? src: tistory.com\rDenk aan de biljoenen mensen die dagelijks Facebook gebruiken en gigantische volumes aan foto\u0026rsquo;s en tekst uploaden. Het aantal gebruikers dat gestaag steeg (velocity) en de variëteit van data zoals APIs tussen Facebook en anderen om elders in te loggen met je account, video en fotomateriaal in plaats van enkel tekst, integratie met WhatsApp/Instagram, \u0026hellip; Maar hoeveel fake data zit er wel niet tussen, en hoeveel accounts worden misbruikt? Hoeveel data lekken en gevoelige data bevat het? (veracity). Vooral uw persoonsgegevens is voor Meta, het bedrijf achter Facebook, letterlijk goud waard, en verkoopt het door aan derden (value), vaak zonder jouw toestemming!\nNog een leuk voorbeeld: de winkel https://ebay.com/ heeft een data warehouse van 45 petabytes\u0026mdash;ofwel 45.000 terabytes!\nUit de vorige hoofdstukken blijkt dat een traditioneel RDBMS systeem niet goed kan scalen (zie nosql introductie). Echter, een NoSQL DB is slechts één component in het gehele Big Data ecosysteem. In de praktijk gaat het over veel pools, clusters, servers, heterogene DB systemen, files, APIs, \u0026hellip; allemaal gecombineerd (zie data warehousing lakes). Het beheren van een Big Data systeem gaat niet over één DB systeem, maar over het beheren van verschillende parameters (snelheid, grootte van input, connecties tussen systemen, \u0026hellip;). De analyse van de inhoud ervan is vaak een taak voor gespecialiseerde data scientists.\n"
},
{
	"uri": "http://localhost:1313/db-course/sql/rdbms-basics/",
	"title": "Database Basics",
	"tags": [],
	"description": "",
	"content": "Een database is niet meer dan een verzameling van gegevens. Een DBMS (DataBase Management System) is de software waarmee databases beheerd of aangemaakt kunnen worden.\n1. Waarom een database gebruiken? Een database wordt ook maar gewoon opgeslagen op een file system. Dus waarom kan ik dan niet zelf files gebruiken om mijn data op te slaan?\nDatabases bieden een aantal key features:\nPerformant (index management) Betere integratie met andere applicaties Uniform DBMS voor bewerken of ophalen van data Concurrency ondersteuning Security \u0026amp; Privacy van data \u0026hellip; In het tweedejaarsvak Besturingssystemen en C leerde je dat IO manipulatie heel dure operaties zijn. Een erg groot bestand openen of een seek() operatie uitvoeren daarop, duizenden bestanden tegelijkertijd openen voor data access, \u0026hellip;\u0026mdash;allemaal voorbeelden van nadelen waar een database de oplossing kan bieden. Achterliggend werkt het DBMS systeem nog steeds met files, maar dat is supergeoptimaliseerd door bijvoorbeeld gebruik te maken van verschillende niveaus van caching, file chunking, gedistribueerde modellen, \u0026hellip; De theorie en implementatie van een DBMS gaan we niet behandelen in deze cursus: de focus ligt op het gebruik van bestaande systemen.\n2. Database Model De data die zich in een database bevindt wordt op een specifieke manier opgeslagen. De structuur waarop deze data bijgehouden wordt, noemen we het database model.\nEen database model bestaat uit meerdere data modellen. Een data model beschrijft één specifiek object.\nWe zien hetzeflde eigenlijk terug als we denken aan Java of Kotlin. We definiëren hoe een klasse eruit ziet. Bijvoorbeeld volgende klasse:\ndata class Book(val isbn: string, val title: string, val author: string, val price: double) public class Book { String isbn; String title; String author; double price; public Book(isbn, title, author, price) { this.isbn = isbn; this.title = title; this.author = author; this.price = price; } } Dit kunnen we ook in een database bepalen. Daar zou het data model van de tabel Book er bijvoorbeeld als volgt kunnen uitzien:\nclassDiagram\rclass Book{\risbn: NVARCHAR(50)\rtitle: NVARCHAR (500)\rauthor: NVARCHAR (500)\rprice: DECIMAL(10,4)\r}\rNet als we in code state kunnen hebben wanneer we onze klasses instantiëren:\nvar book = Book(\u0026#34;0765326353\u0026#34;, \u0026#34;The Way of Kings\u0026#34;, \u0026#34;Brandon Sanderson\u0026#34;, 24.99) var book = new Book(\u0026#34;0765326353\u0026#34;, \u0026#34;The Way of Kings\u0026#34;, \u0026#34;Brandon Sanderson\u0026#34;, 24.99); Zo kunnen we ook state hebben in onze database:\nisbn title author price 0765326353 The Way of Kings Brandon Sanderson 24.99 Elk data model kan een aantal properties bevatten, zoals bovenstaande isbn en title, waarbij een type moet gedefiniëerd worden, zoals bovenstaande NVARCHAR(x). Dit zijn datatype namen die specifiek zijn voor elk DBMS.\nIn de oefeningen gaan wij SQLite gebruiken: zie ook datatypes in SQLite. SQLite\u0026rsquo;s types zijn loosely typed, wat wil zeggen dat er geen verschil is tussen VARCHAR (MSSQL\u0026rsquo;s ASCII) en NVARCHAR (MSSQL\u0026rsquo;s Unicode, UTF-16). Intern worden beide types gemapped naar TEXT. Raadpleeg dus telkens de manual om te controleren welke DBMS welke types ondersteund, en wat deze precies betekenen! Een Java/Kotlin String mapt dus niet altijd 100% op een RDBMS teksttype.\nEen voorbeeld van een simpel database model voor de inventaris van een bibliotheek zou er ongeveer als volgt kunnen uitzien:\nclassDiagram\rBook \"0..*\" --\u003e \"1..*\" Genre\rBook \"1..*\" --\u003e \"1\" Author\rclass Author{\rid: INT\rname: NVARCHAR\rfirstName: NVARCHAR\r}\rclass Genre{\rid: INT\rdescription: NVARCHAR\r}\rclass Book{\risbn: NVARCHAR\rtitle: NVARCHAR\rauthor: INT\rprice: DECIMAL\rgenre: INT\r}\rMerk op dat we hier relaties gebruiken: de DBMS systemen die we eerst behandelen, SQL-varianten, zijn RDBMS systemen: relationele database management systemen. De author in Book is een nummer dat verwijst naar de id van Author in een ander model of tabel. Op deze manier is het mogelijk om, voor elke rij in Author, meerdere Book rijen aan te maken:\ndata class Author(val id: int, val name: string, val books: List\u0026lt;Book\u0026gt;) public class Author { private final int id; private final String name; private final List\u0026lt;Book\u0026gt; books; public Author(int id, int name) { this.id = id; this.name = name; this.books = new ArrayList\u0026lt;\u0026gt;(); } } "
},
{
	"uri": "http://localhost:1313/db-course/sql-ddl-dml/ddl/",
	"title": "DDL",
	"tags": [],
	"description": "",
	"content": "Data Defintion Language is de taal die we gebruiken om de structuur van onze database te veranderen. We kunnen hiermee tabellen aanmaken, wijzigen of verwijderen. Maar ook indexen, views, triggers of stored procedures worden hiermee aangemaakt.\nZowat elke RDBMS heeft tooling om DDL te doen via een handige interface, in plaats van dit zelf uit te schrijven. In de praktijk ga je waarschijnlijk met beiden in contact komen. We gaan DB Browser for SQLite gebruiken tijdens onze lessen.\nKijk naar de Chinook database en maak een schematische voorstelling van hoe deze database eruit ziet. Je kan hiervoor Mermaid gebruiken, of een eigen tool of pen en papier.\nTabellen aanmaken en wijzigen Met DDL definiëer je structuur in SQL. Met DML wijzig of manipuleer je de inhoud ervan. De Chinook database bevat natuurlijk reeds tabellen, maar alles begint met een CREATE TABLE statement. Je kan in SQLite Browser rechtsklikken op tabellen en Copy Create Statement kiezen om te reverse-engineeren hoe de tabellen aangemaakt werden.\nBijvoorbeeld, voor albums:\nCREATE TABLE \u0026#34;albums\u0026#34; ( [AlbumId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, [Title] NVARCHAR(160) NOT NULL, [ArtistId] INTEGER NOT NULL, FOREIGN KEY ([ArtistId]) REFERENCES \u0026#34;artists\u0026#34; ([ArtistId]) ON DELETE NO ACTION ON UPDATE NO ACTION ) Rekening houdend met de vereenvoudigde datatypes van SQLite, zou je het zelf waarschijnlijk ongeveer zo schrijven:\nCREATE TABLE album ( AlbumId INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, Title TEXT NOT NULL, ArtistId INTEGER NOT NULL, FOREIGN KEY ArtistId REFERENCES artists (ArtistId) ) De NO ACTION statements zijn nutteloos. Namen hoeven, afhankelijk van het dialect, al dan niet escaped als [naam] of \u0026quot;naam\u0026quot;. Merk ook op dat bovenstaande SQL zal falen als de tabel artists niet eerst gemaakt wordt, anders kan de DB geen foreign key constraint controle uitvoeren. Meerdere statements worden gebruikelijk gescheiden door puntkomma ;.\nMerk op dat in SQL DDL we keywords met HOOFDLETTER schrijven en tabel of kolomnamen met kleine letter.\nProbleem met je structuur? Geen probleem: DROP TABLE album. Als hier data in zit ben je die ook onherroepelijk kwijt! Kolom vergeten? Geen probleem: ALTER TABLE album ADD COLUMN blah TEXT. Kolom te veel? Geen probleem: ALTER TABLE album DROP COLUMN blah. Not null constraint vergeten? Geen probleem: ALTER TABLE album ALTER COLUMN blah NOT NULL. Oei, syntaxfoutje? SQLite ondersteunt geen alter in alter, andere vendors wel. Check constraint vergeten? Geen probleem: ALTER TABLE album ADD CONSTRAINT my_constraint CHECK(len(blah) \u0026gt; 9) Oei, syntaxfoutje? Zelfde probleem\u0026mdash;enkel op te lossen door DROP en re-create. De CREATE en DROP statements kunnen ook gebruikt worden\u0026mdash;afhankelijk van de compatibiliteit van je RDBMS\u0026mdash;om indexen, aliases, tablespaces, synoniemen, sequenties, \u0026hellip; aan te maken en te verwijderen.\nMaak twee nieuwe tabellen aan: een licenses tabel, die per album (en dus ook artiest) licenties en hun kostprijs opslaat, en een memorabelia tabel die merchandise voor artiesten bevat om te verkopen. Denk goed na over het gebruik van constraints. Voeg daarna met ALTER TABLE kolommen toe om het (variërend) BTW percentage voor beide tabellen te bewaren.\nViews aanmaken Een view is eigenlijk een specifieke query die we een vaste naam geven.\nAls ik een view wil maken van alle tracks van het Rock genre, dan doe ik dat als volgt:\nCREATE VIEW rock_tracks AS SELECT * FROM tracks WHERE GenreId = 1 Vanaf dit punt kan ik in een nieuwe query het volgende uitvoeren:\nSELECT * FROM rock_tracks Merk op dat we vaak geen idee hebben welke ID het genre Rock heeft:\nSELECT * FROM genres WHERE Name = \u0026#39;Rock\u0026#39; In plaats van de resultaten te limiteren op GenreId, kunnen we beide tabellen joinen:\nCREATE VIEW rock_tracks AS SELECT tracks.* FROM tracks INNER JOIN genres ON genres.GenreId = tracks.GenreId WHERE genres.Name = \u0026#39;Rock\u0026#39; Dit kan ook met behulp van een subquery. Zie ook: DDML - JOIN operator en verder, subqueries.\nSchrijf een view die de naam van elke track geeft, alsook de naam van het album en de artiest, gesorteerd op artiest, album en dan track.\n"
},
{
	"uri": "http://localhost:1313/db-course/extra/kennen-kunnen/",
	"title": "Examen: Kennen en kunnen",
	"tags": [],
	"description": "",
	"content": "Voorbeeldvragen Examen databases Voorbeeld vragen:\nLeg uit wat de \u0026ldquo;write-ahead log strategy\u0026rdquo; betekend in transaction management en hoe deze strategie gebruikt wordt om rollbacks uit te voeren bij een transaction failure.\nGegeven een figuur figuur, waarin twee transacties tegelijkertijd items op een beurs verkopen binnen één database\nWelk probleem wordt hier uitgebeeld, én waarom? Beschrijf twee verschillende mogelijkheden om dit probleem op te lossen, én wat de voor- en nadelen zijn van de mogelijkheid. Illustreer voor elke mogelijkheid een figuur zoals bovenstaande dat bewijst dat het probleem verholpen werd. Stel dat een bank je benadert voor advies omdat hun systeem traag is. Ze hebben één groot Maria DB-based database systeem dat zowel smartphone apps als de globale website als applicaties op kantoren data voedt. Ze verwachten van jou twee mogelijke scenario\u0026rsquo;s om dit probleem aan te pakken. Schets deze hieronder.\nLeg uit wat Soft State betekend.\nGeef het verschil in werking tussen een distributed key-value store systeem (bvb. Redis) en een Wide-Column oplossing (bvb. Cassandra). Geef naast dit verschil ook het grootste voordeel van elk database type.\nGegeven volgende xml-file en xsd-schema: Voeg … data toe aan de xml-file op de juiste plaats en pas het xsd-schema aan om aan de nieuwe structuur te voldoen\nWat is het verschil tussen een data lake en een data warehouse? Leg uit.\n"
},
{
	"uri": "http://localhost:1313/db-course/nosql/basics/",
	"title": "NoSQL Basics",
	"tags": [],
	"description": "",
	"content": "Het schaalbaarheid probleem Het probleem met RDMS (relationele database management systems) is vaak schaalbaarheid. Gezien de ACID data validity voorwaarden is altijd de vraag: is dit schaalbaar?\nOptie 1: Vertical scaling De makkelijke oplossing is \u0026ldquo;scaling up\u0026rdquo;: meer storage, CPU, RAM, \u0026hellip; voorzien zodat er meer cycles kunnen benut worden en hopelijk ook meer transacties concurrent kunnen worden verwerkt (zie transacties basics).\nJe botst hier echter snel op hardware limitaties\u0026mdash;niet alles is opgelost met een latje RAM.\nOptie 2: Horizontal scaling In plaats van \u0026ldquo;omhoog\u0026rdquo; te gaan en meer hardware in hetzelfde systeem te steken, kunnen we ook meer hardware op verschillende plaatsen in het netwerk zetten: scaling out in clusters. Dit noemen we horizontaal scalen: meer kleintjes die gedistribueerd hetzelfde doen.\nDeze oplossing introduceert echter een ander probleem: data consistency is niet altijd gegarandeerd. Als ik iets in één server van een cluster bewaar, wordt dit dan onmiddellijk in de andere ook bewaard? Wat als er eentje uitvalt, en dat net mijn access point was? Op welke manier wordt die data verdeeld binnen de cluster? Enzovoort. Distributed computing is een erg complex domein binnen de informatica. We raken in dit vak enkel de top van de ijsberg aan.\nWat is een \u0026ldquo;cluster\u0026rdquo; precies? Denk aan een verzameling van grote data centers: twee of meer fysieke centra waar enorm veel servers geplaatst worden. Eén server kan je eigen laptop zijn. Je kan ook verschillende virtuele servers op je laptop draaien: dat is een node. (We hanteren hier de hierarchie van elementen volgens Cassandara) Cluster \u0026laquo; Data Center \u0026laquo; Rack \u0026laquo; Server \u0026laquo; Node\nEen typische relationele database, met zijn ACID eigenschappen, maakt horizontaal schalen dus moeilijk. Consistentie en availability maakt partitioning tot een uitdaging. Dit is ook zichtbaar in het CAP probleem; of \u0026ldquo;Consistency, Availability, Partitioning Tolerance\u0026rdquo; probleem. Wil je inzetten op partitioning, dan is de kans groot dat je zal moeten inboeten op consistency en availability. De volgende figuur illustreert dit probleem:\nHet CAP probleem. src: freecodecamp.org\rFlexibiliteit van horizontal scaling krijgen we door af te stappen van een typische RDBMS, en te kijken naar wat er kan als de R (relational) wegvalt\u0026mdash;ofwel noSQL databases. De populariteit hiervan groeide exponentieel sinds scalability een groter probleem werd: denk aan gigantische data warehouses van Amazon, Google\u0026rsquo;s zoek engine, Facebook pagina\u0026rsquo;s, enzovoort. NoSQL systemen garanderen ook consistentie\u0026mdash;alleen niet onmiddellijk: dit heet eventual consistency.\nDus, horizontal scaling is eenvoudiger met NoSQL:\nEr is géén relationele data; Er is géén (onmiddellijke) consistentie\u0026mdash;dus ook geen coordinatie overhead!; Dit is zeer goed scalable. NoSQL basics Een vergelijking van eigenschappen tussen een relationele en niet-relationele database systeem:\nEigenschap Relationeel NoSQL Data paradigma relationeel 4 types: key/val, docs, \u0026hellip; (s3.2) Distributie Single-node Distributed Scalability Vertical Horizontal, replication Structuur Schema-based Flexible Querty taal SQL Specialized (JavaScript) Transacties ACID BASE Features views/procs/\u0026hellip; basic API Data vol. \u0026ldquo;normal\u0026rdquo; \u0026ldquo;huge amounts\u0026rdquo; *BASE staat voor Basically Available, Soft state, Eventual concistency\nMerk op dat het niet altijd de beste oplossing is om naar een NoSQL DB te grijpen. Wanneer dan wel of niet? De volgende vragen kunnen hierbij helpen:\nBevat data veel/weinig relaties? Komt er enorm veel data/sec. binnen? Replication vereisten? Scripting mogelijkheden? Bestaande kennis in bedrijf? \u0026hellip; Klassieke relationele databases zijn nog steeds een van de meestgebruikte ter wereld, maar dat wil niet zeggen dat er geen (populaire) alternatieven zijn. Kijk eens naar de db-engines.com ranking trends op db-engines.com:\nDe drie bovenste lijnen zijn Oracle, MySQL en Microsoft SQL Server, de drie giganten die alledrie relationele DBMS systemen zijn. PostgresQL, de oranje stijgende lijn, is volgende\u0026mdash;ook SQL. Maar daarnaast volgen MongoDB, Cassandra, Redis, DynamoDB, \u0026hellip;\u0026mdash;allemaal verschillende soorten noSQL alternatieven.\nNoSQL Types 4 NoSQL types. src: improgrammer.net\rEr zijn, zoals bovenstaande figuur aangeeft, 4 grote groepen van NoSQL systemen:\n1. Document stores. Hier bewaar je een \u0026ldquo;document\u0026rdquo; in, dat meestal in JSON-formaat is, zoals:\n{ \u0026#34;bedrag\u0026#34;: 100.3, \u0026#34;gebruiker\u0026#34;: \u0026#34;Jos Klakmans\u0026#34;, \u0026#34;Stad\u0026#34;: \u0026#34;Diepenbeek\u0026#34;, \u0026#34;certificaten\u0026#34;: [{ \u0026#34;type\u0026#34;: 1, \u0026#34;naam\u0026#34;: \u0026#34;Master in de bliebloe\u0026#34; }, { \u0026#34;type\u0026#34;: 2, \u0026#34;naam\u0026#34;: \u0026#34;Bachelor in de blakkiela\u0026#34; }] } Merk op dat hier geen relaties worden gelegd, alhoewel dat wel kan: bijvoorbeeld document 1 kan een property { id: 1 } hebben, en document 2 { id: 2, relatedDocumentId: 1 }. Dit echter veel gebruiken zal een performance hit geven: document stores dienen voornamelijk om gigantisch veel onafhankelijke data te bewaren, op een ongestructureerde manier. Er zijn geen table definities: een key meer of minder maakt niet uit.\nNoSQL: { name: 'Jos' } -\u0026gt; { name: 'Jos', well-behaved: true }. Geen INSERT INTO student(name) VALUES (\u0026quot;Jos\u0026quot;) dus! Ook hier wordt intern hashing gebruikt (zie onder): Het document { name: 'Jos' } wordt intern opgeslaan als { name: 'Jos', _id: 23235435 }. Data retrieval snelheid blijft belangrijk, dus extra indexen/views kunnen door de gebruiker zelf worden aangemaakt (zie volgende hoofdstukken).\nOm documenten te ordenen worden soms wel collecties aangemaakt, maar dit is bijna altijd optioneel!\nWe zullen ons in deze cursus focussen op document stores. Zie NoSQL - document stores om een idee te hebben hoe dit in de praktijk gebruikt wordt.\n2. Graph-based oplossingen. Wat als we toch veel relationele gegevens hebben, maar het nog steeds over (1) ongestructureerde data gaat en (2) te veel is voor in één klassiek RDBMS systeem te bewaren? Als de relaties de data zelf zijn, dan hebben we een grafen-gebaseerde oplossing nodig. Hier zijn géén dure JOIN statements nodig om de relaties ad-hoc te maken. Een typische toepassing hiervan zou social graphs zijn.\nEen voorbeeld subgraph visualisatie in Neo4j.\rStel dat je alle boeken wilt ophalen geschreven door een bepaalde auteur (= de relatie). In SQL, waar de data typisch in twee tabellen leeft (book en author), heb je een (impliciete) JOIN nodig:\nSELECT book, title FROM book, author, books_authors\rWHERE author.id = books_authors.author_id\rAND book.id = books_authors.book_id\rAND author.name = \u0026#34;De Jos\u0026#34; Maar in Cypher, de querytaal van grafendatabase Neo4J, ziet die query er als volgt uit:\nMATCH (b:Book) \u0026lt;- [ :WRITTEN_BY]-(a:Author)\rWHERE a.name = \u0026#34;De Jos\u0026#34;\rRETURN b.title Data wordt op basis van WRITTEN_BY relatie eigenschap opgehaald. Relationele data\u0026mdash;de letterlijke relaties\u0026mdash;zijn hier altijd expliciet, en niet verborgen in foreign key constraints.\n3. Key-Value stores. Dit is de eenvoudigste soort, waarbij gewoon blobs van data in een hash table opgeslagen worden, zoals jullie gewoon zijn in Java:\nMap\u0026lt;String, Persoon\u0026gt; leeftijden = new HashMap\u0026lt;\u0026gt;(); leeftijden.put(\u0026#34;Wilfried\u0026#34;, new Persoon(\u0026#34;Wilfried\u0026#34;, 20)); leeftijden.put(\u0026#34;Seppe\u0026#34;, new Persoon(\u0026#34;Seppe\u0026#34;, 30)); leeftijden.put(\u0026#34;Bart\u0026#34;, new Persoon(\u0026#34;Bart\u0026#34;, 40)); leeftijden.put(\u0026#34;Jeanne\u0026#34;, new Persoon(\u0026#34;Jeanne\u0026#34;, 18)); Hash functies In dit voorbeeld stopt de HashMap met bestaan zodra die out of scope gaat op je eigen machine, maar er zijn ook distributed hash tables. Hier is de hash functie het belangrijkste onderdeel, die de onderliggende key genereerd en dus bepaald in welke \u0026ldquo;bucket\u0026rdquo; een waarde wordt opgeslagen\u0026mdash;en dus ook, op welke server in een cluster. Een goede hash functie moet (1) deterministisch zijn: atlijd dezelfde hash waarde voor dezelfe input genereren; (2) uniform zijn: er moet een goede verdeling zijn van de output range; en (3) een vaste grootte hebben zodat het makkelijker is voor de data structuur om de hash waarde te bewaren.\nVrijwel alle NoSQL databases gebruiken achterliggend hashing technieken om horizontal scalability makkelijker te maken. Als alle hash values mooi verdeeld worden, kan dit ook mooi over verschillende databases verdeeld worden.\nPartioning/sharding In bovenstaand voorbeeld worden de persoonsgegevens verspreid over 3 verschillende servers door de hashing \u0026ldquo;index\u0026rdquo; (mod3 + 1). Data partitioning noemen we ook wel sharding waarbij een individuele partitie een shard is. Om zo efficient mogelijk te partitioneren schakel je best servers aan elkaar in een soort van \u0026ldquo;ring\u0026rdquo;, zoals in dit schema:\nRing partitioning vereist wel consistent hashing functies, anders klopt de node verdeling (de kleuren in het schema) niet meer. Om zo effient mogelijk data door te geven (replication, zie later NoSQL: replication) hebben nodes weet van elkaar. Het is echter nog steeds niet mogelijk om de ACID regels te volgen: een gedistribueerd systeem zoals deze ring kan nooit én consistent én available én partition tolerant zijn.\nWat is dan een oplossing voor NoSQL systemen? BASE in plaats van ACID:\nBasically Available (BA); elke (gebruikelijk HTTP-based) request ontvangt een respons, hetzij een 200 (OK), hetzij een 4xx/5xx (een externe/interne fout). Ook al zijn niet alle nodes geupdate, toch kan er al een 201 worden teruggegeven\u0026mdash;asynchroon dus. Soft state (S); sate kan wijzigen, ook zonder input! We weten dus nooit exact wat er in de shards zit. Read requests zijn soms out-of-date omdat een shard update in de ring partitie plaats aan het vinden was, maar dat één bepaalde shard nog niet bereikte\u0026hellip; Eventually Consistent (E); NoSQL biedt de \u0026ldquo;ooit is het wel consistent\u0026rdquo; mode aan. In de praktijk verschilt het van NoSQL database tot database systeem hoe dicht deze BASE regels tegen de ACID regels aanleunen. De document-based CouchDB, die we later zullen in detail bekijken, ondersteunt ook vormen van transacties en dergelijke, wat het eerder iets ACID-achtig maakt.\nMemcached Memcached is een distributed in-memory key/value store die op grote schaal gebruikt kan worden als caching mechanisme. Systemen als Memcached zijn enorm performant en worden vaak gebruikt als caching database die geplaatst wordt voor de eigenlijke RDBMS:\ngraph LR;\ruser[User]\rcache{Cache DB}\rdb{Relationele DB}\ruser --\u003e|Haal genres op| cache\rcache --\u003e|cache hit| user cache --\u003e|cache miss| db\rdb --\u003e|refresh| cache\rHet feit dat Netflix Memcached sponsort zegt genoeg. Memcached gebruiken is erg eenvoudig en gewoon een kwestie van de API in Java/Kotlin aan te spreken om data te feeden/op te halen.\nEen simpel Memcached voorbeeld is terug te vinden onder key-value stores: memcached.\n4. Wide-column databases. Wide-column, of column-based databases, zijn eigenlijk relationele databases op zijn kop\u0026mdash;letterlijk.\nWat is het grootste nadeel van het queryen van relationele databases? Deze zijn row-based: als je alle genres uit een BOEK tabel wilt halen, zal je alle rijen moeten afgaan en daar een DISTINCT op doen\u0026mdash;alles behalve performant. Bijvoorbeeld:\nid genre title price\r1 Fantasy book bla 10\r2 Fantasy another title 20\r3 horror wow-book 10 Hoe haal ik hier alle genres op? SELECT DISTINCT(genre) FROM boeken. Wat is de gemiddelde prijs? SELECT AVG(price) FROM boeken\u0026mdash;ook een erg dure operatie indien er miljoenen records aanwezig zijn. Een snelheidswinst valt te boeken door te werken met indexen, maar daar lossen we niet alles mee op.\nWat nu als je de kolommen als rijen beschouwt, op deze manier:\ngenre: fantasy:1,2 horror: 3\rtitle: book bla:1, another title:2 wow-book: 3\rprice: 10:1,3 20:2 Wat is nu de gemiddelde prijs? Haal 1 \u0026ldquo;rij\u0026rdquo; op en deel door het aantal. Welke genres zijn er zoal? De eerste rij is al onmiddellijk het antwoord! We verzamelen hier dus vertical slices van data, wat erg belangrijk kan zijn voor Business Intelligence (BI): super-linked data tussen de \u0026ldquo;echte\u0026rdquo; row-based data.\nDe meest gebruikte column-DB is Cassandra. Op de website staat:\nManage massive amounts of data, fast, without losing sleep.\nCassandra komt met in-memory buffers, tracking \u0026amp; monitoring, \u0026hellip;\nACID vs BASE Aspect ACID (SQL) BASE (NoSQL) Consistentiemodel Sterke consistentie (data is altijd correct en up-to-date) Eventuele consistentie (data wordt uiteindelijk consistent) Transactiefocus Garandeert betrouwbaarheid (bv. banktransacties) Prioriteit op beschikbaarheid en snelheid (bv. sociale media-platforms) Gebruiksvoorbeelden Financiële systemen, ERP, CRM Big data, real-time analytics, IoT, schaalbare webapps Schaalbaarheid Verticaal schalen (krachtigere hardware) Horizontaal schalen (meer servers toevoegen) Datastructuur Gestructureerd (tabellen met vaste schema\u0026rsquo;s) On-gestructureerd/flexibel (documenten, key-value, grafieken, etc.) Schema Strict schema vooraf vereist Schema-loos (flexibele data-modellering) Complexe queries Ondersteund via SQL (JOINs, aggregaties, etc.) Beperkt, vaak afhankelijk van denormalisatie Performantie Lagere latentie voor complexe transacties Hogere doorvoer voor grote datasets Voorbeelden MySQL, PostgreSQL, Oracle MongoDB (document), Cassandra (wide-column), Redis (key-value) Sterke punten Data-integriteit, transactiegaranties, complexe relaties Schaalbaarheid, flexibiliteit, hoge beschikbaarheid Zwakke punten Moeilijker horizontaal schalen, rigiditeit bij schemawijzigingen Geen transactiegaranties, moeilijker voor relationele use cases Case Studies Welke database systemen\u0026mdash;of een combinatie ervan\u0026mdash;denk je dat de volgende grote bedrijven hanteren voor hun producten?\nhttps://www.benl.ebay.be/ Hint: https://www.slideshare.net/jaykumarpatel/cassandra-at-ebay-13920376 (2012) https://www.army.mil/ Hint: https://go.neo4j.com/rs/710-RRC-335/images/Neo4j-case-study-US-army-EN-US.pdf (2019) https://spotify.com/ Hint: https://engineering.atspotify.com/2015/01/09/personalization-at-spotify-using-cassandra/ (2015) https://uber.com/ Hint: http://highscalability.com/blog/2016/9/28/how-uber-manages-a-million-writes-per-second-using-mesos-and.html (2016) Denkvragen Is een RDBMS of een NoSQL database geschikter om aan \u0026ldquo;Big Data\u0026rdquo; te doen? Waarom wel/niet? Waarom vereist ring partitioning consistent hashing? Wat heeft een hashing functie te maken met het horizontaal kunnen schalen van data in een DBMS? Waarom gebruiken zo veel grote bedrijven een combinatie van verschillende DBMS systemen? Zie je hier ook nadelen in? Wanneer denk je dat een column-based database als Cassandra nuttig zou zijn? Leg het verschil tussen ACID en BASE uit in functie van de typische eigenschappen van een database. Waarom werkt vertical scaling niet? Waarom wel? "
},
{
	"uri": "http://localhost:1313/db-course/sql/",
	"title": "RDBMS",
	"tags": [],
	"description": "",
	"content": "RDBMS Zie menu links.\n"
},
{
	"uri": "http://localhost:1313/db-course/timeseries/timeseriesdb/",
	"title": "Timeseries database met InfluxDb",
	"tags": [],
	"description": "",
	"content": "Timeseries database basics Introductie Een timeseries database (TSDB) is een database die is geoptimaliseerd voor het opslaan en ophalen van tijdgestempelde gegevens. Dit type database wordt vaak gebruikt in toepassingen zoals IoT, monitoring, en financiële analyses, waar gegevens in de tijd worden verzameld en geanalyseerd. TSDB\u0026rsquo;s zijn ontworpen om efficiënt om te gaan met grote hoeveelheden gegevens die continu worden toegevoegd en om snelle query\u0026rsquo;s uit te voeren op basis van tijd.\nVerschil met SQL en NoSQL databases Time Series Databases (TSDB\u0026rsquo;s) onderscheiden zich van traditionele SQL- en NoSQL-databases door hun focus op tijdgestempelde gegevens. Terwijl SQL-databases goed zijn in het beheren van gestructureerde gegevens en NoSQL-databases flexibel zijn in het omgaan met verschillende datatypes, zijn TSDB\u0026rsquo;s specifiek ontworpen voor tijdgebaseerde gegevens.\nIndexering: TSDB\u0026rsquo;s gebruiken gespecialiseerde indexstructuren zoals R-trees of Bitmap-indexen, die zijn geoptimaliseerd voor tijdgebaseerde query\u0026rsquo;s. Dit maakt ze efficiënter dan de B-trees van SQL of de hash-indexen van NoSQL. Lees- en schrijfbewerkingen: TSDB\u0026rsquo;s bieden een balans tussen snelle schrijfbewerkingen en efficiënte leesbewerkingen. Ze zijn geoptimaliseerd voor sequentiële schrijfbewerkingen en tijdgebaseerde query\u0026rsquo;s. Beperkingen van SQL en NoSQL: Hoewel SQL en NoSQL kunnen worden gebruikt voor tijdgebaseerde gegevens, missen ze de gespecialiseerde structuren en optimalisaties van TSDB\u0026rsquo;s. Het gebruik van een TSDB is vergelijkbaar met het gebruik van een pizzasnijder in plaats van een botermes om een pizza te snijden: het is ontworpen voor de taak en werkt efficiënter. In een wereld waar tijdgevoelige gegevens steeds belangrijker worden, bieden TSDB\u0026rsquo;s een unieke oplossing voor het efficiënt beheren en analyseren van deze gegevens.\nLink tussen IoT, Industry4.0 en Timeseries Databases IoT (Internet of Things) en Industry 4.0 genereren enorme hoeveelheden tijdgestempelde gegevens. Sensoren in IoT-apparaten en industriële machines verzamelen continu gegevens zoals temperatuur, druk, en trillingen. Deze gegevens zijn essentieel voor het monitoren, analyseren en optimaliseren van processen.\nTimeseries databases spelen een cruciale rol in deze context door:\nEfficiënt opslaan van gegevens: TSDB\u0026rsquo;s kunnen grote hoeveelheden gegevens opslaan met minimale overhead. Snelle query\u0026rsquo;s: Ze bieden snelle toegang tot historische gegevens, wat essentieel is voor analyses en rapportages. Geavanceerde analyses: Met functies zoals aggregaties en trendanalyses kunnen TSDB\u0026rsquo;s waardevolle inzichten bieden. Door de groei van IoT en Industry 4.0 wordt het gebruik van TSDB\u0026rsquo;s steeds belangrijker voor bedrijven die concurrerend willen blijven.\nInfluxDb: een open source timeseries database Installatie lokaal Om InfluxDB lokaal te installeren op Windows:\nDownload de InfluxDB OSS v2 van de officiële website. Pak het gedownloade ZIP-bestand uit in de map C:\\Program Files\\InfluxData\\influxdb. Open een terminal en navigeer naar de map: cd \u0026#34;C:\\Program Files\\InfluxData\\influxdb\u0026#34; Start de InfluxDB-server: ./influxd Open de GUI in je browser via http://localhost:8086. Maak een \u0026ldquo;all-purpose token\u0026rdquo; aan om toegang te krijgen tot de database. Andere populaire timeseries databases Naast InfluxDB zijn er andere populaire TSDB\u0026rsquo;s zoals:\nPrometheus: Gericht op monitoring en alerting. Graphite: Geschikt voor het visualiseren van tijdreeksen. OpenTSDB: Gebouwd op HBase voor schaalbaarheid. Overzicht van Gebruik met de web gui De web GUI van InfluxDB biedt een intuïtieve interface voor het beheren van gegevens. Je kunt:\nBuckets maken en beheren. Query\u0026rsquo;s uitvoeren met Flux. Dashboards maken voor visualisaties. Alerting instellen. Voorbeeld: een CO2 sensor voor een klaslokaal Stel je hebt een CO2-sensor in een klaslokaal. Je kunt de gegevens opslaan in InfluxDB met de volgende structuur:\nMeasurement: \u0026ldquo;CO2_concentration\u0026rdquo; Tags: \u0026ldquo;room\u0026rdquo;, \u0026ldquo;sensor_id\u0026rdquo; Fields: \u0026ldquo;value\u0026rdquo; Basics: buckets, measurements, tags, fields, retention \u0026hellip; InfluxDB gebruikt een aantal kernconcepten:\nBuckets: Containers voor gegevens met een ingestelde retentieperiode. Measurements: De naam van een verzameling gegevenspunten (bijvoorbeeld \u0026ldquo;temperatuur\u0026rdquo;). Tags: Metadata die gegevens beschrijft (bijvoorbeeld \u0026ldquo;locatie\u0026rdquo;). Fields: De werkelijke gegevenswaarden (bijvoorbeeld \u0026ldquo;23.5°C\u0026rdquo;). Retention Policies: Regels die bepalen hoe lang gegevens worden bewaard. Influx query syntax Een voorbeeld query om de gemiddelde CO2-concentratie van de afgelopen 24 uur te berekenen:\nfrom(bucket: \u0026#34;classroom_data\u0026#34;)\r|\u0026gt; range(start: -24h)\r|\u0026gt; filter(fn: (r) =\u0026gt; r[\u0026#34;_measurement\u0026#34;] == \u0026#34;CO2_concentration\u0026#34;)\r|\u0026gt; filter(fn: (r) =\u0026gt; r[\u0026#34;location\u0026#34;] == \u0026#34;Room 101\u0026#34;)\r|\u0026gt; mean() Stap-voor-stap uitleg from(bucket: \u0026quot;classroom_data\u0026quot;)\nDeze regel specificeert de bron van de gegevens. Hier wordt de bucket \u0026ldquo;sensor_data\u0026rdquo; gebruikt, wat een container is voor tijdreeksgegevens.\n|\u0026gt; range(start: -24h)\nBeperkt de gegevens tot de afgelopen 1 uur. Dit zorgt ervoor dat alleen recente gegevens worden geanalyseerd.\n|\u0026gt; filter(fn: (r) =\u0026gt; r[\u0026quot;_measurement\u0026quot;] == \u0026quot;CO2_concentration\u0026quot;)\nFiltert de gegevens om alleen de metingen met de naam \u0026ldquo;CO2_levels\u0026rdquo; te selecteren. Measurements groeperen gegevenspunten met dezelfde naam.\n|\u0026gt; filter(fn: (r) =\u0026gt; r[\u0026quot;location\u0026quot;] == \u0026quot;Room 101\u0026quot;)\nBeperkt de gegevens verder tot alleen die met de tag \u0026ldquo;location\u0026rdquo; gelijk aan \u0026ldquo;Room 101\u0026rdquo;. Tags zijn metadata die gegevens beschrijven.\n|\u0026gt; mean()\nBerekening van het gemiddelde van de geselecteerde gegevenspunten. Dit geeft een samenvattend getal terug dat de gemiddelde waarde vertegenwoordigt.\nOutput De output van deze query is een enkel getal dat het gemiddelde van de CO2-concentratie in \u0026ldquo;Room 101\u0026rdquo; over de afgelopen 1 uur weergeeft. Bijvoorbeeld:\n_time _value\r2025-05-06T10:00:00Z 450.5 _time: De tijdstempel waarop het gemiddelde is berekend. _value: Het gemiddelde van de CO2-concentratie. Meer info over de query syntax vind je hier\nMeasurements Wat zijn measurements? Measurements in InfluxDB zijn de logische namen die worden gebruikt om een verzameling gegevenspunten te groeperen. Ze fungeren als tabellen in een relationele database en bevatten velden en tags die de gegevens beschrijven. Een measurement kan bijvoorbeeld \u0026ldquo;temperature\u0026rdquo; of \u0026ldquo;CO2_concentration\u0026rdquo; heten.\nWerken met measurements Measurements worden gebruikt om gegevens te organiseren en te groeperen. Hier is een voorbeeld van hoe je een measurement kunt gebruiken in een Flux-query:\nfrom(bucket: \u0026#34;classroom_data\u0026#34;)\r|\u0026gt; range(start: -1h)\r|\u0026gt; filter(fn: (r) =\u0026gt; r[\u0026#34;_measurement\u0026#34;] == \u0026#34;CO2_concentration\u0026#34;) In dit voorbeeld worden alleen de gegevens uit de measurement \u0026ldquo;CO2_concentration\u0026rdquo; geselecteerd.\nTags Wat zijn tags? Tags in InfluxDB zijn key-value paren die worden gebruikt om metadata aan gegevens toe te voegen. Ze worden vaak gebruikt om gegevens te groeperen of te filteren. Tags zijn geïndexeerd, wat betekent dat ze efficiënt kunnen worden gebruikt in query\u0026rsquo;s.\nWerken met tags Tags worden gebruikt om gegevens te filteren of te groeperen. Hier is een voorbeeld van een Flux-query die gegevens filtert op basis van een tag:\nfrom(bucket: \u0026#34;classroom_data\u0026#34;)\r|\u0026gt; range(start: -1h)\r|\u0026gt; filter(fn: (r) =\u0026gt; r[\u0026#34;location\u0026#34;] == \u0026#34;Room 101\u0026#34;) In dit voorbeeld worden alleen de gegevens geselecteerd die de tag \u0026ldquo;location\u0026rdquo; hebben met de waarde \u0026ldquo;Room 101\u0026rdquo;.\nFields Wat zijn fields? Fields in InfluxDB zijn de werkelijke gegevenswaarden die worden opgeslagen in een measurement. Ze bevatten numerieke, string-, of booleaanse waarden en worden niet geïndexeerd. Fields worden gebruikt voor berekeningen en analyses.\nWerken met fields Fields worden gebruikt om de werkelijke gegevenswaarden op te slaan. Hier is een voorbeeld van een Flux-query die gegevens filtert op basis van een field:\nfrom(bucket: \u0026#34;classroom_data\u0026#34;)\r|\u0026gt; range(start: -1h)\r|\u0026gt; filter(fn: (r) =\u0026gt; r[\u0026#34;_field\u0026#34;] == \u0026#34;value\u0026#34;) In dit voorbeeld worden alleen de gegevens geselecteerd die het field \u0026ldquo;value\u0026rdquo; bevatten.\nAggregaties InfluxDB ondersteunt aggregaties, waarmee je complexe bewerkingen op tijdreeksen kunt uitvoeren. Aggregaties zijn essentieel voor het analyseren van trends, het berekenen van gemiddelden, en het samenvatten van grote datasets. In Flux, de querytaal van InfluxDB, kun je aggregaties uitvoeren met behulp van functies zoals mean(), sum(), count(), en median().\nWat zijn aggregaties? Aggregaties zijn bewerkingen die worden uitgevoerd op een verzameling gegevens om samenvattende informatie te verkrijgen. In de context van InfluxDB worden aggregaties vaak gebruikt om trends te analyseren, pieken te identificeren, en statistieken te berekenen over tijdreeksen. Aggregaties kunnen worden toegepast op velden binnen een meting en kunnen worden gecombineerd met filters en groeperingen.\nVoorbeeld van een aggregatie Hier is een voorbeeld van een Flux-query die de gemiddelde temperatuur berekent over de afgelopen 7 dagen:\nfrom(bucket: \u0026#34;classroom_data\u0026#34;)\r|\u0026gt; range(start: -7d)\r|\u0026gt; filter(fn: (r) =\u0026gt; r[\u0026#34;_measurement\u0026#34;] == \u0026#34;CO2_concentration\u0026#34;)\r|\u0026gt; mean() In dit voorbeeld:\nrange(start: -7d): Selecteert gegevens van de afgelopen 7 dagen. filter(fn: (r) =\u0026gt; r[\u0026quot;_measurement\u0026quot;] == \u0026quot;CO2_concentration\u0026quot;): Filtert alleen de CO2-metingen. mean(): Berekent het gemiddelde van de geselecteerde gegevens. Aggregaties combineren Je kunt meerdere aggregaties combineren in één query. Bijvoorbeeld, om zowel het gemiddelde als de maximale temperatuur te berekenen:\nfrom(bucket: \u0026#34;classroom_data\u0026#34;)\r|\u0026gt; range(start: -7d)\r|\u0026gt; filter(fn: (r) =\u0026gt; r[\u0026#34;_measurement\u0026#34;] == \u0026#34;CO2_concentration\u0026#34;)\r|\u0026gt; group(columns: [\u0026#34;location\u0026#34;])\r|\u0026gt; pivot(rowKey: [\u0026#34;_time\u0026#34;], columnKey: [\u0026#34;_field\u0026#34;], valueColumn: \u0026#34;_value\u0026#34;)\r|\u0026gt; map(fn: (r) =\u0026gt; ({\rlocation: r.location,\ravg_temp: mean(r.CO2_concentration),\rmax_temp: max(r.CO2_concentration)\r})) Hier worden de gegevens gegroepeerd per locatie en worden zowel het gemiddelde als de maximale temperatuur berekend.\nDemo in Java Om InfluxDB te gebruiken in een Java-toepassing, voeg je de volgende Gradle-dependency toe:\ndependencies { implementation \u0026#39;com.influxdb:influxdb-client-java:7.2.0\u0026#39; } Een voorbeeld van het schrijven en lezen van gegevens:\n/* * This source file was generated by the Gradle \u0026#39;init\u0026#39; task */ package be.kuleuven; import java.util.concurrent.TimeUnit; import com.influxdb.client.InfluxDBClient; import com.influxdb.client.InfluxDBClientFactory; import com.influxdb.client.QueryApi; import com.influxdb.client.WriteApiBlocking; import com.influxdb.client.domain.Bucket; import com.influxdb.client.domain.BucketRetentionRules; import com.influxdb.client.domain.WritePrecision; import com.influxdb.client.write.Point; import com.influxdb.query.FluxRecord; import java.time.Instant; import java.time.LocalDateTime; import java.time.OffsetDateTime; import java.time.ZoneId; import java.util.ArrayList; import java.util.List; import java.util.Map; public class App { private static final String DB_URI = \u0026#34;http://localhost:8086\u0026#34;; private static final String USERNAME = \u0026#34;influxdb\u0026#34;; private static final String ORG = \u0026#34;influxdb\u0026#34;; private static final String PASSWORD = \u0026#34;influxdb\u0026#34;; private static final String TOKEN = \u0026#34;Rp5cZ0FlsUrZi94Mbyx881rLyrgqR2ig2weiot6TWaj_sjMDNmsXCt5gPyBvsMYZdnCraN5QoDs73RUoY1Yn7Q==\u0026#34;; private static final String BUCKET = \u0026#34;classroom_data\u0026#34;; public static void main(String[] args) { InfluxDBClient influxDBClient = InfluxDBClientFactory.create(DB_URI, TOKEN.toCharArray(), ORG, BUCKET); clearBucket(influxDBClient); // Set correct time LocalDateTime customDateTime1 = LocalDateTime.of(2025, 5, 6, 8, 30, 0); Instant time1 = customDateTime1.atZone(ZoneId.systemDefault()).toInstant(); // ADD a value (point) WriteApiBlocking writeApi = influxDBClient.getWriteApiBlocking(); Point point1 = Point.measurement(\u0026#34;CO2_concentration\u0026#34;) .addTag(\u0026#34;location\u0026#34;, \u0026#34;Room 403\u0026#34;) .addField(\u0026#34;value\u0026#34;, 460) .time(time1.minusSeconds(60), WritePrecision.MS); Point point2 = Point.measurement(\u0026#34;CO2_concentration\u0026#34;) .addTag(\u0026#34;location\u0026#34;, \u0026#34;Room 403\u0026#34;) .addField(\u0026#34;value\u0026#34;, 440) .time(System.currentTimeMillis(), WritePrecision.MS); writeApi.writePoint(point1); writeApi.writePoint(point2); // GET all values in range List\u0026lt;CO2Point\u0026gt; points = getPointsInRange(influxDBClient, LocalDateTime.of(2025, 5, 4, 13, 30, 0), LocalDateTime.of(2025, 5, 10, 14, 30, 0)); for (CO2Point co2Point : points) { System.out.println(co2Point); } // Print SUM System.out.println(getSumValues(influxDBClient, LocalDateTime.of(2025, 5, 4, 13, 30, 0), LocalDateTime.of(2025, 5, 10, 14, 30, 0))); influxDBClient.close(); } public static List\u0026lt;CO2Point\u0026gt; getPointsInRange(InfluxDBClient influxDBClient, LocalDateTime startDate, LocalDateTime endDate) { QueryApi queryApi = influxDBClient.getQueryApi(); String fluxQuery = String.format( \u0026#34;from(bucket: \\\u0026#34;%s\\\u0026#34;) |\u0026gt; range(start: %s, stop: %s) |\u0026gt; filter(fn: (r) =\u0026gt; r._measurement == \\\u0026#34;CO2_concentration\\\u0026#34;)\u0026#34;, BUCKET, startDate.atZone(ZoneId.systemDefault()).toInstant().toString(), endDate.atZone(ZoneId.systemDefault()).toInstant().toString()); List\u0026lt;CO2Point\u0026gt; myObjects = new ArrayList\u0026lt;\u0026gt;(); queryApi.query(fluxQuery, ORG).forEach(table -\u0026gt; table.getRecords().forEach(record -\u0026gt; { LocalDateTime time = LocalDateTime.ofInstant(record.getTime(), ZoneId.systemDefault()); double value = ((Number) record.getValueByKey(\u0026#34;_value\u0026#34;)).doubleValue(); myObjects.add(new CO2Point(value, time)); })); return myObjects; } public static double getSumValues(InfluxDBClient influxDBClient, LocalDateTime startDate, LocalDateTime endDate) { QueryApi queryApi = influxDBClient.getQueryApi(); String fluxQuery = String.format( \u0026#34;from(bucket: \\\u0026#34;%s\\\u0026#34;) \u0026#34; + \u0026#34;|\u0026gt; range(start: %s, stop: %s) \u0026#34; + \u0026#34;|\u0026gt; filter(fn: (r) =\u0026gt; r._measurement == \\\u0026#34;CO2_concentration\\\u0026#34;) \u0026#34; + \u0026#34;|\u0026gt; sum(column: \\\u0026#34;_value\\\u0026#34;)\u0026#34;, BUCKET, startDate.atZone(ZoneId.systemDefault()).toInstant().toString(), endDate.atZone(ZoneId.systemDefault()).toInstant().toString()); List\u0026lt;FluxRecord\u0026gt; records = new ArrayList\u0026lt;\u0026gt;(); queryApi.query(fluxQuery, ORG).forEach(table -\u0026gt; records.addAll(table.getRecords())); if (records.get(0).getValueByKey(\u0026#34;_value\u0026#34;) != null) { return (long) records.get(0).getValueByKey(\u0026#34;_value\u0026#34;); } else { return -1.0; } } public static void clearBucket(InfluxDBClient influxDBClient) { // Use DeleteApi to clear all data in the bucket influxDBClient.getDeleteApi().delete( OffsetDateTime.ofInstant(Instant.ofEpochMilli(0), ZoneId.systemDefault()), // Start time (epoch 0 to delete all data) OffsetDateTime.ofInstant(Instant.now(), ZoneId.systemDefault()), // End time (current time) \u0026#34;\u0026#34;, // Empty predicate to delete all measurements BUCKET, // Bucket name ORG // Organization name ); } } Vaak wil je omvormen van LocalDateTime naar milis of omgekeerd:\n// Method to convert LocalDateTime to milliseconds public static long timeToMillis(LocalDateTime time) { return time.atZone(java.time.ZoneId.systemDefault()).toInstant().toEpochMilli(); } // Method to convert milliseconds to LocalDateTime public static LocalDateTime millisToTime(long millis) { return LocalDateTime.ofInstant(java.time.Instant.ofEpochMilli(millis), java.time.ZoneId.systemDefault()); } Meer informatie over tijdformaten in InfluxDB is te vinden in de officiële documentatie.\nMeasurements in Java Met de InfluxDB Java-client kun je gegevens schrijven naar een specifieke measurement. Hier is een voorbeeld:\nPoint point = Point.measurement(\u0026#34;CO2_concentration\u0026#34;) .addTag(\u0026#34;location\u0026#34;, \u0026#34;Room 101\u0026#34;) .addField(\u0026#34;value\u0026#34;, 450) .time(Instant.now(), WritePrecision.MS); writeApi.writePoint(point); Tags in Java Met de InfluxDB Java-client kun je tags toevoegen aan een gegevenspunt. Hier is een voorbeeld:\nPoint point = Point.measurement(\u0026#34;CO2_concentration\u0026#34;) .addTag(\u0026#34;location\u0026#34;, \u0026#34;Room 101\u0026#34;) .addField(\u0026#34;value\u0026#34;, 450) .time(Instant.now(), WritePrecision.MS); writeApi.writePoint(point); Fields in Java Met de InfluxDB Java-client kun je fields toevoegen aan een gegevenspunt. Hier is een voorbeeld:\nPoint point = Point.measurement(\u0026#34;CO2_concentration\u0026#34;) .addTag(\u0026#34;location\u0026#34;, \u0026#34;Room 101\u0026#34;) .addField(\u0026#34;value\u0026#34;, 450) .time(Instant.now(), WritePrecision.MS); writeApi.writePoint(point); Influx Queries in Java public static final InfluxDBClient influxDBClient = InfluxDBClientFactory.create(DB_URI, TOKEN.toCharArray(), ORG, BUCKET); public static void main(String[] args) { // Example usage LocalDateTime start = LocalDateTime.of(2023, 5, 1, 0, 0); LocalDateTime end = LocalDateTime.of(2023, 5, 7, 23, 59); List\u0026lt;FluxRecord\u0026gt; records = getPointsInRange(start, end); records.forEach(record -\u0026gt; System.out.println(record.getTime() + \u0026#34; -\u0026gt; \u0026#34; + record.getValueByKey(\u0026#34;_value\u0026#34;))); double sum = getSumOfValuesInRange(start, end); System.out.println(\u0026#34;Sum of values: \u0026#34; + sum); } public static List\u0026lt;MyClass\u0026gt; getPointsInRange(LocalDateTime start, LocalDateTime end) { QueryApi queryApi = influxDBClient.getQueryApi(); String fluxQuery = String.format( \u0026#34;from(bucket: \\\u0026#34;%s\\\u0026#34;) |\u0026gt; range(start: %s, stop: %s) |\u0026gt; filter(fn: (r) =\u0026gt; r._measurement == \\\u0026#34;my_measurement\\\u0026#34;)\u0026#34;, BUCKET, startDate.atZone(ZoneId.systemDefault()).toInstant().toString(), endDate.atZone(ZoneId.systemDefault()).toInstant().toString()); List\u0026lt;MyClass\u0026gt; myObjects = new ArrayList\u0026lt;\u0026gt;(); queryApi.query(fluxQuery, ORG).forEach(table -\u0026gt; table.getRecords().forEach(record -\u0026gt; { LocalDateTime time = LocalDateTime.ofInstant(record.getTime(), ZoneId.systemDefault()); double value = ((Number) record.getValueByKey(\u0026#34;_value\u0026#34;)).doubleValue(); myObjects.add(new MyClass(value, time)); })); return myObjects; } public static double getSumOfValuesInRange(LocalDateTime start, LocalDateTime end) { QueryApi queryApi = influxDBClient.getQueryApi(); String fluxQuery = String.format( \u0026#34;from(bucket: \\\u0026#34;%s\\\u0026#34;) \u0026#34; + \u0026#34;|\u0026gt; range(start: %s, stop: %s) \u0026#34; + \u0026#34;|\u0026gt; filter(fn: (r) =\u0026gt; r._measurement == \\\u0026#34;my_measurement\\\u0026#34;) \u0026#34; + \u0026#34;|\u0026gt; sum(column: \\\u0026#34;_value\\\u0026#34;)\u0026#34;, BUCKET, startDate.atZone(ZoneId.systemDefault()).toInstant().toString(), endDate.atZone(ZoneId.systemDefault()).toInstant().toString()); List\u0026lt;FluxRecord\u0026gt; records = new ArrayList\u0026lt;\u0026gt;(); queryApi.query(fluxQuery, ORG).forEach(table -\u0026gt; records.addAll(table.getRecords())); if (records.get(0).getValueByKey(\u0026#34;_value\u0026#34;) != null) { return (long) records.get(0).getValueByKey(\u0026#34;_value\u0026#34;); } else { return -1.0; } } Aggregaties in Java Met de InfluxDB Java-client kun je ook aggregaties uitvoeren. Hier is een voorbeeld:\nimport com.influxdb.client.*; import com.influxdb.query.FluxTable; import java.util.List; public class AggregationExample { public static void main(String[] args) { try (InfluxDBClient client = InfluxDBClientFactory.create(DB_URL, TOKEN.toCharArray())) { String flux = \u0026#34;from(bucket: \\\u0026#34;sensor_data\\\u0026#34;) \u0026#34; + \u0026#34;|\u0026gt; range(start: -7d) \u0026#34; + \u0026#34;|\u0026gt; filter(fn: (r) =\u0026gt; r[\\\u0026#34;_measurement\\\u0026#34;] == \\\u0026#34;temperature\\\u0026#34;) \u0026#34; + \u0026#34;|\u0026gt; mean()\u0026#34;; List\u0026lt;FluxTable\u0026gt; tables = client.getQueryApi().query(flux, ORG); tables.forEach(table -\u0026gt; table.getRecords().forEach(record -\u0026gt; { System.out.println(\u0026#34;Gemiddelde temperatuur: \u0026#34; + record.getValueByKey(\u0026#34;_value\u0026#34;)); })); } } } Met deze aanpak kun je aggregaties uitvoeren en analyseren in zowel de Flux-querytaal als in Java.\nInteressante videos Algemene tutorial over CMD-commands met InfluxDb Full overview en visualisatie met Graphana InfluxDb met Java 1 InfluxDb met Java 2 InfluxDb met VSCode: Flux Demo _Hier vind je een zipfolder met een oplossing voor InfluxDb demo CO2\nOefening: Maak een klein appje dat CO2 gegevens aanmaakt in een Java project en doorstuurt naar InfluxDb. Lees de waarden uit een zet een boolean op true wanneer CO2 een bepaalde hoeveelheid overschrijdt (= een raam open zetten). En zet weer op false wanneer de waarde weer daalt.\n"
},
{
	"uri": "http://localhost:1313/db-course/xml/basics/",
	"title": "XML Basics",
	"tags": [],
	"description": "",
	"content": "XML Data Storage XML staat voor Extensible Markup Language. Het is een taal die we gebruiken om met tags gegevens te structureren. Een tag opent zich op volgende manier: \u0026lt;boek\u0026gt; en sluit op deze manier: \u0026lt;/boek\u0026gt;. Je herkent misschien het gebruik van deze tags van HTML? Dat komt omdat HTML en XML allebei gegroeid zijn uit dezelfde taal (SGML).\nWaarom XML gebruiken? XML is nog steeds een vaak voorkomende manier om data te structureren en definiëren. Denk bijvoorbeeld aan webhook calls waarbij je kan kiezen tussen een response te krijgen in XML of JSON. Al krijgt JSON de laatste jaren meer en meer voorkeur omwille van zijn makkelijke en leesbare syntax, toch zijn er nog instanties waarbij XML de enige optie is.\nToepassingen van XML XML wordt vaak gebruikt in verschillende domeinen zoals:\nConfiguratiebestanden: Veel softwaretoepassingen gebruiken XML om configuratie-instellingen op te slaan. Data-uitwisseling: XML is platformonafhankelijk en wordt vaak gebruikt om gegevens tussen verschillende systemen te verzenden. Documentopslag: XML wordt gebruikt in formaten zoals Microsoft Office-documenten (.docx, .xlsx) en andere opmaakstandaarden zoals EPUB. Webservices: SOAP-webservices maken gebruik van XML voor het structureren van berichten. Voordelen van XML Menselijk leesbaar: XML is eenvoudig te begrijpen en te lezen door mensen. Flexibel: Het is uitbreidbaar en kan worden aangepast aan specifieke behoeften. Breed ondersteund: XML wordt ondersteund door een breed scala aan tools en programmeertalen. Hiërarchische structuur: Het biedt een natuurlijke manier om gegevens met een hiërarchische relatie te modelleren. Hoewel XML niet altijd de meest efficiënte keuze is, blijft het een krachtige en veelzijdige standaard voor gegevensrepresentatie.\nVoorbeeld \u0026lt;boeken\u0026gt; \u0026lt;boek\u0026gt; \u0026lt;titel\u0026gt;De gouden kooi\u0026lt;/titel\u0026gt; \u0026lt;auteur\u0026gt;A.F.Th. van der Heijden\u0026lt;/auteur\u0026gt; \u0026lt;jaar\u0026gt;1981\u0026lt;/jaar\u0026gt; \u0026lt;/boek\u0026gt; \u0026lt;boek\u0026gt; \u0026lt;titel\u0026gt;Het diner\u0026lt;/titel\u0026gt; \u0026lt;auteur\u0026gt;Herman Koch\u0026lt;/auteur\u0026gt; \u0026lt;jaar\u0026gt;2009\u0026lt;/jaar\u0026gt; \u0026lt;/boek\u0026gt; \u0026lt;boek\u0026gt; \u0026lt;titel\u0026gt;De ontdekking van de hemel\u0026lt;/titel\u0026gt; \u0026lt;auteur\u0026gt;Harry Mulisch\u0026lt;/auteur\u0026gt; \u0026lt;jaar\u0026gt;1982\u0026lt;/jaar\u0026gt; \u0026lt;/boek\u0026gt; \u0026lt;/boeken\u0026gt; Wat opvalt is dat we in XML maar 1 root element hebben. De \u0026lt;boeken\u0026gt; tag is het root element van dit voorbeeld en er is geen enkele andere tag op dit niveau. Daarbinnen kunnen we dan wel meerdere keren het \u0026lt;boek\u0026gt; element definiëren.\nOefening Voeg zelf een boek toe aan het bovenstaande voorbeeld Solution: Klik hier om de oplossing te zien/verbergen🔽\r\u0026lt;boeken\u0026gt; \u0026lt;boek\u0026gt; \u0026lt;titel\u0026gt;De gouden kooi\u0026lt;/titel\u0026gt; \u0026lt;auteur\u0026gt;A.F.Th. van der Heijden\u0026lt;/auteur\u0026gt; \u0026lt;jaar\u0026gt;1981\u0026lt;/jaar\u0026gt; \u0026lt;/boek\u0026gt; \u0026lt;boek\u0026gt; \u0026lt;titel\u0026gt;Het diner\u0026lt;/titel\u0026gt; \u0026lt;auteur\u0026gt;Herman Koch\u0026lt;/auteur\u0026gt; \u0026lt;jaar\u0026gt;2009\u0026lt;/jaar\u0026gt; \u0026lt;/boek\u0026gt; \u0026lt;boek\u0026gt; \u0026lt;titel\u0026gt;De ontdekking van de hemel\u0026lt;/titel\u0026gt; \u0026lt;auteur\u0026gt;Harry Mulisch\u0026lt;/auteur\u0026gt; \u0026lt;jaar\u0026gt;1982\u0026lt;/jaar\u0026gt; \u0026lt;/boek\u0026gt; \u0026lt;boek\u0026gt; \u0026lt;titel\u0026gt;Databases boek\u0026lt;/titel\u0026gt; \u0026lt;auteur\u0026gt;Kris Aerts\u0026lt;/auteur\u0026gt; \u0026lt;jaar\u0026gt;2024\u0026lt;/jaar\u0026gt; \u0026lt;/boek\u0026gt; \u0026lt;/boeken\u0026gt; Schrijf zelf een XML bestand dat studenten omschrijft. Volgende elementen moeten aanwezig zijn: studnr, naam, voornaam en goedbezig (boolean). Solution: Klik hier om de oplossing te zien/verbergen🔽\r\u0026lt;school\u0026gt; \u0026lt;student\u0026gt; \u0026lt;studnr\u0026gt;123\u0026lt;/studnr\u0026gt; \u0026lt;naam\u0026gt;Trekhaak\u0026lt;/naam\u0026gt; \u0026lt;voornaam\u0026gt;Jaak\u0026lt;/voornaam\u0026gt; \u0026lt;goedbezig\u0026gt;false\u0026lt;/goedbezig\u0026gt; \u0026lt;/student\u0026gt; \u0026lt;student\u0026gt; \u0026lt;studnr\u0026gt;456\u0026lt;/studnr\u0026gt; \u0026lt;naam\u0026gt;Peeters\u0026lt;/naam\u0026gt; \u0026lt;voornaam\u0026gt;Jos\u0026lt;/voornaam\u0026gt; \u0026lt;goedbezig\u0026gt;false\u0026lt;/goedbezig\u0026gt; \u0026lt;/student\u0026gt; \u0026lt;student\u0026gt; \u0026lt;studnr\u0026gt;890\u0026lt;/studnr\u0026gt; \u0026lt;naam\u0026gt;Dongmans\u0026lt;/naam\u0026gt; \u0026lt;voornaam\u0026gt;Ding\u0026lt;/voornaam\u0026gt; \u0026lt;goedbezig\u0026gt;true\u0026lt;/goedbezig\u0026gt; \u0026lt;/student\u0026gt; \u0026lt;/school\u0026gt; "
},
{
	"uri": "http://localhost:1313/db-course/bigdata/datawarehousing/",
	"title": "Data Warehousing &amp; BI",
	"tags": [],
	"description": "",
	"content": "Tot nu toe hebben we ons toegelegd op het zo optimaal mogelijk bewaren en ophalen van data\u0026mdash;rekening houdend met integriteit en anderen ACID/BASE principes. Maar wat zijn we hier nu allemaal mee, los van een werkende applicatie? In dit hoofdstuk gaan we data benaderen vanuit business perspectief.\nEen bedrijf kan gebaseerd op de miljoenen eenheden data dat het verzameld, op verschillende plekken en in verschillende formaten, beter beslissingen nemen. Strategische business beslissingen worden meestal op verschillende niveau\u0026rsquo;s genomen:\nNiveau 1 noemen we het operationeel niveau. Hier worden dagelijkse beslissingen genomen op korte termijn, vaak zonder al te veel naar de toekomst te kijken. Denk maar aan verschillende verkopen, implementatie beslissingen vanuit business perspectief, enzovoort. Niveau 2 noemen we het tactisch niveau. Middle management probeert hier op middellange termijn (bijvoorbeeld een maand/kwartaa/jaar) een vooruitblik te doen en gebaseerd daarop een partnerschap aan te gaan of af te wijzen. Niveau 3 noemen we het strategisch niveau. Senior management neemt beslissingen op dit niveau om het bedrijf op lange termijn (5 jaar of misschien zelfs langer) op koers te houden. Stel dat jij een bedrijf van 100 man hebt. Hoe beslis je waarin te investeren, om binnen 5 jaar niet failliet te zijn, maar misschien uit te groeien tot een bedrijf van 150 man? Hier kan data helpen met beslissen. De term Business Intelligence (BI) slaat hier op: de set van activiteiten, technieken, en tools om (1) patronen in data van het verleden te herkennen en (2) voorspellingen te maken naar te toekomst toe.\nEen bijkomend probleem is dat je groot bedrijf enorm veel data heeft:\nHet genereert dagelijks facturen en sales; Werknemers gebruiken de tikklok die weer data voorstelt; Je eindgebruikers interageren met jullie software, wat misschien verspreid zit over verschillende systemen; De helpdesk beheert tickets; HR houdt indexen van lonen bij en iemand beheert de fleet van bedrijfswagens; \u0026hellip; Het is duidelijk dat één simpele SELECT sales FROM income niet voldoende gaat zijn voor het management om beslissingen te helpen maken. We hebben een data warehouse nodig.\nsrc: sqlhammer.com\rProbeer op bovenstaand schema, een voorbeeld van hoe moderne data warehouses werken in een groot bedrijf, alle verschillende componenten te identificeren. Links staat de data die binnenkomt, en rechts het resultaat van de analyse. We bespreken hieronder kort elk blok. Experimentatie met de praktische kant, door middel van libraries als Apache Hadoop, is een optionele oefening voor de student.\nWe beginnen met de grote blauwe blok: een \u0026ldquo;data warehouse\u0026rdquo;.\nEen Data Warehouse (DW) Wat is een data warehouse? Volgens de uitvinder, Bill Inmon:\nA data warehouse is a (1) subject-oriented, (2) integrated, (3) time-variant, and (4) non-volatile collection of data in support of management\u0026rsquo;s decision-making process.\nDat klinkt ingewikkelder dan het is:\nSubject-oriented; de data is gecentreerd rond subjecten ofwel klanten, producten, sales, etc. De data is niet gecentreerd rond transacties of applicaties. Het moet managers helpen een beslissing maken, niet developers helpen ontwikkelen. Integrated; het is één grote structuur met als input verschillende andere DB sources met elk hun eigen formaat. Hier is dus conversie voor nodig: zie verder. Time-variant; een data warehouse bewaart data als een series van snapshots: bij intervallen van bijvoorbeeld elk kwartaal wordt data vernieuwd. Non-volatile; data is voornamelijk read-only (na initieel inladen): de belangrijkste operaties zijn dus loading \u0026amp; retrieval. Bekijk de volgende schematische weergave van een typische data warehouse:\ngraph BT;\rWH[(Data Warehouse)]\rRDBMS1[[RDBMS 1]]\rRDBMS2[[RDBMS 2]]\rXML[/XML\\]\rMAIL[/E-MAIL\\]\rETL1{{ETL}}\rETL2{{ETL}}\rETL3{{ETL}}\rETL4{{ETL}}\rETL1 --\u003e WH\rETL2 --\u003e WH\rETL3 --\u003e WH\rETL4 --\u003e WH\rRDBMS1 --\u003e ETL1\rRDBMS2 --\u003e ETL2\rXML --\u003e ETL3\rMAIL --\u003e ETL4\rAan de onderkant zie je verschillende bronnen van data\u0026mdash;zeer waarschijnlijk op zichzelf relationele of niet-relationele databases, XML, emails, HTML, andere losse CSV bestanden, \u0026hellip; Die moeten vanwege het integrated principe allemaal in één grote blok als snapshot worden bewaard zodat het management deze makkelijk kan queryen. Daarom moet er voor élke data source een ETL of Extraction, Transformation, and Loading proces worden opgestart: bepaalde attributen in de XML veranderen van structuur, de datum in RDBMS1 is opgeslaan als YYYY-MM-DD maar in RDBMS2 als DD-MM-YYYY, afrondingsverschillen worden weggewerkt, etc etc. Merk op dat niet altijd alle gegevens genormaliseerd worden. Een data warehouse kan dus best ook gedenormaliseerde data bevatten, maar dat komt minder frequent voor.\nEen data warehouse is dus geaggregeerde en opgekuiste data dankzij de ETLs. Er is vooral high-level data te vinden die kan gebruikt worden voor tactische, en vooral strategische beslissingen, maar minder voor operationele. Daarvoor kijk je best gewoon naar de bronnen zelf.\nHet opzetten van de ETL is waarschijnlijk 80% van het werk, dit is héél tijdsintensief!\nQ: Over hoeveel data gaat het in een data warehouse? A: Veel. Erg veel. Er zijn warehouses (vandaar ook \u0026ldquo;warenhuis\u0026rdquo; en niet zomaar \u0026ldquo;database\u0026rdquo; of \u0026ldquo;servertje\u0026rdquo;) van 12 petabytes, wat 12000 terabytes is. Bedenk hoeveel data wij elke minuut genereren: hoeveel tweets, hoeveel beurstransacties, hoeveel commits, \u0026hellip;\nIn sommige gevallen wenst men niet onmiddellijk alle data in de productie warehouse te bewaren, maar eest nog een staging area op te zetten. Deze warehouse staat voor de \u0026ldquo;echte\u0026rdquo; en en kan bijvoorbeeld worden gebruikt om intensieve machine learning algoritmes op te laten draaien zonde dat de performantie van de effectieve warehouse in het gedrang komt. Zo\u0026rsquo;n staging warehouse noemen we een operational data store. (zie schema bovenaan: links van het Hadoop logo).\nVoor BI systemen om effectief te zijn, moet de data ook kwalitatief zijn. Gebaseerd op \u0026ldquo;rommel\u0026rdquo; een grote toekomstgerichte business planning maken is een recept voor mislukking\u0026mdash;en mogelijks faillisement. Dus: Garbage In, Garbage Out (GIGO). ETLs moeten correct zijn, en het is ook een kwestie van de juiste data op te nemen in je warehouse. Niet alles hoeft of kan van belang zijn.\nRDBMS vs DW Hoe verhoudt een data warehouse zich ten opzichte van een typisch transactionele database?\nTransactional system Data Warehouse Usage Day to day ops Decision support mngt Latency real-time periodic snapshots Design app-oriented subject-oriented Normalization normalized (sometimes also) denormalized data Manipulation insert/update/delete/select insert (once)/select Transaction mngt important less of a concern Type of queries many simple few complex, some ad-hoc Mini DWs: Data Marts In de praktijk is één gigantische data warehouse, afhankelijk van de grootte van het bedrijf, niet bruikbaar. Data snapshot storage wordt meestal in functionele stukken opgeslagen, misschien opgedeeld per business unit, zoals het bedrijf ook is ingedeeld:\ngraph BT;\rWH[(Data Warehouse)]\rDM1[(Mart 1: Sales)]\rDM2[(Mart 2: Accounting)]\rDM3[(Mart 3: Finances)]\rDM1 --\u003e WH\rDM2 --\u003e WH\rDM3 --\u003e WH\rElke mart wordt op zijn beurt gevoed via een ETL zoals het eerste schema in dit hoofdstuk.\nHet voordeel van zo\u0026rsquo;n opdeling in data marts is (1) focused content en (2) uiteraard performantiewinst. De managers van sales moeten niet onnodig in de accounting mart queries afvuren en omgekeerd, alhoewel het hoger management natuurlijk nog steeds graag een overzicht van alles heeft in bepaalde verslagen.\nData Lakes Soms zijn data warehouses en hun marts niet flexibel genoeg om de gigantische (en eindeloze) stroom aan data te kunnen opslaan. Vergeet niet dat ETLs ook veel processorkracht vereisten, en een snapshot maken maar op een vaste periode gebeurt. In dat geval is een data lake handig: letterlijk een meer waar alle inkomende data (relatief) ongestructureerd wordt ingedumpt (zie lichtblauwe balk op schema bovenaan).\nWanneer voedt men een lake en wanneer een warehouse? Een lake wordt vooral gebruikt voor native data\u0026mdash;in zijn ruw formaat. Hier is nog geen ETL aan bod gekomen. Als het over ruwe signaaldata gaat, clickstreams, social media feeds, server logs, etc, dan is een data lake interessanter.\nNet omdat dit allemaal ruwe data is, is het analyseren van die data werk voor specialisten: dit zijn de \u0026ldquo;advanced analytics\u0026rdquo; op het bovenstaande schema. Hier zijn data scientists mee bezig. De gemiddelde bedrijfsmanager heeft hier echter niets aan! Een data lake is dus niet voldoende om bedrijfsbeslissingen te kunnen helpen maken\u0026mdash;de voornaamste reden waarom we met warehousing bezig zijn. Vaak wordt data van een lake nog doorgesluisd naar een operational data store, die op zijn beurt data laat doordruppelen naar de productie warehouse. Het wordt ingewikkeld\u0026hellip;\nData warehousing wordt ook aangeboden in de cloud. Data lakes worden vaak in de cloud gehost om kosten te drukken aangezien hier nog grotere hoeveelheden data wordt bewaard. Een voorbeeld hiervan is Amazon Redshift, dat wordt gebruikt door Nasdaq. Ze verwerken er 70 miljoen records per dag: https://aws.amazon.com/solutions/case-studies/nasdaq-case-study/.\n"
},
{
	"uri": "http://localhost:1313/db-course/sql/rdbms-components/",
	"title": "Database Componenten",
	"tags": [],
	"description": "",
	"content": "1. Three Layer Architecture Logical Layer De Logical Layer is waar we bepalen hoe onze data gestructureerd wordt. Hier bepalen we wat voor data we bijhouden, hoe die data eruitziet en hoe die zich gedraagt ten op zichte van onze andere datamodellen.\nEnkele voorbeelden hiervan zijn:\nEen BOEK mag door maximum 0 of 1 PERSONEN ontleend worden. Een PERSOON mag meerdere BOEKEN ontlenen. Een PERSOON is een subtype van een GEBRUIKER. Oefening Hoe zou een database model van een bibliotheek eruit zien? Teken zelf eens uit hoe dit gemodelleerd zou kunnen worden. Hoe houdt ik bij dat een boek uitgeleend werd? Wat als ik ook andere dingen wil uitlenen uit de bibliotheek, zoals DVD\u0026rsquo;s of eBooks? Internal Layer De Internal Layer houdt zich bezig met alle details over hoe de data bewaard wordt. Sommige van de concepten die hier aan bod komen zijn de volgenden:\nIndex management Constraint definities (uniek, referentieel, \u0026hellip;) Gepartitioneerde tabellen \u0026hellip; Deze technologieën hebben allemaal te maken met performantie of data integriteit. We willen onze data op een zo\u0026rsquo;n performante manier opslaan én nog belangrijker op een zo performante manier terug ophalen.\nIndexatie Data wordt ongestructureerd bijgehouden in een tabel. Een tabel is eigenlijk niet meer dan een ongesorteerde lijst van gegevens. Bij elke nieuw element wordt dat aan het eind van de lijst toegevoegd.\nWat als we nu uit een lijst van miljoenen boeken de verzamelde werken van Tolkien willen ophalen? In plaats van door de hele lijst één voor één te gaan zoeken, kunnen we gelukkig gebruik maken van indexen.\nEen index is een inhoudstafel die we bijhouden van een bepaald aantal velden van onze tabel. Stel we hebben een Book tabel met miljoenen rijen, waaronder de volgende:\nid isbn title author price 1345 0765326353 The Way of Kings Brandon Sanderson 24.99 6789 0395177111 The Hobbit J.R.R. Tolkien 24.99 3240 0812511816 The Eye of the World Robert Jordan 24.99 8939 0358439191 The Lord of the Rings J.R.R. Tolkien 24.99 1230 0143111582 Dune Frank Herbert 24.99 Als we een index zouden leggen op de author kolom dan zou die volgende informatie kunnen bevatten:\nauthor id Brandon Sanderson 1345 Frank Herbert 1230 J.R.R. Tolkien 6789 J.R.R. Tolkien 8939 Robert Jordan 3240 Een index houdt een mini-tabel bij van de velden die aan de index worden toegevoegd in combinatie met het identity1 veld. Deze tabel is wel gesorteerd op de velden uit de index, wat zoekopdrachten versnelt.\nConstraints We kunnen onze tabellen behoeden van corrupte data te bevatten door constraints te gebruiken. Het ISBN veld is bijvoorbeeld een uniek veld en mag nooit een waarde bevatten die al gekend is in onze database. Hiervoor kunnen we een unique constraint toevoegen. Bij data insertion gaat de database zelf nakijken of deze value al bestaat en zo ja wordt het toevoegen van de record geblokkeerd. Zo behouden we onze data integriteit.\nAndere voorbeelden van constraints kunnen zijn:\nDe prijs van een boek moet groter dan 0 zijn Een boek kan niet verwijderd worden als het ooit werd uitgeleend Het ISBN-13 nummer moet 13 karakters hebben Het ISBN nummer moet - een aantal keren bevatten Een naam veld mag niet NULL (niet ingevuld) zijn Een email veld moet @ bevatten \u0026hellip; Hoe constraints in de praktijk worden toegevoegd aan data modellen wordt behandeld in het hoofdstuk SQL DDL \u0026amp; DML. Zie ook SQLite table-constraint syntax.\nGepartitioneerde tabellen Sommige tabellen in productie omgevingen bevatten immense hoeveelheden aan data. We spreken over een grootorde van meerdere miljarden rijen. Hoe groter een tabel wordt, hoe trager het wordt om data op te halen. Het maakt niet uit hoeveel indexen we hebben gelegd, of welke SSD schijven we onderliggend op de fysieke storage hebben zitten. Meer data gaat altijd gelijk staan aan tragere data retrieval.\nOm tegen te gaan dat we tabellen krijgen die té groot worden en we daar niet meer zinvol data van kunnen ophalen bestaan hier een paar oplossingen voor. Het partitioneren van tabellen is er eentje van. Archivatie is een andere oplossing.\nNeem als voorbeeld een bank, die elke overschrijving van een rekening moet bewaren. De overschrijving tabel zou kunnen gepartitioneerd worden op jaar. Zodat er nog steeds 1 tabel overschrijving is, maar waarbij we die op de fysieke schijf opsplitsen per jaar, en elke op bijvoorbeeld de recentste 3 jaren index management voor bijhouden. De andere jaren kunnen nog steeds opgevraagd worden maar niet met dezelfde performantie als de meest recente data.\nExternal Layer De External Layer is wat we van onze database laten zien aan de buitenwereld. Dit zijn views van data. Een view is een virtuele representatie van data. We schrijven een query op onze tabellen en bewaren deze query als een view. Op deze manier kunnen we garanderen aan integrerende applicaties dat onze data er steeds hetzelfde gaat uitzien en tevens beschermen van informatie die voor het integrerende systeem niet relevant is.\nIn onze bibliotheek kunnen we een aantal views osptellen op basis van de noden van de verschillende applicaties. In het online platform waar je boeken kan uitlenen is het niet nodig de informatie over een auteur als een aparte entiteit weer te geven. We kunnen de tabel van Authors dus verbergen en enkel een view aanbieden op niveau van Books die er als volgt zou uitzien:\nclassDiagram\rclass LendingAppBooks{\risbn: NVARCHAR\rtitle: NVARCHAR\rauthor: NVARCHAR\rprice: DECIMAL\rgenre: NVARCHAR\r}\rVoor de inventaris applicatie moeten we wel in staat zijn om nieuwe boeken en auteurs toe te voegen. Daar kunnen de views er dan als volgt uitzien:\nclassDiagram\rInventoryBooks \"1..*\" --\u003e \"1\" InventoryAuthors\rclass InventoryAuthors{\rid: INT\rname: NVARCHAR\rfirstName: NVARCHAR\r}\rclass InventoryBooks{\risbn: NVARCHAR\rtitle: NVARCHAR\rauthor: INT\rprice: DECIMAL\r}\r2. Catalog Dit is het hart van de de database. Dit bevat alle metadata die zich in de database bevindt. Onder andere, de tabellen; views; stored procedures; \u0026hellip;\nDe SQL standaard om deze informatie in te bewaren is in INFORMATION_SCHEMA. Niet alle SQL Database providers voldoen hier echter aan. SQLite doet dit niet en daar vind je die informatie in de tabel sqlite_master.\n3. Database Languages SQL is onderverdeeld in twee verschillende talen. Enerzijds heb je DDL (Data Definition Language). Dit gebruik je om de database structuur te wijzigen. Nieuwe tabellen toevoegen, indexen aanmaken, views verwijderen, \u0026hellip;\nAnderzijds heb je DML (Data Manipulation Language). Dit gebruik je voor alle CRUD (Create, Read, Update, Delete) acties op data. Hier gaan we in een volgend hoofdstuk verder op in gaan.\nEigenlijk niet het Identity veld, maar het veld dat in de Clustered Index zit van de tabel. Als er dan in de query meer informatie nodig is om op te halen kan die via die manier de rest van de data ophalen.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"
},
{
	"uri": "http://localhost:1313/db-course/sql-ddl-dml/dml/",
	"title": "DML",
	"tags": [],
	"description": "",
	"content": "Data Modification Language is de taal die we gebruiken om de data van onze database te bekijken of aan te passen. Met DML kunnen we CRUD operaties uitvoeren. Create, Read, Update en Delete.\nSELECT SELECT is het commando dat we gebruiken om data op te vragen uit de database.\nSELECT { DISTINCT } expression\rFROM table\r{ WHERE condition } LIKE operator LIKE wordt gebruikt om wildcard searches uit te voeren. Deze kan enkel gebruikt worden op alfanumerieke velden.\n% is een match anything character voor een onbeperkt aantal karakters (0 tot n). Zo matcht Gen% met volgende waardes: Gen, Genk, Gent, Genève, Genua, \u0026hellip;\n_ is een match anything character voor één karakter. Zo matcht Gen_ met volgende waardes: Genk, Gent, \u0026hellip; Maar niet met volgende waardes: Gen, Genève, Genua, \u0026hellip;\nNULL NULL is het ontbreken van een waarde. Het is een onbekende. We kunnen in DML niet zomaar vergelijken met NULL. We kunnen namelijk niet zeggen dat een onbekende gelijk is aan een andere onbekende.\nHieronder een overzicht van een binary AND table met True, False en NULL waardes.\nAND True False NULL True True False NULL False False False False NULL NULL False NULL Denkvraag: Waarom is False == NULL gelijk aan False?\nHieronder vinden we de binary OR table met True, False en NULL waardes.\nOR True False NULL True True True True False True False NULL NULL True NULL NULL Als we willen vergelijken met NULL in queries, dan gebruiken we volgende code:\n\u0026lt;value\u0026gt; IS NULL\nen\n\u0026lt;value\u0026gt; IS NOT NULL\nSchrijf een query die alle Customers (volledige naam, customer ID en land) laat zien die niet wonen in de USA. Schrijf een query die enkel de Customers laat zien die in Brazilië wonen. Schrijf een query die alle Employees laat zien die werken in de Sales afdeling. Schrijf een query die een unieke lijst van landen laat zien uit de Invoices tabel. Schrijf een query die alle Tracks laat zien waarvoor er geen componist bekend is. Schrijf een query van alle unieke Componisten. Als de componist niet bekend is, dan moet er \u0026lsquo;Onbekend\u0026rsquo; weergegeven worden gesorteerd op naam. Schrijf een query die het maximumbedrag van alle Invoices laat zien. JOIN Wanneer we informatie willen ophalen uit meerdere tabellen dan gebruiken we daar een JOIN statement voor. Die syntax ziet er als volgt uit:\nSELECT { DISTINCT } expression\rFROM table\rINNER JOIN other_table ON join_condition\r{ WHERE condition } Hiermee matchen we alle data van de ene tabel met de andere tabel op de meegegeven conditie. Er bestaan drie verschillende types JOINs:\nINNER JOIN - Geeft alle resultaten die bestaan zowel in de ene als de andere tabel LEFT JOIN - Geeft alle resultaten die bestaan in de base tabel, ook al bestaan die niet in de tabel waarop we joinen RIGHT JOIN - Wordt in de praktijk zelden tot nooit gebruikt. Geeft alle resultaten die bestaan in de gejoinde tabel ook al bestaan ze niet in de base tabel. Schrijf een query die alle Invoices laat zien van alle Customers uit Brazilië. Het resultaat moet de volledige naam van de Customer, Invoice ID, Invoice Datum en Billing Country weergeven. Schrijf een query die alle Invoices laat zien voor elke Sales Agent. Het resultaat moet de volledige naam van de Sales Agent weergeven. Schrijf een query die het Invoice Totaal, de Customer naam en land en de naam van de Sales Agent weergeeft voor alle Invoices en Customers. Schrijf een query die het aantal invoice lijnen weergeeft voor Invoice ID 37. Schrijf een query die de track naam weergeeft langs elke invoice lijn. Schrijf een query die de track naam en de artiest naam weergeeft langs elke invoice lijn. Schrijf een query die alle tracks laat zien, maar geen ID\u0026rsquo;s. Het resultaat moet de album titel, het media type en het genre bevatten. Schrijf een query die alle genres weergeeft waarvoor er geen tracks bestaan. GROUP BY Soms willen we data aggregeren. In Basic Engineering Skills in Python werd aggregratie gebruikt om bijvoorbeeld de som van een lijst te nemen, of met funtools.reduce() een custom functie los te laten op een lijst. (Dit gaan we ook nog zien in het hoofdstuk rond NoSQL \u0026ndash; Advanced map-reduce queries).\nIn RDBMS bestaan hiervoor een aantal verschillende functies. De meest courante zijn hieronder te vinden:\nMAX() MIN() COUNT() AVG() SUM() Elke waarde die je extra selecteert in een query bovenop een aggregate function, moet in een GROUP BY clause komen. Hoe ziet dit er dan bijvoorbeeld uit?\nSELECT BillingCity, SUM(Total) FROM Invoices GROUP BY BillingCity Zonder GROUP BY statement krijg je ofwel een fout ofwel maar één record terug, zoals in SQLite.\nHAVING Als we willen filteren op een grouping function, dan gaat dat niet via een WHERE clause, dan krijg je namelijk een foutmelding:\nSELECT BillingCity, count(*) FROM invoices WHERE count(*) \u0026gt; 2 GROUP BY BillingCity Om te filteren op een grouping function schrijven we dit in een HAVING clause die de query gebruikerlijks afsluit:\nSELECT BillingCity, count(*) FROM invoices GROUP BY BillingCity HAVING count(*) \u0026gt; 2 Schrijf een query die het aantal Invoices laat zien voor 2009 en 2011. Schrijf een query die het aantal invoices per land laat zien. Schrijf een query die per Invoice ID het aantal invoice lijnen laat zien. Schrijf een query die de naam van elke playlist laat zien, alsook het aantal tracks in elke playlist. Schrijf een query die alle data uit de Invoices tabel laat zien, aangevuld met het aantal invoice lijnen. Schrijf een query die de totale verkoopcijfers per Sales Agent laat zien. Schrijf een query die laat zien welke Sales Agent de grootste verkoopcijfers heeft voor 2009. Schrijf een query die laat zien welke Sales Agent de grootste verkoopcijfers heeft voor 2010. Schrijf een query die laat zien welke Sales Agent de grootste verkoopcijfers heeft over alle jaren heen. Schrijf een query die het aantal Customers laat zien per Sales Agent. Schrijf een query die de totale verkoopcijfers per land laat zien. In welk land werd er het meest uitgegeven? Schrijf een query die laat zien welke track er in 2013 het meest werd verkocht. Schrijf een query die laat zien wat de top 5 tracks zijn die ooit werden verkocht. Schrijf een query die laat zien wie de top 3 artiesten zijn die het meest verkocht werden. Schrijf een query die laat zien welk media type het meest verkocht werd. Schrijf een query die de tracks laat zien die meer dan 4 keer verkocht zijn. Subqueries Een query die we uitvoeren geeft een set van resultaten terug. Die set kunnen we opnieuw gebruiken als input voor een nieuwe query. We kunnen die set op verschillende plaatsen gebruiken als input voor een nieuwe query. Hieronder een aantal voorbeelden.\nIn een WHERE clause SELECT * FROM invoice_items WHERE invoice_items.TrackId IN ( SELECT tracks.TrackId FROM tracks WHERE name LIKE \u0026#39;%hell%\u0026#39; ) In een FROM clause SELECT * FROM ( SELECT tracks.TrackId, tracks.Name FROM tracks WHERE name LIKE \u0026#39;%hell%\u0026#39; ) In een JOIN clause SELECT * FROM tracks INNER JOIN ( SELECT tracks.TrackId, tracks.Name FROM tracks WHERE name LIKE \u0026#39;%hell%\u0026#39; ) hell_tracks ON hell_tracks.TrackId = tracks.TrackId Subqueries in een WHERE IN statement worden geëvauleerd voor elke voor elke rij uit de outer query, dus zijn eigenlijk niet zo heel performant. We kunnen dat iets verbeteren door dat te herschrijven naar een WHERE EXISTS statement. Zie hieronder.\nSELECT * FROM invoice_items WHERE EXISTS ( SELECT 1 FROM tracks WHERE name LIKE \u0026#39;%hell%\u0026#39; AND tracks.TrackId = invoice_items.TrackId ) De IN clause gaat een subquery volledig ophalen om alle rijen te hebben om dan in die lijst van rijen te kunnen zoeken. Een EXISTS clause gaat een subquery maar zo lang uitvoeren tot er een resultaat gevonden is. Als de tabel uit de subquery 1000 rijen bevat en er wordt een match gevonden op rij 200, dan gaan de andere 800 niet meer geëvauleerd worden.\nDe meeste van deze queries kunnen ook geschreven worden met een JOIN statement. Dit is echter niet waar we hier op willen oefenen. Los dus volgende oefeningen op met minstens één subquery. Als je hier moeite mee hebt kan her handig zijn om eerst een werkende query te bekomen met JOIN en die dan om te vormen naar een subquery.\nSchrijf een query die alle invoices laat zien die een track bevatten van Iron Maiden. Schrijf een query die alle invoices laat zien die verkocht werden door Margaret Park. Schrijf een query die alle genres laat zien waarvoor er geen track bestaat. Schrijf een query die alle invoices laat zien waarvan de prijs groter is dan het gemiddelde van alle invoices. Schrijf een query die alle invoices laat zien waarin een Metallica track verkocht is, waarvan de prijs groter is dan het gemiddelde van alle invoices waarin een Metallica track verkocht is. Data manipulatie INSERT Met een INSERT Statement gaan we data toevoegen in de database. We gebruiken een column listing om aan te geven welke kolommen, in welke volgorde, we van een waarde kan voorzien. Kolommen die NULL values ondersteunen mogen uit de column listing gelaten worden.\nINSERT INTO Genres(Name) VALUES(\u0026#39;Rock\u0026#39;) Voeg je favoriete album (inclusief artiest en tracks) toe aan de database.\nUPDATE Met een UPDATE statement kunnen we één of meerdere waardes in een set van data aanpassen.\nUPDATE Tracks SET MediaTypeId = 1 WHERE AlbumId = 2 Wijzig de UnitPrice en de Composer voor de 3e track van je toegevoegde album. Wijzig de titel van je favoriete album (zie oefening hierboven).\nDELETE Hiermee kunnen we een set van data verwijderen.\nLET OP! Een DELETE statement zonder WHERE clause verwijdert alles uit de tabel!\nDELETE FROM Genre WHERE Name = \u0026#39;Rock\u0026#39; Verwijder het album (inclusief artiest en tracks) dat je hierboven hebt toegevoegd.\nWat is volgens jou het verschil tussen DELETE FROM zonder WHERE en DROP TABLE?\n"
},
{
	"uri": "http://localhost:1313/db-course/nosql/documentstores/",
	"title": "Document Stores met MongoDB",
	"tags": [],
	"description": "",
	"content": "MongoDB Introductie Wat is MongoDB en hoe verschilt het van andere soorten databases? MongoDB is een NoSQL gedistribueerde database. Omdat gegevens niet binnen de strikte grenzen van een relationeel model hoeven te passen, kan MongoDB functioneren als een algemene gegevensopslag. Dit biedt verschillende voordelen.\nIn MongoDB worden gegevens opgeslagen in een flexibel schema. Als de behoeften van je applicatie veranderen, kun je eenvoudig de structuur van je gegevens aanpassen. Dankzij schema-validatie kun je bepalen hoe strikt of flexibel je schema moet zijn. Dit maakt MongoDB geschikt voor uiteenlopende databehoeften.\nIn relationele databases worden relaties tussen gegevens in verschillende tabellen gerealiseerd via joins. In hiërarchische databases zijn relaties tussen knooppunten vaak onmogelijk. MongoDB biedt echter de mogelijkheid om documenten te koppelen via operaties zoals $lookup of door middel van referenties.\nDaarnaast heeft MongoDB geen single point of failure, wat betekent dat het systeem robuuster is. Bovendien ondersteunt MongoDB transacties, wat de atomische uitvoering van lees- en schrijfbewerkingen over meerdere documenten garandeert. Dit is vooral handig bij complexe query\u0026rsquo;s over meerdere documenten.\nMongoDB is ontworpen voor applicaties in het internet era, waar gebruikers gegevens vanaf verschillende locaties kunnen manipuleren. Met ingebouwde ondersteuning voor replicatie, load balancing en aggregatie is MongoDB een veelzijdig onderdeel van moderne softwarearchitectuur.\nVoordelen van MongoDB Atlas: Cloud omgeving MongoDB Atlas is een multi-cloud documentdatabaseservice. Het is een volledig beheerde dienst die wordt uitgevoerd door een team van MongoDB-systeembeheerders, zodat jij je kunt richten op je eigen applicatie. MongoDB Atlas is beschikbaar op cloudproviders zoals AWS, Microsoft Azure en Google Cloud Platform, waardoor het flexibel inzetbaar is.\nBedrijven die MongoDB gebruiken Enkele bekende bedrijven die MongoDB gebruiken zijn eBay, Forbes en FEMA.\nInstallatie Om MongoDB te installeren:\nDownload de communityversie via MongoDB Community Download. Deze versie biedt krachtige manieren om gegevens te analyseren en te bevragen, inclusief ondersteuning voor ad-hoc queries, secundaire indexering en real-time aggregaties. Installeer ook de GUI-tool MongoDB Compass om je gegevens visueel te beheren. Volg de installatiehandleiding: Install MongoDB op Windows. Deze handleiding biedt stapsgewijze instructies voor het installeren van MongoDB Community Edition op Windows. (Bekijk de installatievideo: MongoDB Installatie op Windows.) Voor gebruik in VSCode kun je de extensie \u0026ldquo;MongoDB for VSCode\u0026rdquo; installeren. Hiermee kun je gegevens exporteren naar bijvoorbeeld Java code.\nOverzicht van Gebruik MongoDB is een NoSQL-database zonder vast schema. Gegevens worden opgeslagen in BSON (Binary JSON), een JSON-formaat met uitgebreide datatypes. In MongoDB worden tabellen \u0026ldquo;collecties\u0026rdquo; genoemd en rijen \u0026ldquo;documenten\u0026rdquo;. Elk document heeft een unieke _id.\nVoorbeeld Een voorbeeld van een collectie met 3 documents in MongoDB:\n[ { \u0026#34;_id\u0026#34;: \u0026#34;00000020f51bb4362eee2a4d\u0026#34;, \u0026#34;studnr\u0026#34;: 123, \u0026#34;naam\u0026#34;: \u0026#34;Trekhaak\u0026#34;, \u0026#34;voornaam\u0026#34;: \u0026#34;Jaak\u0026#34;, \u0026#34;goedBezig\u0026#34;: false, \u0026#34;opleiding\u0026#34;: { \u0026#34;naam\u0026#34;: \u0026#34;IIW\u0026#34;, \u0026#34;keuze\u0026#34;: \u0026#34;Informatica\u0026#34; }, \u0026#34;vakken\u0026#34;: [\u0026#34;DAB\u0026#34;, \u0026#34;SES\u0026#34;, \u0026#34;FSWEB\u0026#34;] }, { \u0026#34;_id\u0026#34;: \u0026#34;507f191e810c19729de860ea\u0026#34;, \u0026#34;studnr\u0026#34;: 456, \u0026#34;naam\u0026#34;: \u0026#34;Peeters\u0026#34;, \u0026#34;voornaam\u0026#34;: \u0026#34;Jos\u0026#34;, \u0026#34;goedBezig\u0026#34;: false, \u0026#34;opleiding\u0026#34;: { \u0026#34;naam\u0026#34;: \u0026#34;IIW\u0026#34;, \u0026#34;keuze\u0026#34;: \u0026#34;EA-ICT\u0026#34; }, \u0026#34;vakken\u0026#34;: [\u0026#34;DAB\u0026#34;, \u0026#34;SES\u0026#34;] }, { \u0026#34;_id\u0026#34;: \u0026#34;6592008029c8c3e4dc76256c\u0026#34;, \u0026#34;studnr\u0026#34;: 890, \u0026#34;naam\u0026#34;: \u0026#34;Dongmans\u0026#34;, \u0026#34;voornaam\u0026#34;: \u0026#34;Ding\u0026#34;, \u0026#34;goedBezig\u0026#34;: true, \u0026#34;vakken\u0026#34;: [\u0026#34;DAB\u0026#34;] } ] Merk op dat voor de derde student de Opleiding ontbreekt, dit kan dus in NoSql document stores. Het is dan wel belangrijk dat je software hier rekening mee houdt!\nBelangrijke Commando\u0026rsquo;s Hieronder een overzicht van veelgebruikte commando\u0026rsquo;s in MongoDB:\n# Versie controleren $ mongosh --version # Verbinden met een database $ mongosh \u0026#34;\u0026lt;connection string\u0026gt;\u0026#34; --username \u0026lt;username\u0026gt; # Lokale verbinding $ mongosh # IN MONGOSH # Databases weergeven show dbs # Database selecteren use \u0026lt;db name\u0026gt; # Database verwijderen db.dropDatabase() # MongoSH beëindigen exit # Collectie aanmaken db.createCollection(\u0026#34;\u0026lt;collection name\u0026gt;\u0026#34;) # Document toevoegen db.\u0026lt;collection name\u0026gt;.insertOne(\u0026lt;json\u0026gt;) # Meerdere documenten toevoegen db.\u0026lt;collection name\u0026gt;.insertMany(\u0026lt;jsonArray\u0026gt;) # Documenten opvragen db.\u0026lt;collection name\u0026gt;.find() db.\u0026lt;collection name\u0026gt;.find({\u0026lt;keyname\u0026gt;: \u0026#34;\u0026lt;value\u0026gt;\u0026#34;}) db.\u0026lt;collection name\u0026gt;.find({\u0026lt;keyname\u0026gt;: {\u0026lt;operation\u0026gt;}}) db.\u0026lt;collection name\u0026gt;.find()[0]._id db.\u0026lt;collection name\u0026gt;.find(…).sort({\u0026lt;key\u0026gt;: 1}) or -1 for descending order db.\u0026lt;collection name\u0026gt;.find(…).count() db.\u0026lt;collection name\u0026gt;.find(…).limit(2) db.\u0026lt;collection name\u0026gt;.find(…).skip(\u0026lt;nr\u0026gt;) db.\u0026lt;collection name\u0026gt;.findOne({\u0026lt;keyname\u0026gt;: {\u0026lt;operation\u0026gt;}}) # Operation example: {$gt: 3} # other operators: $gt, $lt, $gte, $lte, $eq, $in, $exists: true, # Documenten bijwerken db.\u0026lt;collection name\u0026gt;.updateOne({\u0026lt;criteria\u0026gt;}, {$set: {\u0026lt;key\u0026gt;: \u0026lt;value\u0026gt;}}) # criteria: search criteria like you use in find # without $set the whole document is replaced with the new JSON and not only the corresponding values change db.\u0026lt;collection name\u0026gt;.updateMany() # Documenten verwijderen db.\u0026lt;db name\u0026gt;.deleteOne(\u0026lt;query\u0026gt;) db.\u0026lt;db name\u0026gt;.deleteMany(\u0026lt;query\u0026gt;) db.\u0026lt;db name\u0026gt;.replaceOne(\u0026lt;query\u0026gt;) db.\u0026lt;db name\u0026gt;.replaceMany(\u0026lt;query\u0026gt;) Meer informatie over query- en updateoperators is te vinden in de MongoDB-documentatie.\nAggregaties MongoDB ondersteunt aggregaties, waarmee je complexe bewerkingen op gegevens kunt uitvoeren. Aggregaties zijn een krachtige manier om gegevens te transformeren en te analyseren. Ze worden vaak gebruikt om samenvattingen, statistieken of andere berekeningen uit te voeren op grote datasets. Aggregaties worden uitgevoerd met behulp van een aggregatie-pijplijn, die bestaat uit een reeks stappen die gegevens verwerken en transformeren.\nEnkele veelgebruikte aggregatiestappen zijn:\n$match: Filteren van gegevens op basis van criteria. $sort: Sorteren van gegevens op een specifieke volgorde. $project: Selecteren en transformeren van specifieke velden in documenten. $group: Groeperen van gegevens en uitvoeren van berekeningen zoals som, gemiddelde of telling. $limit: Beperken van het aantal resultaten. $skip: Overslaan van een bepaald aantal resultaten. Een voorbeeld van een aggregatie in Java en in de console.\nAggregatie in Java:\nimport com.mongodb.client.MongoClient; import com.mongodb.client.MongoClients; import com.mongodb.client.MongoCollection; import com.mongodb.client.MongoDatabase; import org.bson.Document; import java.util.Arrays; public class AggregationExample { public static void main(String[] args) { String uri = \u0026#34;mongodb://localhost:27017\u0026#34;; try (MongoClient mongoClient = MongoClients.create(uri)) { MongoDatabase database = mongoClient.getDatabase(\u0026#34;mijnDatabase\u0026#34;); MongoCollection\u0026lt;Document\u0026gt; collectie = database.getCollection(\u0026#34;mijnCollectie\u0026#34;); collectie.aggregate(Arrays.asList( new Document(\u0026#34;$match\u0026#34;, new Document(\u0026#34;date\u0026#34;, new Document(\u0026#34;$gte\u0026#34;, \u0026#34;2014-01-01\u0026#34;).append(\u0026#34;$lt\u0026#34;, \u0026#34;2015-01-01\u0026#34;))), new Document(\u0026#34;$group\u0026#34;, new Document(\u0026#34;_id\u0026#34;, \u0026#34;$item\u0026#34;) .append(\u0026#34;totalSaleAmount\u0026#34;, new Document(\u0026#34;$sum\u0026#34;, new Document(\u0026#34;$multiply\u0026#34;, Arrays.asList(\u0026#34;$price\u0026#34;, \u0026#34;$quantity\u0026#34;))))), new Document(\u0026#34;$sort\u0026#34;, new Document(\u0026#34;totalSaleAmount\u0026#34;, -1)), new Document(\u0026#34;$limit\u0026#34;, 5) )).forEach(doc -\u0026gt; System.out.println(doc.toJson())); } } } Aggregatie in de console (mongosh)\nuse mijnDatabase; db.mijnCollectie.aggregate([ { $match: { date: { $gte: new Date(\u0026#39;2014-01-01\u0026#39;), $lt: new Date(\u0026#39;2015-01-01\u0026#39;) } } }, { $group: { _id: \u0026#34;$item\u0026#34;, totalSaleAmount: { $sum: { $multiply: [ \u0026#34;$price\u0026#34;, \u0026#34;$quantity\u0026#34; ] } } } }, { $sort: { totalSaleAmount: -1 } }, { $limit: 5 } ]); Let wel op! De volgorde bij aggregaties zeer belangrijk is: Stel dat je in bovenstaande voorbeeld eerst een $limit 5 zou doen en daarna pas een $match, dan zou je dus enkel matchen zoeken binnen de 5 documenten die nog over hebt na je voorgaande $limit commando.\nMeer informatie over aggregaties is te vinden in de MongoDB-documentatie.\nSchema-validatie Schema-validatie in MongoDB biedt een manier om de structuur van documenten in een collectie te definiëren en te valideren. Dit helpt om de integriteit van gegevens te waarborgen en fouten te voorkomen. Met schema-validatie kun je regels instellen voor de velden in een document, zoals het type, de vereiste velden en de toegestane waarden.\nHoe schema-validatie aanbrengen? Schema-validatie wordt ingesteld op het niveau van een collectie. Bij het aanmaken van een collectie kun je een validatieregelspecificatie opgeven met behulp van het validator-veld. Dit veld bevat een query die de validatieregels definieert.\nSyntax voor schema-validatie in Java Hier is een voorbeeld van hoe je schema-validatie kunt instellen in Java:\nimport com.mongodb.client.MongoClient; import com.mongodb.client.MongoClients; import com.mongodb.client.MongoDatabase; import org.bson.Document; public class SchemaValidationExample { public static void main(String[] args) { String uri = \u0026#34;mongodb://localhost:27017\u0026#34;; try (MongoClient mongoClient = MongoClients.create(uri)) { MongoDatabase database = mongoClient.getDatabase(\u0026#34;mijnDatabase\u0026#34;); Document schema = new Document(\u0026#34;bsonType\u0026#34;, \u0026#34;object\u0026#34;) .append(\u0026#34;required\u0026#34;, List.of(\u0026#34;naam\u0026#34;, \u0026#34;leeftijd\u0026#34;)) .append(\u0026#34;properties\u0026#34;, new Document(\u0026#34;naam\u0026#34;, new Document(\u0026#34;bsonType\u0026#34;, \u0026#34;string\u0026#34;) .append(\u0026#34;description\u0026#34;, \u0026#34;Naam is verplicht en moet een string zijn.\u0026#34;)) .append(\u0026#34;leeftijd\u0026#34;, new Document(\u0026#34;bsonType\u0026#34;, \u0026#34;int\u0026#34;) .append(\u0026#34;minimum\u0026#34;, 0) .append(\u0026#34;description\u0026#34;, \u0026#34;Leeftijd is verplicht en moet een positief geheel getal zijn.\u0026#34;))); database.createCollection(\u0026#34;mijnCollectie\u0026#34;, new Document(\u0026#34;validator\u0026#34;, new Document(\u0026#34;$jsonSchema\u0026#34;, schema))); } } } Syntax voor schema-validatie in de console (mongosh)\nuse mijnDatabase; db.createCollection(\u0026#34;mijnCollectie\u0026#34;, { validator: { $jsonSchema: { bsonType: \u0026#34;object\u0026#34;, required: [\u0026#34;naam\u0026#34;, \u0026#34;leeftijd\u0026#34;], properties: { naam: { bsonType: \u0026#34;string\u0026#34;, description: \u0026#34;Naam is verplicht en moet een string zijn.\u0026#34; }, leeftijd: { bsonType: \u0026#34;int\u0026#34;, minimum: 0, description: \u0026#34;Leeftijd is verplicht en moet een positief geheel getal zijn.\u0026#34; } } } } }); Validatie aanpassen Je kunt de validatieregels van een bestaande collectie aanpassen met het commando collMod:\ndb.runCommand({ collMod: \u0026#34;mijnCollectie\u0026#34;, validator: { $jsonSchema: { bsonType: \u0026#34;object\u0026#34;, required: [\u0026#34;naam\u0026#34;, \u0026#34;leeftijd\u0026#34;], properties: { naam: { bsonType: \u0026#34;string\u0026#34; }, leeftijd: { bsonType: \u0026#34;int\u0026#34;, minimum: 0 } } } } }); Validatie uitschakelen Als je schema-validatie tijdelijk wilt uitschakelen, kun je de validatieactie instellen op \u0026ldquo;warn\u0026rdquo; in plaats van \u0026ldquo;error\u0026rdquo;:\ndb.runCommand({ collMod: \u0026#34;mijnCollectie\u0026#34;, validationAction: \u0026#34;warn\u0026#34; }); Met deze aanpak kun je schema-validatie flexibel toepassen en aanpassen aan de behoeften van je applicatie.\nMongoDB Compass MongoDB Compass is een grafische gebruikersinterface (GUI) waarmee je eenvoudig je MongoDB-databases kan beheren en verkennen. Met Compass kan je:\nDatabases en collecties bekijken: Je kan de structuur van je database en collecties visueel inspecteren. Query\u0026rsquo;s uitvoeren: Compass biedt een intuïtieve interface om query\u0026rsquo;s te schrijven en resultaten te bekijken. Documenten bewerken: Je kan individuele documenten in je collecties bekijken, bewerken en verwijderen. Indexen beheren: Compass laat je indexen bekijken en beheren om de prestaties van je database te optimaliseren. Aggregaties uitvoeren: Met de ingebouwde aggregatie-pipeline-builder kan je complexe aggregaties maken en testen. Om MongoDB Compass te gebruiken:\nStart Compass en verbind met je MongoDB-database door de connection string in te voeren. Navigeer door je databases en collecties in de linkernavigatiebalk. Gebruik de zoekbalk om query\u0026rsquo;s te schrijven en resultaten te filteren. Klik op een document om het te bewerken of te verwijderen. Meer informatie over MongoDB Compass is te vinden op de officiële website.\nMongoDB VSCode Extensie De MongoDB VSCode-extensie integreert MongoDB-functionaliteit direct in Visual Studio Code. Hiermee kun je:\nVerbinden met MongoDB: Maak verbinding met een lokale of cloudgebaseerde MongoDB-database. Gegevens verkennen: Bekijk databases, collecties en documenten in een boomstructuur. Query\u0026rsquo;s uitvoeren: Schrijf en voer query\u0026rsquo;s uit in een geïntegreerde editor. Gegevens exporteren: Exporteer gegevens naar JSON of andere formaten. Code genereren: Genereer codefragmenten voor je applicatie, zoals Java of Node.js. Om de extensie te gebruiken:\nInstalleer de extensie \u0026ldquo;MongoDB for VSCode\u0026rdquo; via de VSCode Marketplace. Open de MongoDB Explorer in de zijbalk. Voeg een nieuwe verbinding toe door de connection string in te voeren. Navigeer door je databases en voer query\u0026rsquo;s uit in de editor. Meer informatie over de extensie is te vinden op de VSCode Marketplace.\nMongoDB in Java Om een verbinding te maken met een MongoDB-database in Java, heb je de volgende Gradle-dependency nodig:\ndependencies { implementation \u0026#39;com.google.code.gson:gson:2.10.1\u0026#39; implementation \u0026#39;org.mongodb:mongodb-driver-sync:4.9.0\u0026#39; } Hier is een voorbeeld van hoe je een verbinding kunt maken en een eenvoudige CRUD-operatie kunt uitvoeren:\nimport com.mongodb.client.*; import org.bson.Document; public class MongoDBExample { public static void main(String[] args) { // Verbinden met de MongoDB-database String uri = \u0026#34;mongodb://localhost:27017\u0026#34;; try (MongoClient mongoClient = MongoClients.create(uri)) { MongoDatabase database = mongoClient.getDatabase(\u0026#34;mijnDatabase\u0026#34;); MongoCollection\u0026lt;Document\u0026gt; collectie = database.getCollection(\u0026#34;mijnCollectie\u0026#34;); // Create: Document toevoegen Document nieuwDocument = new Document(\u0026#34;naam\u0026#34;, \u0026#34;John Doe\u0026#34;) .append(\u0026#34;leeftijd\u0026#34;, 30) .append(\u0026#34;beroep\u0026#34;, \u0026#34;Software Engineer\u0026#34;); collectie.insertOne(nieuwDocument); // Read: Documenten opvragen for (Document doc : collectie.find()) { System.out.println(doc.toJson()); } // Update: Document bijwerken collectie.updateOne(new Document(\u0026#34;naam\u0026#34;, \u0026#34;John Doe\u0026#34;), new Document(\u0026#34;$set\u0026#34;, new Document(\u0026#34;leeftijd\u0026#34;, 31))); // Delete: Document verwijderen collectie.deleteOne(new Document(\u0026#34;naam\u0026#34;, \u0026#34;John Doe\u0026#34;)); } } } Meer informatie kan je hier in de documentatie terugvinden!\nCRUD-commando\u0026rsquo;s Hier zijn de belangrijkste commando\u0026rsquo;s die je nodig hebt om een kleine CRUD-applicatie te maken:\nCreate:\ncollectie.insertOne(new Document(\u0026#34;key\u0026#34;, \u0026#34;value\u0026#34;)); Read:\nfor (Document doc : collectie.find()) { System.out.println(doc.toJson()); } Update:\ncollectie.updateOne(new Document(\u0026#34;key\u0026#34;, \u0026#34;value\u0026#34;), new Document(\u0026#34;$set\u0026#34;, new Document(\u0026#34;key\u0026#34;, \u0026#34;newValue\u0026#34;))); Delete:\ncollectie.deleteOne(new Document(\u0026#34;key\u0026#34;, \u0026#34;value\u0026#34;)); Met deze tools en voorbeelden kun je eenvoudig aan de slag met MongoDB in Java en andere ontwikkelomgevingen.\nJava Objecten en JSON Om een Java-object om te zetten naar JSON en omgekeerd, kun je gebruik maken van Gson. Deze bibliotheek maakt het eenvoudig om objecten te serialiseren en deserialiseren. Hier is een voorbeeld met de klasse Student en het bijbehorende JSON-document.\nGradle Dependencies Voor Gson:\ndependencies { implementation \u0026#39;com.google.code.gson:gson:2.10.1\u0026#39; implementation \u0026#39;org.mongodb:mongodb-driver-sync:4.9.0\u0026#39; } Klasse Student Hier is een voorbeeld van hoe je de klasse Student kunt definiëren:\nimport com.google.gson.annotations.SerializedName; import org.bson.types.ObjectId; import java.util.List; public class Student { @SerializedName(\u0026#34;_id\u0026#34;) private ObjectId id; private int studnr; private String naam; private String voornaam; private boolean goedBezig; private Opleiding opleiding; private List\u0026lt;String\u0026gt; vakken; // Getters en setters public static class Opleiding { private String naam; private String keuze; // Getters en setters } } Object naar JSON en JSON naar Object Hier is een voorbeeld van hoe je een Student-object kunt omzetten naar JSON en hoe je JSON kunt omzetten naar een Student-object met Gson:\nimport com.google.gson.Gson; import com.google.gson.GsonBuilder; import org.bson.Document; public class JsonExample { public static void main(String[] args) { Gson gson = new GsonBuilder().create(); // Voorbeeld: Student-object naar JSON Student student = new Student(); student.setStudnr(123); student.setNaam(\u0026#34;Trekhaak\u0026#34;); student.setVoornaam(\u0026#34;Jaak\u0026#34;); student.setGoedBezig(false); Student.Opleiding opleiding = new Student.Opleiding(); opleiding.setNaam(\u0026#34;IIW\u0026#34;); opleiding.setKeuze(\u0026#34;Informatica\u0026#34;); student.setOpleiding(opleiding); student.setVakken(List.of(\u0026#34;DAB\u0026#34;, \u0026#34;SES\u0026#34;, \u0026#34;FSWEB\u0026#34;)); // Object naar JSON String json = gson.toJson(student); System.out.println(\u0026#34;JSON: \u0026#34; + json); // JSON naar Object String jsonString = \u0026#34;{\\\u0026#34;_id\\\u0026#34;:\\\u0026#34;00000020f51bb4362eee2a4d\\\u0026#34;,\\\u0026#34;studnr\\\u0026#34;:123,\\\u0026#34;naam\\\u0026#34;:\\\u0026#34;Trekhaak\\\u0026#34;,\\\u0026#34;voornaam\\\u0026#34;:\\\u0026#34;Jaak\\\u0026#34;,\\\u0026#34;goedBezig\\\u0026#34;:false,\\\u0026#34;opleiding\\\u0026#34;:{\\\u0026#34;naam\\\u0026#34;:\\\u0026#34;IIW\\\u0026#34;,\\\u0026#34;keuze\\\u0026#34;:\\\u0026#34;Informatica\\\u0026#34;},\\\u0026#34;vakken\\\u0026#34;:[\\\u0026#34;DAB\\\u0026#34;,\\\u0026#34;SES\\\u0026#34;,\\\u0026#34;FSWEB\\\u0026#34;]}\u0026#34;; Student deserializedStudent = gson.fromJson(jsonString, Student.class); System.out.println(\u0026#34;Student: \u0026#34; + deserializedStudent.getNaam()); } } Opslaan en Ophalen in MongoDB Hier is een voorbeeld van hoe je een Student-object kunt opslaan in en ophalen uit een MongoDB-database met Gson:\nimport com.google.gson.Gson; import com.google.gson.GsonBuilder; import com.mongodb.client.MongoClient; import com.mongodb.client.MongoClients; import com.mongodb.client.MongoCollection; import com.mongodb.client.MongoDatabase; import org.bson.Document; public class MongoDBJsonExample { public static void main(String[] args) { String uri = \u0026#34;mongodb://localhost:27017\u0026#34;; Gson gson = new GsonBuilder().create(); try (MongoClient mongoClient = MongoClients.create(uri)) { MongoDatabase database = mongoClient.getDatabase(\u0026#34;mijnDatabase\u0026#34;); MongoCollection\u0026lt;Document\u0026gt; collectie = database.getCollection(\u0026#34;studenten\u0026#34;); // Student-object naar JSON en opslaan in MongoDB Student student = new Student(); student.setStudnr(123); student.setNaam(\u0026#34;Trekhaak\u0026#34;); student.setVoornaam(\u0026#34;Jaak\u0026#34;); student.setGoedBezig(false); Student.Opleiding opleiding = new Student.Opleiding(); opleiding.setNaam(\u0026#34;IIW\u0026#34;); opleiding.setKeuze(\u0026#34;Informatica\u0026#34;); student.setOpleiding(opleiding); student.setVakken(List.of(\u0026#34;DAB\u0026#34;, \u0026#34;SES\u0026#34;, \u0026#34;FSWEB\u0026#34;)); String json = gson.toJson(student); Document document = Document.parse(json); collectie.insertOne(document); // Ophalen uit MongoDB en JSON naar Student-object Document dbDocument = collectie.find().first(); if (dbDocument != null) { String dbJson = dbDocument.toJson(); Student deserializedStudent = gson.fromJson(dbJson, Student.class); System.out.println(\u0026#34;Ophalen uit MongoDB: \u0026#34; + deserializedStudent.getNaam()); } } } } Replicatie Bij gebruik van MongoDB Atlas is replicatie standaard inbegrepen. Dit zorgt voor een hogere beschikbaarheid en betrouwbaarheid. Replicatie in MongoDB wordt gerealiseerd via een replica set, een groep mongod-processen die dezelfde dataset onderhouden. Een replica set bevat meerdere data-dragers en optioneel een arbiter.\nReplicatie houdt in dat gegevens automatisch worden gekopieerd van de primaire node naar secundaire nodes. Dit biedt niet alleen fouttolerantie, maar verhoogt ook de leescapaciteit, omdat leesbewerkingen naar secundaire nodes kunnen worden gestuurd. In het geval van een storing van de primaire node, wordt een secundaire node automatisch gepromoveerd tot primaire node via een verkiezingsproces. Dit garandeert dat de database beschikbaar blijft, zelfs bij hardware- of netwerkproblemen.\nMeer informatie over replicatie is te vinden in de MongoDB-documentatie en in deze video.\nHet concept van Foreign Keys in MongoDb In document stores gaan we hier zo min mogelijk van gebruik maken, maar het kan wel geïmplementeerd worden. We gaan daar hier echter niet verder op in en daarvoor kijk je best de videos over dit onderwerp hieronder.\nInteressante videos Algemene tutorial over CMD-commands, Mongo Atlas, Aggregations, MongoDb Compass en VSCode extention Tutorial met meer complexe queries Meer advanced tutorial over Mongo Atlas, auto replication, ACID \u0026amp; Transaction, aggregations, schema-validation, relationships, join, indexes Praktisch gebruik van Foreign keys in MongoDb Tutorial over het gebruik van MongoDb in Java inclusief aggregaties Tutorial over de VSCode extensie Extra video over de installatie van de GUI voor mongoDb (Compass) op Windows Extra video over Replication Extra tutorial voor beginners 1 Extra tutorial voor beginners 2 Demo Hier vind je een zipfolder met een oplossing voor MongoDb demo Student\n"
},
{
	"uri": "http://localhost:1313/db-course/apis/jdbc/",
	"title": "JDBC",
	"tags": [],
	"description": "",
	"content": "Java Database Connectivity (JDBC) Hoe verbind ik Java met de DB? JDBC is een interface in de JDK die ons in staat stelt om een connectie te openen naar een database. JDBC is een API: een abstracitelaag of een protocol. Dit betekent dat we met JDBC kunnen verbinden naar eender welke server van eender welke flavor: een Oracle SQL, MSSQL, of SQLite database. De database vendor wordt verborgen achter de JDBC laag. Voor deze oefeningen beperken we ons tot MySQL.\nVoor elke database moet er dus een vendor-specifieke driver als dependency worden toegevoegd. In het geval van MySQL is dit de mysql-jdbc driver, de mysql-jdbc package. JDBC zelf leeft in java.sql en is een integraal onderdeel van de JDK: dit moeten we dus niet apart oplijsten als dependency of downloaden.\ngraph LR;\rJava[Java]\rJDBC[JDBC]\rMYSQL[MySQL-JDBC]\rDB[(MySQL Database)]\rsubgraph Java space\rsubgraph JDK\rJava -.-\u003e JDBC\rend\rJDBC --\u003e MYSQL\rend\rsubgraph DB space\rMYSQL --\u003e DB\rend\rDe mysql-jdbc package zorgt voor de brug tussen onze Java applicatie en de database, maar we spreken die aan via JDBC.\nGradle dependency:\n// https://mvnrepository.com/artifact/mysql/mysql-connector-java implementation \u0026#39;mysql:mysql-connector-java:5.1.6\u0026#39; Enkele belangrijke statements:\nEen connectie naar een database vastleggen: var connection = DriverManager.getConnection(\u0026#34;jdbc:mysql://localhost:3306/\u0026lt;database_name\u0026gt;\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;\u0026#34;); Wil je meerdere queries tegelijk uitvoeren dan moet je dit ook nog specifiek vermelden in de driver door ?allowMultiQueries=true toe te voegen aan de database url bv: \u0026quot;jdbc:mysql://localhost:3306/\u0026lt;database_name\u0026gt;?allowMultiQueries=true\u0026quot;\n2. Een SELECT query uitvoeren:\nvar s = connection.createStatement(); var result = s.executeQuery(\u0026#34;...\u0026#34;); var cell = result.getString(\u0026#34;\u0026lt;column_name\u0026gt;\u0026#34;); Een INSERT/UPDATE/\u0026hellip; query uitvoeren (die de structuur of inhoud van de database wijzigt): var s = connection.createStatement(); s.executeUpdate(\u0026#34;...\u0026#34;); Een voorbeeld Het volgende voorbeeld opent een verbinding naar een DB, maakt een tabel aan, voegt een record toe, en telt het aantal records:\npublic static void main(String[] args) { try { App.createDb(); App.verifyDbContents(); } catch (SQLException e) { e.printStackTrace(); } } public static void createDb() throws SQLException { var connection = DriverManager.getConnection(\u0026#34;jdbc:mysql://localhost:3306/school?allowMultiQueries=true\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;\u0026#34;); var s = connection.createStatement(); s.executeUpdate(\u0026#34;\u0026#34;\u0026#34; DROP TABLE IF EXISTS student; CREATE TABLE student( studnr INT NOT NULL PRIMARY KEY, naam VARCHAR(200) NOT NULL, voornaam VARCHAR(200), goedbezig BOOLEAN ); INSERT INTO student (studnr, naam, voornaam, goedbezig) VALUES (123, \u0026#39;Trekhaak\u0026#39;, \u0026#39;Jaak\u0026#39;, 0), (456, \u0026#39;Peeters\u0026#39;, \u0026#39;Jos\u0026#39;, 0), (890, \u0026#39;Dongmans\u0026#39;, \u0026#39;Ding\u0026#39;, 1); \u0026#34;\u0026#34;\u0026#34;); s.close(); connection.close(); } public static void verifyDbContents() throws SQLException { var connection = DriverManager.getConnection(\u0026#34;jdbc:mysql://localhost:3306/school?allowMultiQueries=true\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;\u0026#34;); var s = connection.createStatement(); var result = s.executeQuery(\u0026#34;SELECT COUNT(*) as cnt FROM student;\u0026#34;); while (result.next()) { System.out.println(\u0026#34;Assert that number of rows is 3: \u0026#34; + (result.getInt(\u0026#34;cnt\u0026#34;) == 3)); assert result.getInt(\u0026#34;cnt\u0026#34;) == 1; } s.close(); connection.close(); } Merk op dat SQLException een checked exception is die je constant moet meespelen in de method signature of expliciet moet opvangen. Het probleem van een try { } catch { } finally { } block is dat in de finally je ook geen close() kan uitvoeren zonder opnieuw een try block te openen\u0026hellip; Inception!\nHet connection.close() statement moet er voor zorgen dat voor elke request de connection netjes wordt afgesloten. Een database heeft meestal een connection pool van x aantal beschikbare connections, bijvoorbeeld 5. Als een connection per request niet wordt gesloten, heeft de volgende bezoeker van onze website geen enkele kans om zijn search query te lanceren, omdat de database dan zegt dat alle connecties zijn opgebruikt!\nMerk op dat de String jdbc:mysql://localhost:3306/school?allowMultiQueries=true een connectie met je MariaDB aanmaakt en de meegegeven database, zodat je met PHPmyAdmin data kan inspecteren. Indien je een tabel aanmaakt de eerste keer, gaat dit de tweede keer crashen met table already exists. Houd hier dus rekening mee (e.v.t. met IF NOT EXISTS).\nConstant CREATE TABLE() statements issuen (bv. voor testing), vervuilen je broncode. Als je veel SQL moet uitvoeren is het beter om dit in een .sql bestand te bewaren in src/main/resources en eenmalig als een String in te lezen met:\nURI create_tables_path = Objects.requireNonNull(App.class.getClassLoader().getResource(\u0026#34;create_tables.sql\u0026#34;)).toURI(); String create_tables_sql = new String(Files.readAllBytes(Paths.get(create_tables_path))); Die String kan je dan als Query executen in een statement!\nQueries/Objecten in JDBC Stel dat we het eerste voorbeeld van een school database willen uitbreiden en studenten die in de database opgeslagen zijn willen inladen in een Student klasse instantie: van de TABLE STUDENT naar de class Student. In geval van JDBC is dat veel handwerk:\nMaak een verbinding met de database. Voer de SELECT statements uit. Loop door de ResultSet en maak een nieuwe Student instantie aan. Vang alle mogelijke fouten zelf op: wat met lege kolommen, null? Wat met INTEGER kolommen die je wilt mappen op een String property? Om van de huidige resultatenrij naar de volgende te springen in ResultSet gebruikt men de methode next() in een typisch while() formaat:\nvar result = statement.executeQuery(\u0026#34;SELECT * FROM \u0026lt;table_name\u0026gt;\u0026#34;); while(result.next()) { var eenString = result.getString(\u0026#34;\u0026lt;column_name\u0026gt;\u0026#34;); // doe iets! } Zie ook ResultSet Oracle Javadoc.\nDemos We breiden bovenstaand voorbeeld uit:\nWe maken hier weer gebruik van Gradle, maar aangezien onze database op onze Windows host draait gaan we geen verbinding kunnen maken via onze WSL. Daarvoor kan je je bestanden aanmaken in je WSL en dan de projectmap kopiëren naar je windows file explorer. Dan kan je de Gradle wrapper voor Windows gebruiken ./gradlew.bat. Of je kan Gradle voor windows installeren. De stappen daarvoor vind je hier.\nAlles in de main Om dingen te doen met de database moeten we dus een aantal stappen doorlopen, die hier beschreven en gecodeerd zijn in een main-method:\npublic static void main(String[] args){ try{ // CONNECT TO MYSQL var connection = DriverManager.getConnection(\u0026#34;jdbc:mysql://localhost:3306/school?allowMultiQueries=true\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;\u0026#34;); // CREATE THE TABLES var statement = connection.createStatement(); statement.executeUpdate(\u0026#34;\u0026#34;\u0026#34; DROP TABLE IF EXISTS student; CREATE TABLE student( studnr INT NOT NULL PRIMARY KEY, naam VARCHAR(200) NOT NULL, voornaam VARCHAR(200), goedbezig BOOLEAN ); INSERT INTO student (studnr, naam, voornaam, goedbezig) VALUES (123, \u0026#39;Trekhaak\u0026#39;, \u0026#39;Jaak\u0026#39;, 0), (456, \u0026#39;Peeters\u0026#39;, \u0026#39;Jos\u0026#39;, 0), (890, \u0026#39;Dongmans\u0026#39;, \u0026#39;Ding\u0026#39;, 1); \u0026#34;\u0026#34;\u0026#34;); statement.close(); //VERIFY DATABASE CONTENT statement = connection.createStatement(); var result = statement.executeQuery(\u0026#34;SELECT COUNT(*) as cnt FROM student;\u0026#34;); while (result.next()) { System.out.println(\u0026#34;Assert that number of rows is 3: \u0026#34; + (result.getInt(\u0026#34;cnt\u0026#34;) == 3)); assert result.getInt(\u0026#34;cnt\u0026#34;) == 3; } //READ FROM DB statement = connection.createStatement(); result = statement.executeQuery(\u0026#34;SELECT * FROM student;\u0026#34;); while (result.next()){ System.out.println(\u0026#34;Studnr: \u0026#34;+result.getInt(\u0026#34;studnr\u0026#34;)); } //UPDATE DB statement = connection.createStatement(); statement.executeUpdate(\u0026#34;UPDATE student SET voornaam = \u0026#39;Jaqueline\u0026#39; WHERE studnr = 123;\u0026#34;); // OPTIONAL VERIFY UPDATE WITH A READ // Closing all connections correctly result.close(); statement.close(); connection.close(); }catch (Exception e){ e.printStackTrace(); } } Opsplitsen in verschillende methoden Je merkt onmiddellijk dat deze code onoverzichtelijk is, je kan dus beter verschillende methoden aanmaken en dan oproepen in de main-method. We splitsen op in:\npublic static void connectToDbMysql(String connectionString, String user, String pwd){...} public static void createDbMysql(Connection connection, Statement statement){...} public static void verifyDbContents(Connection connection, Statement statement){...} public static ResultSet readFromDb(Connection connection, Statement statement, String query){...} public static void updateDb(Connection connection, Statement statement, String updateStr){...} public static void closeAllConnections(Connection connection, Statement statement){...} Dit laten we als een oefening voor de student.\nOpsplitsen van verantwoordelijkheden in verschillende klasses Wanneer we nu met meerdere databases en meerdere tabellen werken gaat het niet meer overzichtelijk zijn om alles in dezelfde klasse te doen, daarom gaan we verantwoordelijkheden opsplitsen in verschillende klassen:\nConnectionManager klasse die instaat voor de verbinding met de database. \u0026lt;naam\u0026gt;Repository klasse die instaat voor de logica die te maken heeft met queries uitvoeren die over een bepaalde model klasse gaan bv. We hebben een klasse Student en een tabel student dan komt in de StudentRepository alle code om onder andere data uit de studententabel op te halen en om te vormen tot echte student objecten in Java. In de main komt dan alles samen, data ophalen uit database en omvormen tot Java objecten en dan die objecten gebruiken. We maken en testen een klasse StudentRepository die de volgende methode implementeert. Zoals je ziet is het de bedoeling dat de JDBC Connection instance elders wordt aangemaakt, bijvoorbeeld in een aparte ConnectionManager klasse.\npublic class StudentRepository { public StudentRepository(Connection connection); public List\u0026lt;Student\u0026gt; getStudentsByName(String name); } Hier vind je een zipfolder met een oplossing voor onderstaande oefening: dashboard voor database met enkel studenten tabel\nEnkele vragen/oefeningen:\nBreid de StudentRepository-klasse uit met de volgende methoden (CREATE, READ, UPDATE, DELETE = CRUD): public class StudentRepository { public StudentRepository(Connection connection); public List\u0026lt;Student\u0026gt; getAllStudents(); public void addStudentToDb(Student student); public Student getStudentsByStudnr(int studnr); public void updateStudentInDb(Student student); public void deleteStudentInDb(int studnr); } Hoe zou je bovenstaande StudentRepository unit (integratie) testen, zonder de \u0026ldquo;productie database\u0026rdquo; op te vullen met testdata? (Hint: kijk naar het constructor argument). Hoe kan je getStudentsByName() testen zonder de volgende oefening afgewerkt te hebben, die nieuwe studenten bewaren pas mogelijk maakt? Breid dit uit met addStudentToDb(Student). Breid dit uit met updateStudentInDb(Student). Wat moet je doen als deze student nog niet in de database zit? Welke gegevens update je wel en welke niet? Merk op dat elke keer als je je project opstart je geen CREATE TABLE student kan uitvoeren als je een file-based SQLite bestand hanteert: eens de tabel is aangemaakt geeft een nieuwe create foutmeldingen. DROP TABLE IF EXISTS student; lost dit op, maar daardoor ben je ook altijd je data kwijt. Hoe los je dit probleem op? Stel dat een Student is ingeschreven in een Vak met properties naam (vb. \u0026ldquo;databases\u0026rdquo;) en ects (vb. 4). Maak een VakRepository om nieuwe vakken te bewaren. Hoe link je de Student klasse met de Vak klasse? wat verandert er in de query van getStudentsByName()? BELANGRIJK:\nexecuteUpdate() van een Statement is erg omslachtig als je een string moet samenstellen die een INSERT query voorstelt (haakjes, enkele quotes, \u0026hellip;). Wat meer is, als de input van een UI komt, kan dit gehacked worden, door zelf de quote te sluiten in de string. Dit noemt men SQL Injection, en om dat te vermijden gebruik je in JDBC de prepareStatement() methode. Zie JDBC Basics: Prepared Statements. De String die je meegeeft bevat in de plaats van parameters een vraagteken: INSERT INTO STUDENT(bla, bla) VALUES(?, ?). Die parameters vul je daarna aan met preparedStatement.setString() of setInt(). Op die manier is de code zowel netjes als injectie-vrij! Als je data wenst op te halen dat is verspreid over verschillende tabellen, is de kans groot dat een JOIN SQL statement nodig is. Probeer eerst de query te schrijven in de PhpMyAdmin tool. De Java objecten opvullen is de laatste taak. Bijvoorbeeld:\n... try { String stmt = \u0026#34;INSERT INTO student(naam, voornaam, studnr, goedbezig) VALUES (?, ?, ?, ?)\u0026#34;; System.out.println(stmt); PreparedStatement prepared = connection.prepareStatement(stmt); prepared.setString(1, \u0026#34;Verboven\u0026#34;); prepared.setString(2, \u0026#34;Mark\u0026#34;); prepared.setInt(3, 666); prepared.setBoolean(4, false); prepared.execute(); prepared.close(); } catch(SQLException ex) { throw new RuntimeException(ex); } ... Jdbc met SQLite Een SQLite database kan handig zijn omdat een lokale .db-file al als database kan dienen wat de overhead van moeilijke connecties kan verminderen. Hiervoor moeten we dan een aantal dingen aanpassen:\nDe driver dependency wordt nu: implementation 'org.xerial:sqlite-jdbc:3.42.0.0' De Connection met een lokale database file kan je dan maken met: var connection = (Connection) DriverManager.getConnection(\u0026quot;jdbc:sqlite:mydatabase.db\u0026quot;); Indien de databasefile nog niet bestaat wordt deze aangemaakt. Let op! De import van connection mag nu niet de import zijn van de Mysql dependency maar wordt: import java.sql.Connection; Let op dat je nu ook correcte SQLite syntax gebruikt in je queries, maar voor de rest zal alles gelijkaardig werken. EXTRA: In memory database Je kan ook een in-memory database aanmaken, die volledig in RAM leeft en bij elke opstart opnieuw wordt aangemaakt, met de String jdbc:sqlite::memory:, dus zonder database naam. (Hiervoor gebruiken we dan de sqlite JDBC-connector, hier gaan we in deze cursus echter niet verder op in.)\nWerk je met een andere database maar heb je geen idee hoe die speciale connection string te vormen? Geen probleem, daarvoor dient https://www.connectionstrings.com/. Bijvoorbeeld, een connectie naar de Microsoft Azure cloud kan met de volgende syntax:\nServer=tcp:myserver.database.windows.net,1433;Database=myDataBase;User ID=mylogin@myserver;Password=myPassword;Trusted_Connection=False;Encrypt=True; Het is de connection string die bepaalt welke dependency binding gebruikt wordt! Dit noemen we late binding: er is geen expliciete referentie naar iets van MySQL in de Java code; we werken enkel met JDBC zelf. Als je de vendor driver vergeet toe te voegen als Gradle dependency gebeurt er dit:\nException in thread \u0026#34;main\u0026#34; java.sql.SQLException: No suitable driver found for jdbc:mysql://localhost:3306/school\rat java.sql/java.mysql.DriverManager.getConnection(DriverManager.java:702)\rat java.sql/java.mysql.DriverManager.getConnection(DriverManager.java:251)\rat Demo.main(Demo.java:8) EER-schema/database mapping naar Java Objects Om dit te verduidelijken en in te oefenen gaan we de demo database wat uitbreiden zodat er ook one-to-many en many-to-many relationships in voorkomen. We voegen opleidingen toe en elke student volgt één opleiding en een opleiding heeft dus meerdere studenten. We voegen ook vakken toe wat een many-to-many relatie oplevert aangezien een student meerdere vakken kan volgen en een vak meerdere studenten kan hebben.\nKlik hier voor de sql code van deze uitgebreidere database🔽\rDROP TABLE IF EXISTS student_volgt_vak; DROP TABLE IF EXISTS student; DROP TABLE IF EXISTS vak; DROP TABLE IF EXISTS opleiding; CREATE TABLE opleiding( id INT NOT NULL PRIMARY KEY, opleidingsnaam VARCHAR(200) NOT NULL ); CREATE TABLE student( studnr INT NOT NULL PRIMARY KEY, naam VARCHAR(200) NOT NULL, voornaam VARCHAR(200), goedbezig BOOLEAN, opleiding INT DEFAULT NULL, FOREIGN KEY (opleiding) REFERENCES opleiding(id) ON DELETE CASCADE ON UPDATE CASCADE ); CREATE TABLE vak( vaknr INT NOT NULL PRIMARY KEY, vaknaam VARCHAR(200) NOT NULL ); CREATE TABLE student_volgt_vak( id INT AUTO_INCREMENT PRIMARY KEY, student INT, vak INT, FOREIGN KEY (student) REFERENCES student(studnr) ON DELETE CASCADE ON UPDATE CASCADE, FOREIGN KEY (vak) REFERENCES vak(vaknr) ON DELETE CASCADE ON UPDATE CASCADE ); INSERT INTO opleiding(id, opleidingsnaam) VALUES (1, \u0026#39;IIW\u0026#39;); INSERT INTO student(studnr, naam, voornaam, goedbezig, opleiding) VALUES (123, \u0026#39;Trekhaak\u0026#39;, \u0026#39;Jaak\u0026#39;, 0, 1); INSERT INTO student(studnr, naam, voornaam, goedbezig, opleiding) VALUES (456, \u0026#39;Peeters\u0026#39;, \u0026#39;Jos\u0026#39;, 0, 1); INSERT INTO student(studnr, naam, voornaam, goedbezig, opleiding) VALUES (890, \u0026#39;Dongmans\u0026#39;, \u0026#39;Ding\u0026#39;, 1, NULL); INSERT INTO vak(vaknr, vaknaam) VALUES (1, \u0026#39;DAB\u0026#39;); INSERT INTO vak(vaknr, vaknaam) VALUES (2, \u0026#39;SES\u0026#39;); INSERT INTO vak(vaknr, vaknaam) VALUES (3, \u0026#39;FSWEB\u0026#39;); INSERT INTO student_volgt_vak(student, vak) VALUES (123, 1); INSERT INTO student_volgt_vak(student, vak) VALUES (123, 2); INSERT INTO student_volgt_vak(student, vak) VALUES (123, 3); INSERT INTO student_volgt_vak(student, vak) VALUES (456, 1); INSERT INTO student_volgt_vak(student, vak) VALUES (456, 2); INSERT INTO student_volgt_vak(student, vak) VALUES (890, 1); Hoe kan je die relaties nu zichtbaar maken in Java. Dit is vrij eenvoudig, een Student kan meerdere vakken volgen dus krijgt de student klasse een Lijst met vakken. Een student behoort ook tot een Opleiding, dus die opleiding wordt ook een datamember voor Student. Analoog zullen de klassen Vak en Opleiding een lijst van studenten hebben en zal een vak ook een opleiding als datamember hebben.\nNu zal je zeker zeggen maar zijn we dan niet veel data aan het dupliceren en is dat niet waarom we een SQL database gebruiken. Dan ben je helemaal correct. Het verschil met de Java klassem/objecten is dat we enkel die objecten aanmaken die we op dat moment nodig hebben en die halen we uit \u0026hellip; je raad het al: de database. Dus het is volledig ok om zoveel data in een object op te slaan omdat we nooit alle data van de database in Java Objeten omvormen, we weten op voorhand bijvoorbeeld welke student we willen bekijken en kunnen dus specifiek voor die student een object aanmaken en de rest blijft gewoon in de database staan tot we het nodig hebben!\nGrotere oefening Breid de oefening van hierboven uit met alle data van de nieuwe database:\nBreid de Student klasse uit met een lijst van vakken en een opleiding. // Template van de klasse Student public class Student { private int studnr; private String voornaam, achternaam; private boolean goedBezig; private Opleiding opleiding; private ArrayList\u0026lt;Vak\u0026gt; vakken; // Constructors // Getters en Setters // To String // Equals // Hash } Maak een klasse voor Vak en Opleiding. // Template van de klasse Vak public class Vak { private int vaknr; private String naam; private ArrayList\u0026lt;Student\u0026gt; studenten; // Constructors // Getters en Setters // To String // Equals // Hash } // Template van de klasse Opleiding public class Opleiding { private String naam; private ArrayList\u0026lt;Student\u0026gt; inschrijvingen; // Constructors // Getters en Setters // To String // Equals // Hash } Merk op dat wanneer je uit de database een Student ophaalt, dat die een lijst van vakken wil hebben. Dat Vak zou dan ook een lijst van studenten hebben, die weer een lijst van Vakken hebben \u0026hellip; Je weet zelf wel dat dit oneindig blijft doorgaan en dus voor problemen zal zorgen. Neem die relaties dan ook niet op in de contrutor van Student bijvoorbeeld. Initieer bij de constructor van Student de lijst vakken gewoon als een lege lijst en geef voor de Opleiding voorlopig null. Voorzie dan methoden waarmee je later die waarden met de correct values kan populeren zoals addVak(Vak vak) en setOpleiding(Opleiding opleiding). Idem voor de Vak en Opleiding klassen.\nVoorzie nu voor de verschillende corresponderende Repository-klassen waar alle logica in te staan komt om de data over de verschillende klassen uit de database te halen en om te vormen tot Java objecten. (Je moet dus je StudentRepository-klasse aanpassen, een VakRepository-klasse en een OpleidingRepository-klasse aanmaken) Om de lijsten op te stellen kan je best van handige SQL-queries gebruik maken. Hier vind je enkele voorbeelden: -- Krijg alle studenten die behoren tot een opleiding (vervang ? door een opleiding id) SELECT s.* FROM student s WHERE s.opleiding = ?; -- Krijg alle vakken waarvoor een specifieke student is ingeschreven (vervang ? door een studnr) SELECT v.* FROM student_volgt_vak svv JOIN vak v ON svv.vak = v.vaknr WHERE svv.student = ?; -- Krijg alle studenten die behoren tot een vak (vervang ? door een vaknr) SELECT s.* FROM student_volgt_vak svv JOIN student s ON svv.student = s.studnr WHERE svv.vak = ?; Maak van je App.java een soort command line administratie systeem waarmee je een aantal dingen kan doen (Zorg er natuurlijk voor dat alle wijzigingen ook doorgevoerd worden in de database): voor een gegeven student opvragen voor welke vakken hij/zij is ingeschreven. voor een gegeven student opvragen voor welke opleiding hij/zij is ingeschreven. voor een gegeven vak opvragen welke studenten ingeschreven zijn. voor een gegeven opleiding opvragen welke studenten ingeschreven zijn. studenten uitschrijven voor een vak. studenten inschrijven voor een vak. studenten uitschrijven voor een opleiding. studenten inschrijven voor een opleiding. studenten, vakken en opleidingen aanmaken. "
},
{
	"uri": "http://localhost:1313/db-course/sql-ddl-dml/",
	"title": "SQL DDL &amp; DML",
	"tags": [],
	"description": "",
	"content": "SQL DDL \u0026amp; DML Zie menu links.\n"
},
{
	"uri": "http://localhost:1313/db-course/transacties/basics/",
	"title": "Transaction Mgmt. Basics",
	"tags": [],
	"description": "",
	"content": "Moesten de database systemen voornamelijk single-user systemen zijn, dan was er geen probleem geweest want dan werden alle opdrachten gewoon sequentieel uitgevoerd. SQL Database Management Systems (DBMS) systemen zijn echter vooral multi-user systemen. Om zowel verschillende gebruikers te kunnen behandelen als nog steeds de ACID regels ondersteunen, is er een systeem nodig dat soms gebruikers \u0026ldquo;in wacht\u0026rdquo; zet. Stel je voor dat Jens en Jolien tegelijkertijd data lezen én updaten\u0026mdash;in dezelfde tabel, hetzelfde record. Jens leest uit \u0026ldquo;de rekening staat op 100 EUR\u0026rdquo; en Jolien haalt er 10 EUR vanaf. Wie mag eerst? Kan dit tegelijkertijd? Jens krijgt te horen dat er 100 EUR op de rekening staat, terwijl in werkelijkheid dit 10 EUR minder is.\nWaarom transacties? Heel simpel:\nLinks: het verkeer zonder transacties, rechts: met transacties\rOm Atomicity, Consistency, Isolation, Durability te garanderen is er dus een transactie manager nodig die alles in goede banen leidt op het moment dat verschillende gebruikers data gaan raadplegen en/of manipuleren. Dit principe is ruwweg hetzelfde als task management van het vak Besturingssystemen en C\u0026mdash;maar dan op database-applicatie niveau.\nWat is een transactie? Een transactie is een set van database operaties (bij relationele databases dus een aantal SQL operaties), dat door één gebruiker of applicatie als één unit of work aanzien wordt. Bijvoorbeeld, Jens wilt geld overschrijven van zijn rekening naar die van Jolien. Dit gaat meestal in verschillende stappen:\nHaal €10 van balans van Jens; Stort €10 op balans van Jolien. graph LR;\rJN[Rekening van Jens]\rJL[Rekening van Jolien]\rJN --\u003e|10 EUR| JL\rDit is één transactie\u0026mdash;het zou nooit mogen gebeuren dat er €10 verdwijnt van Jens\u0026rsquo; account zonder dat Jolien dat geld ontvangt. Met andere woorden, er kan niets tussen stap 1 en stap 2 komen: geen systeem crash, geen andere gebruiker (bijvoorbeeld Marianne die Jens €20 voor zijn verjaardag stort op de rekening). Dit is één \u0026ldquo;unit of work\u0026rdquo;: als er iets misloopt zou alles teruggedraaid moeten worden. Dus, een transactie heeft als resultaat ofwel success, ofwel failure, maar niets tussenin. Indien dit succesvol wordt afgerond zou het DBMS systeem moeten garanderen dat het geld effectief op Jolien\u0026rsquo;s rekening staat. Indien het faalde zou het DBMS systeem moeten garanderen dat Jens zijn geld niet kwijt is.\nIn de praktijk worden veel verschillende transacties tegelijkertijd uitgevoerd\u0026mdash;eventueel door verschillende gebruikers. Er moet dus iemand zijn die dit beheert, en dat is de transactie manager. Wie beslist of Jens eerst zijn geld mag afhalen, of dat Marianne eerst zijn verjaardagscadeau mag overmaken op de rekening? Wie beslist dat tussen stap 1 en 2 bij de transfer van het geld van Jens naar Jolien niemand mag tussenkomen? Juist: de transactie manager. Hier zijn verschillende strategieën voor, zoals we later zullen zien in Concurrency Control.\nHet beheren van transacties Formeel gezien wordt er een transactie afgelijnd door aan te kondigen wanneer een transactie begint en wanneer hij stopt, zodat de manager de juiste acties kan doorvoeren. De gebruiker kan met SQL ook zelf een transactie terugdraaien als er een programmafout voorkomt, bijvoorbeeld met een try { ... } catch(Exception ex) { rollback }. Meer hierover in sectie failures/rollbacks.\nDe transactie manager, die afgelijnde transacties ontvangt, kan dit dan in een schedule zetten, om te beslissen welke (delen van) transacties wanneer moeten worden uitgevoerd, net zoals Round Robin CPU scheduling algoritmes. Uiteindelijk wordt er een status toegekend aan een transactie:\nCommitted\u0026mdash;het is gelukt en de data is permanent gepersisteerd. Aborted\u0026mdash;Een error tijdens het uitvoeren van de transactie. Indien er halverwege de abort data is gewijzigd moet dit worden teruggezet, of worden rollbacked. Vorige versies van data moeten dus ook worden bijgehouden. Jens\u0026rsquo; rekening kan bijvoorbeeld €90 zijn initieel, hij haalt er €10 af om over te maken wat dit tot €80 maakt, maar er loopt iets mis: de rollback triggert het terugdraaien van het getal 80 naar de originele 90.\nDit is een pseudocode voorbeeld van bovenstaande afgelijnde transactie:\nBEGIN TRANSACTION;\rUPDATE account SET waarde = waarde - :over_te_maken_bedrag\rWHERE eigenaar = \u0026#39;Jens\u0026#39;\rUPDATE account SET waarde = waarde + :over_te_maken_bedrag\rWHERE eigenaar = \u0026#39;Jolien\u0026#39;\rCOMMIT; Transacties kosten CPU en RAM en zijn configureerbaar maar dus gelimiteerd in aantal. De manager kan worden ingesteld tot bijvoorbeeld ondersteunen van maximum 10 transacties tegelijkertijd.\nDBMS Componenten bij een transactie Een voorbeeld van een transactie workflow (de nummers komen overeen met het schema):\nDBMS componenten aan het werk tijdens een transactie. src: pdbmbook.com\rDe manager waarschuwt de scheduler dat er een nieuwe transactie moet worden ingepland. Deze optimaliseert dit naar throughput (zie BESC); De recovery en stored data manager worden op de hoogte gebracht. Deze optimaliseert disk access: het zou kunnen dat DB reads/writes via een buffer verlopen omdat fysieke file operaties duur zijn; De scheduler is nu klaar om input te ontvangen en uit te voeren in de juiste volgorde; Dit triggert mogelijks interactie met de stored data manager (het effectief wegschrijven van wijzigingen); De uitkomst wordt doorgegevan via een output area: ofwel is het gelukt (5b), ofwel is het mislukt (5a), waardoor de recovery manager zijn werk moet doen om dingen terug te draaien. State transition diagram illustrating the states for transaction execution.\rBEGIN_TRANSACTION: Dit markeert het begin van de uitvoering van een transactie. READ of WRITE: Deze commando\u0026rsquo;s specificeren lees- of schrijfoperaties op de database-items die worden uitgevoerd als onderdeel van een transactie. END_TRANSACTION: Dit specificeert dat de lees- en schrijfoperaties van de transactie zijn beëindigd en markeert het einde van de uitvoering van de transactie. Op dit punt kan het nodig zijn om te controleren of de wijzigingen die door de transactie zijn geïntroduceerd permanent kunnen worden toegepast op de database (gecommit) of dat de transactie moet worden afgebroken vanwege schending van serialiseerbaarheid of om een andere reden. COMMIT_TRANSACTION: Dit signaleert een succesvol einde van de transactie, zodat alle wijzigingen (updates) die door de transactie zijn uitgevoerd veilig kunnen worden gecommit naar de database en niet ongedaan zullen worden gemaakt. Wat als er iet misgaat? Hoe werkt dat blokje \u0026ldquo;recovery manager\u0026rdquo; precies? Een DBMS systeem gebruikt achterliggend een logfile als belangrijk element om eventuele recoveries te kunnen doorvoeren. Een logfile bevat in principe redundante data: in het beste geval wordt dit nooit gebruikt. Maar in het geval dat er ook maar iets misloopt is het van groot belang dat de log entries kunnen worden gebruikt om de data terug te zetten.\nVoor elke transactie EN operatie wordt relevante informatie geregistreerd in de logfile als een log record. Deze bevat de volgende informatie:\nEen unieke Log ID; Een unieke Transactie identifier om de records te kunnen koppelen aan de transacties; Een markering voor de start van de transactie + tijd + type (read/write/read-write combo) aan te duiden; Database record identifiers en operaties (select/insert/\u0026hellip;.) die bij de transactie horen; before images: een snapshot van de data voordat de transactie werd doorgevoerd. Deze worden gebruikt om een undo uit te voeren; after images: een snapshot van de data nadat de transactie werd doorgevoerd. Deze worden gebruikt om een redo uit te voeren, moest bijvoorbeeld een fysieke file write mislukken en hier een retry op worden toegepast. Er wordt altijd eerst in de logfile geschreven: dit noemen we een write-ahead log strategy. Op die manier is de DBMS manager voorbereid op een mogelijke rollback. Alle updates worden dus eerst naar de logfile geschreven voordat er iets fysiek veranderd op de harde schijf. Merk op dat de \u0026ldquo;logFILE\u0026rdquo; ook (gedeeltelijk) in-memory kan zijn.\nWat kan er zoal misgaan? Failures worden in drie groepen opgedeeld:\nTransaction failures: fouten in de logica van de transactie, zoals verkeerde input, incorrecte variabelen, statements die niet kloppen, etc. Sommige applicaties vangen dit al op voordat het naar de DBMS gaat. System failures: DB of OS crashes op de server, bijvoorbeeld door stroomonderbrekingen of bugs in de database zelf. Het zou kunnen dat de DBMS buffer inhoud hierdoor leeg raakt en delen van data niet teruggezet kunnen worden. Media failures: Storage access fouten door bijvoorbeeld disk crashes, een storing in het netwerk, etc. Het zou kunnen dat de logfile niet toegankelijk is. Hoe werkt de write-ahead logging nu precies (WAL) Logging: Wanneer een transactie wordt gestart, worden alle bewerkingen eerst naar het transaction log geschreven. Dit logboek bevindt zich in non-volatile memory, zodat de gegevens veilig zijn. Commit: Zodra alle bewerkingen succesvol zijn uitgevoerd, wordt de transactie \u0026ldquo;gecommit\u0026rdquo; en worden de wijzigingen permanent toegepast op de database. Recovery: Als er een systeemfout optreedt, kan het DBMS het transaction log gebruiken om de database te herstellen naar een consistente staat. Het systeem voert de acties uit het logboek opnieuw uit om de database terug te brengen naar de laatste bekende goede staat. Stel je voor dat je een transactie hebt die een nieuwe bestelling toevoegt aan een e-commerce database. De stappen zouden als volgt zijn:\nLoggen van acties: INSERT INTO orders (order_id, customer_id, order_date) VALUES (1, 123, \u0026#39;2025-03-10\u0026#39;); UPDATE inventory SET stock = stock - 1 WHERE product_id = 456; INSERT INTO order_items (order_id, product_id, quantity) VALUES (1, 456, 1); Commit: Zodra alle acties zijn gelogd en succesvol uitgevoerd, wordt de transactie gecommit. Recovery: Als er een systeemcrash optreedt, gebruikt het DBMS het transaction log om de database te herstellen door de gelogde acties opnieuw uit te voeren, REDO. (Als een systeemcrash optreedt voordat de commit is geschreven, wordt de transactie als niet voltooid beschouwd en worden alle wijzigingen teruggedraaid tijdens het herstelproces. UNDO) Kortom, door de combinatie van write‐ahead logging, het gebruik van commitmarkers en het toepassen van zorgvuldig ontworpen herstelprocedures (die rekening houden met de exacte volgorde en status van logrecords), zorgt een DBMS ervoor dat acties niet dubbel worden uitgevoerd en dat gedeeltelijk uitgevoerde transacties volledig worden teruggedraaid.\nSystem Recovery Veronderstel dat er 5 transacties door de DBMS worden verwerkt. Tc duidt een checkpoint aan van de data in de buffer manager van de afbeelding hierboven. Er treedt een systeemfout op rechts bij Tf:\nUNDO/REDO operaties voor 5 \u0026#39;kapotte\u0026#39; transacties. src: pdbmbook.com\rWe bekijken de 5 transacties afzonderlijk:\nT1\u0026mdash;Aangezien deze succesvol werd gecommit voor de systeemfout én voor de snapshot Tc, hoeft hier niets voor te gebeuren na een crash. T2\u0026mdash;Deze transactie is ook compleet voor Tf, maar er zit nog data in de buffer van de snapshot Tc die niet naar disk geschreven werd. Hier is dus een REDO nodig. T3\u0026mdash;Deze transactie was nog bezig bij de crash van Tf. We hebben niet alle data in de buffer: er is dus transactie data verloren gegaan: een UNDO dus. T4\u0026mdash;Deze transactie is gelukt maar alle data is nog steeds pending to disk (bij T2 was dit een deel): REDO. T5\u0026mdash;De DBMS scheduler had deze net ingepland toen het systeem crashte. Het is niet nodig om hier is van terug te draaien omdat er niks pending was (na de Tc checkpoint). Meer info hier, maar is enkel ter info en moet niet in detail gekend zijn\nMedia Recovery Wat doe jij als er een harde schijf gerashed is van je computer? Bidden voor een werkende backup? Indien fysieke files niet meer toegankelijk zijn is een \u0026ldquo;media recovery\u0026rdquo; afhankelijk van data redundancy principes. Hopelijk heb je een backup strategie voorzien. Voor database systemen wordt dit streng toegepast en moeten we een recovery doorvoeren via een mirror op een andere HDD in RAID, op een cloud backup, een offline backup, \u0026hellip;\nOm de failover time te minimaliseren wordt dit vaak automatisch opgezet. Een harde schijf kan in een server in RAID-1 modus geplaatst worden: die functioneert als 100% clone van de andere. Indien er één van beide HDDs faalt, neemt onmiddellijk de andere het werk over, zodat gebruikers zo goed als geen last hebben van de media failure. Systemen als backup copies door iets te \u0026ldquo;archiveren\u0026rdquo; (een .tar.gz of .zip op bepaalde tijdstippen aan de kant zetten) vereist meestal meer werk om terug te zetten. Backups nemen betekent ook beslissen of het een full of incremental backup zal zijn, waarbij je (1) alle files apart zet, of (2) enkel de wijzigingen ten opzichte van vorige snapshots (denk aan git bijvoorbeeld).\nEen goede backup strategie\u0026mdash;dit geldt zowel voor databases als voor je eigen data!!\u0026mdash;volgt het 1-2-3 backup principe:\nZorg voor minstens drie backups van je data, waarvan twee lokaal maar op verschillende media (bvb, verschillende servers, op HDD en op USB, \u0026hellip;), en één off-site kopie, zoals cloud-based backup systemen als Dropbox, Google Drive, Backblaze\u0026rsquo;s Cloud Storage. En dan hebben we het nog niet over security gehad\u0026hellip;\nDenkvragen Waarom kan je niet gewoon media recovery strategieën toepassen bij systeemcrashes? Waarom wel, maar is dat misschien geen goed idee?\nSolution:\nMedia recovery strategieën (zoals herstel vanuit back-ups) zijn primair bedoeld voor fysieke schade aan opslagmedia. Bij een systeemcrash gaat het erom de consistente staat van de database snel te herstellen, wat met media recovery vaak te traag en omslachtig is. Bovendien kan een dergelijk herstel risico’s met zich meebrengen, zoals het herstellen van een oude, mogelijk inconsistente staat. In principe kan media recovery ingezet worden als laatste redmiddel. Echter, vanwege de langere hersteltijd en de kans op inconsistente data (bijvoorbeeld als sommige transacties al deels uitgevoerd waren) is het in de praktijk verstandiger om gespecialiseerde crash recovery technieken (zoals log-gebaseerde UNDO/REDO mechanismen) te gebruiken. Als er verschillende transacties tegelijkertijd aan één record iets wijzigen, welke problemen zie jij dan zoal? Hoe zou je dat door de transaction manager laten oplossen? Kan je zo twee verschillende oplossingen bedenken?\nSolution:\nEén transactie overschrijft de wijzigingen van een andere, waardoor data verloren gaat.\nEen transactie leest onvoltooide (nog niet gecommitte) data van een andere transactie.\nHerhaalde leesoperaties geven verschillende resultaten, wat leidt tot inconsistenties.\nEen record vergrendelen zodra een transactie eraan werkt. Andere transacties moeten wachten tot de vergrendeling wordt opgeheven, waardoor conflicten worden voorkomen.\nTransacties voeren wijzigingen uit zonder directe vergrendeling. Bij het committen controleert het systeem of er tussentijds wijzigingen hebben plaatsgevonden. Als er een conflict wordt gedetecteerd, wordt de transactie teruggedraaid.\nWat is alweer het verschil tussen UNDO en REDO recovery technieken?\nSolution:\nUNDO: Wordt gebruikt om de effecten van transacties die niet succesvol zijn afgerond of die foutief zijn uitgevoerd, ongedaan te maken. Dit zorgt ervoor dat de database terugkeert naar een consistente staat. REDO: Wordt toegepast om de wijzigingen van transacties die wel gecommit waren opnieuw toe te passen. Dit is vooral belangrijk na een crash wanneer sommige wijzigingen nog niet permanent op de schijf waren vastgelegd. Als ier iets misgaat op applicatieniveau, bijvoorbeeld een crash in je Java applicatie, moet de DBMS dan iets doen, of moet de programmeur dan iets doen, om ACID te blijven garanderen?\nSolution:\nHet DBMS is primair verantwoordelijk voor het waarborgen van ACID-eigenschappen via zijn transactie- en recoverymechanismen. Bij een crash op applicatieniveau (bijvoorbeeld in een Java-applicatie) zorgt het DBMS er met behulp van commit/rollback en log-based recovery voor dat de database in een consistente staat blijft. De programmeur moet wel zorgen voor een correcte afhandeling van transacties in de applicatie, zoals het op de juiste wijze starten, committen en, indien nodig, terugdraaien van transacties. Wat zou er volgens jou moeten gebeuren als twee personen tegelijkertijd op bol.com een item bestellen waarvan maar één hoeveelheid in stock is? Wie trekt hier aan het korte eind? Hoe vangen grote webwinkels dit probleem op?\nSolution:\nHet voorraadbeheersysteem (onderdeel van het DBMS of een gekoppeld systeem) moet atomische transacties gebruiken. Dit zorgt ervoor dat slechts één transactie succesvol het laatste item kan reserveren, terwijl de andere transactie een foutmelding krijgt of wordt teruggedraaid. "
},
{
	"uri": "http://localhost:1313/db-course/timeseries/opdracht/",
	"title": "VERPLICHTE opdracht",
	"tags": [],
	"description": "",
	"content": "Opdracht InfluxDb met als deadline vrijdag 16 mei 2025 23u59 Voor de verplichte opdracht meld je je aan onder de correcte naam bij volgende Github Classroom. En pull je de repository van Opdracht rond InfluxDb. Deze repository bevat een Java Gradle project met een aantal TODO\u0026rsquo;s die je moet oplossen. Hieronder staat de opdracht nog beschreven:\nOpdracht: Je krijgt alweer een startproject met de TempPoint klasse gegeven, je krijgt ook al de start van de TemperatureRepository klasse waarvan je weer zelf de implementaties van de gevraagde methoden moet programmeren volgens de TODOs zodat alle testen slagen. Alle dependencies en imports zijn al ingevoegd en correct, het kan echter zijn dat voor jouw manier een extra import nodig is. Je moet er ook wel voor zorgen dat je op je lokale InfluxDb server al een bucket hebt met de naam \u0026ldquo;test\u0026rdquo; en dat je de gegevens van Token en dergelijke correct zet voor jouw InfluxDb. Zorg er ook voor dat je database opstaat.\nJe bent volledig vrij in hoe je de methoden implementeert.\nPush voor de deadline van vrijdag 16 mei 2025 23u59 je oplossingen naar je repository\nVeel succes! Je mag me altijd contacteren via arne.duyver@kuleuven.be voor vragen of in de les aanspreken.\n"
},
{
	"uri": "http://localhost:1313/db-course/xml/xsd/",
	"title": "XSD",
	"tags": [],
	"description": "",
	"content": "XML Schema Definition Wanneer we met XML communiceren tussen twee verschillende partijen, hebben we natuurlijk ook spelregels nodig. Daarvoor kunnen we een XML schema of XSD gebruiken. Daarin leggen we vast:\nwelke tags wel of niet mogen voorkomen, in welke volgorde die moeten staan, hoe vaak een element mag voorkomen, of een element optioneel of verplicht is, het datatype van het element, welke attributen op een tag toegelaten zijn Ons vorige XML representatie van een collectie boeken kan met volgende XSD beschreven worden:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;xs:schema xmlns:xs=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34;\u0026gt; \u0026lt;xs:element name=\u0026#34;boeken\u0026#34; type=\u0026#34;boekenType\u0026#34;/\u0026gt; \u0026lt;xs:complexType name=\u0026#34;boekenType\u0026#34;\u0026gt; \u0026lt;xs:sequence\u0026gt; \u0026lt;xs:element name=\u0026#34;boek\u0026#34; type=\u0026#34;boekType\u0026#34; maxOccurs=\u0026#34;unbounded\u0026#34;/\u0026gt; \u0026lt;/xs:sequence\u0026gt; \u0026lt;/xs:complexType\u0026gt; \u0026lt;xs:complexType name=\u0026#34;boekType\u0026#34;\u0026gt; \u0026lt;xs:sequence\u0026gt; \u0026lt;xs:element name=\u0026#34;titel\u0026#34; type=\u0026#34;xs:string\u0026#34;/\u0026gt; \u0026lt;xs:element name=\u0026#34;auteur\u0026#34; type=\u0026#34;xs:string\u0026#34;/\u0026gt; \u0026lt;xs:element name=\u0026#34;jaar\u0026#34; type=\u0026#34;xs:integer\u0026#34;/\u0026gt; \u0026lt;/xs:sequence\u0026gt; \u0026lt;/xs:complexType\u0026gt; \u0026lt;/xs:schema\u0026gt; Het boeken element, ons root element wordt als eerste beschreven en krijgt een custom type toegekend, namelijk een zelf gedefinieerd type boekenType. Daaronder beschrijven we precies wat een boekenType is. In dit geval is het een complexType. Het bestaat namelijk uit meerdere andere elementen. We geven hier mee dat enkel het element boek mag voorkomen in ons boekType. Wat valt nog op? Dat is de toevoeging van maxOccurs=\u0026quot;unbounded\u0026quot;. Hiermee geven we aan dat dit element ongelimiteerd gebruikt mag worden. De tegenhanger van maxOccurs is minOccurs, door die de waarde 1 mee te geven bijvoorbeeld, maken we een element verplicht. De combinatie kan ook gebruikt worden. Stel dat we een ISBN nummer zouden toevoegen, dan zouden we dat als volgt kunnen doen:\n\u0026lt;xs:element name=\u0026#34;isbn\u0026#34; type=\u0026#34;xs:string\u0026#34; minOccurs=\u0026#34;1\u0026#34; maxOccurs=\u0026#34;1\u0026#34;/\u0026gt; Hierbij geven we dus aan dat het ISBN nummer een verplicht veld is, en ik maar maximaal 1 keer kan voorkomen.\nHet boekType ten slotte heeft ook zijn eigen definitie. Dit type bevat een sequence, dat betekent dat de elementen die gedefinieerd zijn onder het boekType moeten voorkomen in de volgorde dat ze zijn gedefinieerd. De elementen zelf zijn hier primitieve elementen. Daarom krijgen ze reeds bestaande types, zoals string en integer.\nJe kan je XML bestanden laten valideren tegen een XSD schema, door daar zelf logica voor te schrijven. In typische Enterprise applicaties, waarbij XML gebruikt wordt als communicatiemiddel ga je dat steeds willen doen om te verifiëren dat alle data doorgegeven werd op de op voorhand beschreven manier. Of je kan een online validatie tool gebruiken. Hiervoor kan je gedurende deze les deze XSD Validator gebruiken\nOefeningen Maak een XSD schema voor de studenten XML die jullie in de vorige opgave hebben gemaakt. Voeg een nieuw element Gender toe aan XML en XSD. We kunnen ons XSD schema uitbreiden door er ook attributen aan toe te voegen. Laten we een Genre attribuut definiëren op ons boek element.\n\u0026lt;xs:complexType name=\u0026#34;boekType\u0026#34;\u0026gt; \u0026lt;xs:sequence\u0026gt; \u0026lt;xs:element name=\u0026#34;titel\u0026#34; type=\u0026#34;xs:string\u0026#34;/\u0026gt; \u0026lt;xs:element name=\u0026#34;auteur\u0026#34; type=\u0026#34;xs:string\u0026#34;/\u0026gt; \u0026lt;xs:element name=\u0026#34;jaar\u0026#34; type=\u0026#34;xs:integer\u0026#34;/\u0026gt; \u0026lt;/xs:sequence\u0026gt; \u0026lt;xs:attribute name=\u0026#34;genre\u0026#34; type=\u0026#34;xs:string\u0026#34; /\u0026gt; \u0026lt;/xs:complexType\u0026gt; In XML vorm zou dat er dan als volgt uitzien:\n\u0026lt;boek genre=\u0026#34;fantasy\u0026#34;\u0026gt; \u0026lt;titel\u0026gt;The Lost Metal\u0026lt;/titel\u0026gt; \u0026lt;auteur\u0026gt;Brandon Sanderson\u0026lt;/auteur\u0026gt; \u0026lt;jaar\u0026gt;2022\u0026lt;/jaar\u0026gt; \u0026lt;/boek\u0026gt; Validaties We kunnen aan XSD ook nog validatieregels toevoegen die de data-integriteit verzekeren. Hieronder volgen een aantal voorbeelden:\nMinimum en maximumwaarde voor getal \u0026lt;xs:element name = \u0026#34;score\u0026#34;\u0026gt; \u0026lt;xs:simpleType\u0026gt; \u0026lt;xs:restriction base = \u0026#34;xs:integer\u0026#34;\u0026gt; \u0026lt;xs:minInclusive value = \u0026#34;0\u0026#34;/\u0026gt; \u0026lt;xs:maxInclusive value = \u0026#34;100\u0026#34;/\u0026gt; \u0026lt;/xs:restriction\u0026gt; \u0026lt;/xs:simpleType\u0026gt; \u0026lt;/xs:element\u0026gt; Enumeratie van waardes \u0026lt;xs:element name = \u0026#34;Score\u0026#34;\u0026gt; \u0026lt;xs:simpleType\u0026gt; \u0026lt;xs:restriction base = \u0026#34;xs:string\u0026#34;\u0026gt; \u0026lt;xs:enumeration value = \u0026#34;Laag\u0026#34;/\u0026gt; \u0026lt;xs:enumeration value = \u0026#34;Gemiddeld\u0026#34;/\u0026gt; \u0026lt;xs:enumeration value = \u0026#34;Hoog\u0026#34;/\u0026gt; \u0026lt;/xs:restriction\u0026gt; \u0026lt;/xs:simpleType\u0026gt; \u0026lt;/xs:element\u0026gt; Regex \u0026lt;xs:element name = \u0026#34;Naam\u0026#34;\u0026gt; \u0026lt;xs:simpleType\u0026gt; \u0026lt;xs:restriction base = \u0026#34;xs:string\u0026#34;\u0026gt; \u0026lt;xs:pattern value = \u0026#34;[a-z]\u0026#34;/\u0026gt; \u0026lt;/xs:restriction\u0026gt; \u0026lt;/xs:simpleType\u0026gt; \u0026lt;/xs:element\u0026gt; Oefeningen Voeg een attribuut toe aan de studenten XML dat aangeeft of de student een Bachelor of Master volgt. Voeg voor elke student nu ook een lijst van vakken toe. Per vak willen we zien voor hoeveel studiepunten dit meetelt en een score? Voeg dit ook toe in je XML en valideer dit tegen je XSD. "
},
{
	"uri": "http://localhost:1313/db-course/sql/rdbms-acid/",
	"title": "ACID",
	"tags": [],
	"description": "",
	"content": "ACID is een acronym die we gebruiken binnen databases dat een lijst van voorwaarden omschrijft waar dat database systeem aan moet voldoen. De regels van ACID worden over het algemeen geïmplementeerd door het concept van Transacties. ACID omschrijft vier principes:\nAtomicity Consistency Isolation Durability De ACID principes komen in de praktijk nog verder aan bod in het hoofdstuk over RDBMS transacties, dus geen paniek als onderstaande theorie nog niet onmiddellijk duidelijk is.\nAtomicity Transacties bestaan vaak uit meerdere statements. Atomicity verwacht dat al deze statements als één geheel worden beschouwd. Ofwel faalt alles, ofwel slaagt alles. Zo wordt er nooit slechts een deel van de changes bewaard.\nHet gevolg hiervan is dat de staat van een transactie niet gezien kan worden door andere gebruikers. Stel we hebben een typische bankoverschrijving die €10 gaat overschrijven van de rekening van Alice naar de rekening van Bob.\nsequenceDiagram\rNote over Application,DB: BEGIN TRANSACTION\rDB--\u003eDB: Rekening van Alice: €100\rDB--\u003eDB: Rekening van Bob: €150\rApplication-\u003e\u003eDB: UPDATE Account SET Amount -= 10 WHERE Owner = 'Alice'\rDB--\u003eDB: Rekening van Alice: €100\rDB--\u003eDB: Rekening van Bob: €150\rApplication-\u003e\u003eDB: UPDATE Account SET Amount += 10 WHERE Owner = 'Bob'\rNote over Application,DB: COMMIT TRANSACTION\rDB--\u003eDB: Rekening van Alice: €90\rDB--\u003eDB: Rekening van Bob: €160\rConsistency Consistency zorgt ervoor dat een database altijd in een consistente staat moet blijven. Met andere woorden, er moet altijd voldaan worden aan de constraints die op de database gedfinieerd zijn. Dit kan een referentiële constraint zijn, als er een rij wordt verwijderd die nog gebruikt wordt in een andere tabel waar onderliggend een referentiële constraint op ligt.\nNeem bovenstaand voorbeeld waarbij Alice geld overschrijft naar de rekening van Bob. Maar nu wil ze €150 overschrijven. We hebben op de Account tabel een CHECK CONSTRAINT gedefinieerd dat de Amount altijd groter of gelijk aan 0 moet zijn. Wat gebeurt er dan?\nsequenceDiagram\rNote over Application,DB: BEGIN TRANSACTION\rDB--\u003eDB: Rekening van Alice: €100\rDB--\u003eDB: Rekening van Bob: €150\rApplication-\u003e\u003eDB: UPDATE Account SET Amount -= 150 WHERE Owner = 'Alice'\rrect rgb(194, 24, 7)\rDB--\u003eDB: Error: Amount should be \u003e= 0\rend\rNote over Application,DB: ROLLBACK TRANSACTION\rDB--\u003eDB: Rekening van Alice: €100\rDB--\u003eDB: Rekening van Bob: €150\rDe transactie wordt teruggedraaid. Er werd geen geld afgehaald van de rekening van Alice en Bob heeft ook geen geld gekregen. We zitten opnieuw in een geldige consistente staat.\nIsolation Als je een query of transactie uitvoert op een database, ga je zelden als enige gebruiker actief zijn. Er zullen wellicht andere transacties uitgevoerd worden terwijl jij de jouwe uitvoert. Om ervoor te zorgen dat deze geen invloed hebben op elkaar worden er locks gelegd op een set van data. Daar bovenop worden isolation levels gedefinieerd die bepalen wat andere gebruikers mogen zien en lezen van andere transacties.\nDit komt meer detail aan bod in een volgend hoofdstuk.\nDurability Als we een transactie afronden en hij wordt gecommit dan is die nog steeds committed ook in het geval van een crash of stroomonderbreking. Veel DBMS providers lossen dit op door data weg te schrijven in non-volatile memory en door gebruik te maken van een transaction log.\nDat laatste is een logboek van alle acties die uitgevoerd werden op een database, die opnieuw uitgevoerd kunnen worden als een database gerestored moet worden vanaf een eerdere staat.\n"
},
{
	"uri": "http://localhost:1313/db-course/transacties/concurrency-control/",
	"title": "Concurrency Control",
	"tags": [],
	"description": "",
	"content": "De transactie management scheduler (zie transacties - basics) is verantwoordelijk om verschillende transacties correct in te plannen zonder dat er data problemen of clashes optreden.\nProblemen? Welke problemen? Denk terug aan het bank transfer probleem van de vorige sectie. Veronderstel dat deze keer zowel Jens als Marianne €10 willen overmaken naar Jolien. Als we dat als volgt doen:\nLees het huidige bedrag op de source rekening Verminder bedrag van source rekening Lees het huidige bedrag op de destination rekening Verhoog bedrag van destination rekening Dan zou het kunnen dat bij het uitlezen van #3, Jolien\u0026rsquo;s rekening bij de transactie van Marianne al €110 uitleest omdat Jens zijn verhoging al is doorgekomen. Maar net dan canceled de transactie van Jens en wordt de rekening van Jolien en van Jens (terecht) terug op de vorige waarde ingesteld (dus €100 voor Jolien). De transactie van Marjanne check nie nog eens het startbedrag en werkt dus nog altijd onder de veronderstelling dat er €10 bij €110 moet worden opgeteld en is op het einde het eindtotaal van Jolien toch €120 en Jens heeft geen geld verloren. Oeps!\ngraph LR;\rJN[Rekening van Jens]\rJL[Rekening van Jolien]\rML[Rekening van Marianne]\rJN --\u003e|10 EUR| JL\rML --\u003e|10 EUR| JL\rHieronder volgen een aantal veel voorkomende concurrency problemen die de transaction scheduler uitdagen.\nDirty Read Dit is exact bovenstaande situatie. Een dirty read probleem is het lezen van uncommited \u0026ldquo;dirty\u0026rdquo; data\u0026mdash;data die eigenlijk voor de andere transactie nog niet zichtbaar mag zijn omdat het hier over uncommitted data gaat. Als Marianne\u0026rsquo;s read(bedrag) toch het juiste bedrag zou inlezen (110), voordat Jens\u0026rsquo; transactie compleet is, maar op een of andere manier is die transactie teruggedraaid, dan spreken we over een dirty read, en krijgt Jolien onterecht toch €20, terwijl Jens zijn €10 mag houden. It prints money!\ntime T1 (Marianne) T2 (Jens) bedrag 1 begin trans 100 2 read(bedrag): 100 100 3 bedrag = bedrag + 10 100 4 begin trans write(bedrag) 110 5 read(bedrag): 110 110 6 bedrag = bedrag + 10 ROLLBACK! 120 (oeps) 7 write(bedrag) 120 8 commit 120 Lost Updates Stel je nu voor dat bij het uitlezen, tijdens het verhogen van het bedrag op de destination rekening, Jolien\u0026rsquo;s rekening op €100 staat. Maar als de transactie van Marianne dit ook leest als €100, en niet wacht tot de €110 die het zou moeten zijn na de commit van de transactie van Jens, dan gaat in totaal Marianne slechts €10 rijker zijn in plaats van twee keer dat bedrag. Weer een oepsie!\nHet UPDATE statement van Jens\u0026rsquo; transactie (verhoog Jolien\u0026rsquo;s rekening met 10) is eigenlijk \u0026ldquo;verloren\u0026rdquo; gegaan, omdat Marianne hier tussen komt, en haar UPDATE die terug ongedaan maakt:\ntime T1 (Marianne) T2 (Jens) bedrag 1 begin trans 100 2 begin trans read(bedrag): 100 100 3 read(bedrag): 100 bedrag = bedrag + 10 100 4 bedrag = bedrag + 10 write(bedrag) 110 5 write(bedrag) commit 110 (oeps) 6 commit 110 Dit voorbeeld illustreert dat, alhoewel beide transacties 100% correct zijn, er toch nog problemen kunnen optreden als transacties elkaar gaan storen. Het spreekt voor zich dat als T1 data zou lezen en schrijven uit een andere rij of tabel, dit geen probleem zou zijn\u0026mdash;in dit specifieke voorbeeld.\nEen lost update is dus een fout in een databasesysteem die optreedt wanneer twee of meer transacties tegelijkertijd eenzelfde record wijzigen, maar één van de wijzigingen onbedoeld wordt overschreven. Hierdoor gaat de update van één transactie \u0026ldquo;lost\u0026rdquo; of verloren, waardoor het eindresultaat niet alle beoogde wijzigingen bevat.\nNon-repeatable reads Er zijn nog verschillende andere mogelijkheden waarbij de scheduler de bal kan misslaan. Bijvoorbeeld door non-repeateable reads, dit doet zich voor wanneer een transactie dezelfde data meerdere keren leest en daarbij verschillende resultaten ontvangt, doordat een andere transactie in de tussentijd de data heeft gewijzigd en gecommit.\nAls we naar het voorbeeld van Jolien kijken. Stel dat ze een verslag wil uittrekken van haar jaaroverzicht en ze wil het totale bedrag op haar rekening en de interest (2%) die ze krijgt op haar totale bedrag, samen met nog andere statistieken. Stel nu dat ze haar totale bedrag van €120 krijgt te zien als totale bedrag, maar net voordat de interest berekend wordt wordt er een andere transactie uitgevoerd die €20 wegneemt van haar totale bedrag. Dan krijgt Jolien een verslag waar op staat: Totale bedrag = €120, interest = €2. Wees maar zeker dat Jolien een kwade email zal sturen!\nHet probleem hier is dat alles wat voor het verslag opgehaald moet worden consistent moet blijven voor dat hele verslag, er mogen tegelijk dus geen andere transacties updates doorvoeren.\ntime T1 (Verslag maken) T2 (20 euro wegnemen) bedrag 1 begin trans 120 2 read(bedrag): 120 [om totaal bedrag te weten] begin trans 120 3 \u0026hellip; bedrag = bedrag - 10 120 4 \u0026hellip; write(bedrag) 100 5 read(bedrag): 100 [om interest te berekenen, oeps] commit 100 6 commit 100 Phantom read Of wat dacht je van phantom reads: dit treedt op wanneer een transactie herhaaldelijk dezelfde query uitvoert en bij de tweede uitvoering extra records ziet (of juist records mist) die door een andere transactie zijn toegevoegd of verwijderd en al gecommit. Bijvoorbeeld: transactie T2 is bezig met rijen effectief te verwijderen, terwijl T1 deze toch nog inleest. Het zou kunnen dat hierdoor verschillende rijen ontstaan (T2 rollback, T1 die een nieuwe rij maakt), of dat voorgaande bestaande rijen verdwijnen door T2, waardoor de transactie van T1 mogelijks faalt.\nWeer teruggrijpende naar ons Jolien voorbeeld, stel dat Jens Jolien €5 wilt betalen per klusje dat ze gedaan heeft. Klusjes worden opgeslagen in een aparte tabel, maar ondertussen kan Jolien een nieuw klusje afvinken, waardoor Jens\u0026rsquo; transactie een phantom read krijgt:\ntime T1 (Jens) T2 (Jolien) klusjes 1 read 1: SELECT * FROM klusjes WHERE naam = 'Jolien' (2) 2 2 \u0026hellip; 2 3 \u0026hellip; INSERT INTO klusjes ... 3 4 \u0026hellip; 3 5 read 2: SELECT * FROM klusjes WHERE naam = 'Jolien' (3, oeps) 3 6 sort 3x5 op rekening (teveel) 3 7 commit Bijgevolg verkrijgt T1 inconsistente data: de ene keer 2, de andere keer 3\u0026mdash;vergeet niet dat het goed zou kunnen dat T2 nog teruggedraaid wordt.\nWat is het verschil tussen een non-repeatable read, een phantom read, en een dirty read? Dirty reads lezen foutieve uncommitted data van een andere transactie\u0026mdash;het lezen van \u0026lsquo;in progress\u0026rsquo; data. Non-repeateable reads en phantom reads lezen foutieve committed data van een andere transactie. Bij non-repeatable reads gaat het over UPDATEs, en bij phantom reads over INSERTs en/of DELETEs: rijen die plots verschijnen of zijn verdwenen sinds de transactie begon.\nMeerdere fouten tegelijk Bijvoorbeeld bij inconsistente analyse tussen twee transacties gaat het over een sequentie van verschillende dirty reads die de situatie alleen maar verergeren, zelfs zonder de eventuele rollback. Stel dat het over te schrijven bedrag in stukjes van €2 wordt overgeschreven, waarbij telkens tussenin een read(bedrag) plaats vindt\u0026mdash;die natuurlijk de data van de andere transactie inleest. Het resultaat is, opnieuw, een veel te grote som, en een mogelijks erg blije Jolien.\nWe laten een schematische voorstelling van dit probleem als oefening voor de student.\nScheduler oplossingen Wat is de simpelste manier om bovenstaande problemen allemaal integraal te vermijden? Juist ja\u0026mdash;sequentieel transacties verwerken.\nSerial scheduling Met serial scheduling is T2 verplicht te wachten op T1 totdat die zijn zaakje op orde heeft. De lost update transactie flow van hierboven ziet er dan als volgt uit:\ntime T1 (Marianne) T2 (Jens) bedrag 1 begin trans 100 2 read(bedrag): 100 100 3 bedrag = bedrag + 10 100 4 write(bedrag) 110 5 commit 110 6 begin trans 110 7 read(bedrag): 110 110 8 bedrag = bedrag + 10 120 9 write(bedrag) 120 10 commit 120 Wat valt je op als je naar de time kolom kijkt? Serial scheduling duurt nu 10 ticks t.o.v. 6 bij de potentiële lost update\u0026mdash;da\u0026rsquo;s een redelijk grote performance hit van 40%! Serial scheduling lost misschien wel alles op, maar daardoor verliezen we alle kracht van het woord parallel: dit is in essentie non-concurrency.\nWat zijn dan betere scheduling alternatieven?\nOptimistic scheduling Bij \u0026ldquo;optimistic\u0026rdquo; scheduling gaan we ervan uit dat conflicten tussen simultane transacties nauwelijks voorkomen. Met andere woorden, we benaderen het probleem (misschien te) optimistisch. Transacties mogen gerust tegelijkertijd lopen als ze bijvoorbeeld verschillende tabellen of stukken data in dezelfde tabel bewerken\u0026mdash;zolang er geen conflict is, geniet paralellisatie de voorkeur.\nBij optimistische schedulers worden alle transactie operaties doorgevoerd. Wanneer deze klaar zijn voor een eventuele COMMIT, wordt er gecontroleerd op potentiële conflicten. Indien geen, success. Indien wel, abort en ROLLBACK.\nMerk op dat rollback operaties erg dure operaties zijn: de manager moet graven in de logfile, moet beslissen of er UNDO/REDO operaties moeten worden uitgevoerd, er is mogelijke trage disc access (I/O), \u0026hellip; Als je vaak rollbacks hebt/verwacht is optimistic scheduling nog steeds geen performant alternatief.\nPessimistic scheduling Bij \u0026ldquo;pessimistic\u0026rdquo; scheduling gaan we van het omgekeerde uit: transacties gaan heel zeker conflicteren. De scheduler probeert transactie executies te verlaten om dit zo veel mogelijk te vermijden. Een serial scheduler is een extreem geval van een pessimistic scheduler.\nLocking In de praktijk wordt locking gebruikt op een pre-emptive manier (zie besturingssystemen: scheduling algorithms) om toch transacties waar mogelijk concurrent te laten doorlopen. Bij transacties die schrijven in plaats van die enkel lezen zullen locks eerder nodig zijn. Er bestaan uiteraard erg veel verschillende locking technieken/algoritmes.\nWe maken onderscheid tussen twee groepen:\nexclusive locks: als T1 een exclusieve lock neemt, kan er geen enkele andere transactie op die database worden uitgevoerd. Dit is een erg strict systeem, denk aan serial scheduling. shared locks: zolang T1 een lock neemt op een object (zie onder), krijgt het de garantie dat geen enkele andere transactie dat object kan manipuleren totdat de lock terug wordt vrijgegeven (eventueel ook door middel van een rollback). Locks worden \u0026ldquo;gedeeld\u0026rdquo;: T1 krijgt write access, en T2 moet wachten met schrijven, maar mag wel lezen. De database \u0026ldquo;objecten\u0026rdquo; waar een lock op genomen kan worden zijn onder andere (van kleine locks naar groot):\nrow locks; column locks; page locks (delen van een tabel zoals stored in files); table locks; databas locks (bvb een file lock in SQLite); Volgorde hierboven gaat ongeveer van minder wachttijd (1) naar meer wachttijd (2)\nEen lock op één rij in beslag genomen door T1 betekent dat voor diezelfde tabel T2 nog steeds bedragen kunnen wijzigen van een andere rekening. Column locks kunnen tot meer wachttijd leiden. Page locks zijn \u0026ldquo;stukken\u0026rdquo; van een tabel (rijen voor x en erna). Een page is een chunk van een tabel die op die manier wordt opgeslaan. Dit verschilt van DB implementatie tot implementatie. Tenslotte kan een hele tabel gelockt worden\u0026mdash;of de hele tablespace\u0026mdash;wat meer pessimistisch dan optimistisch is.\nLocks zijn onderhevig aan dezelfde nadelen als rollbacks: ze zijn duur. Als er erg veel row locks zijn op een bepaalde tabel kan dit veel CPU/RAM in beslag nemen. In dat geval kan de lock manager beslissen om aan lock escalation te doen: de 100 row locks worden één page of table lock. Dit vermindert resource gebruik drastisch\u0026mdash;maar zou transacties ook langer kunnen laten wachten. You win some, you lose some\u0026hellip;\nProblemen bij oplossen van problemen: deadlocks Stel dat Jens cash geld wilt deponeren, en Marianne hem ook geld wenst over te schrijven. Voordat Marianne dat doet koopt ze eerst nog een cinema ticket. Jens wil ook een ticket in dezelfde transactie. Het gevolg is dat T1 en T2 oneindig op elkaar blijven wachten. Dat ziet er zo uit:\ngraph LR;\rC{Cinema tabel}\rR{Rekening tabel}\rJ[Jens]\rM[Marianne]\rJ --\u003e|deposit 10| R\rM --\u003e|transfer 10| R\rM --\u003e|koop ticket| C\rJ --\u003e|koop ticket| C\rDe kruisende pijlen duiden al op een conflict:\ntime T1 (Jens) T2 (Marianne) 1 begin tarns 2 begin tarns 3 update rekening (acquire lock R) 4 update cinema (acquire lock C) 5 poging tot update cinema (WACHT) 6 poging tot update rekening (WACHT) Deze (simpele) deadlock kan gelukkig gedetecteerd worden door het DBMS systeem. Die zal één van beide transacties als \u0026ldquo;slachtoffer\u0026rdquo; kiezen en die transactie terugdraaien\u0026mdash;meestal gebaseerd op welke het makkelijkste is om terug te draaien. Het probleem is dat de meeste applicaties niet onmiddellijk voorzien zijn op zo\u0026rsquo;n onverwachte rollback. Dit resulteert meestal in een negatieve gebruikerservaring.\nDeadlocks en algoritmes om dit te detecteren en op te lossen zijn erg complexe materie. Het volstaat om bovenstaand eenvoudig voorbeeld te kennen, en te weten hoe dat aangepakt zou kunnen worden\u0026mdash;bijvoorbeeld met:\nstarvation: een transactie voortdurend als slachtoffer kiezen met als risico dat deze nooit de kans krijgt om succesvol af te ronden. In dit geval wordt de transactie steeds teruggedraaid terwijl andere transacties wel kunnen doorgaan, wat resulteert in oneerlijke behandeling en voortdurende vertraging van de starvende transactie. timeouts: een mechanisme om vastgelopen transacties op te sporen en af te breken. Wanneer een transactie langer dan een vooraf ingestelde tijd op een resource wacht, wordt deze afgebroken (rollback) om te voorkomen dat de deadlock het systeem blijft blokkeren. priority shift: een strategie waarbij de prioriteit van een transactie dynamisch wordt aangepast om deadlocks te helpen voorkomen of op te lossen. In plaats van dat een transactie telkens wordt gekozen als slachtoffer en teruggedraaid (wat kan leiden tot starvation), wordt de prioriteit verhoogd zodat deze transactie eerder de benodigde resources krijgt om haar bewerkingen af te ronden. Dit mechanisme zorgt ervoor dat starvende transacties uiteindelijk kunnen doorstromen en dat het systeem efficiënter functioneert bij het oplossen van conflicten. Deze concepten komen ook terug in schedulers voor besturingssystemen en zie ook A beginners guide to DB deadlocks.\nIsolation levels Pessimistic locking kan veel problemen opvangen, maar ten koste van performantie\u0026mdash;wat meestal belangrijker is. Voor veel transacties is het oké om met een minimum aan conflicten de throughput van transacties zo hoog mogelijk te houden. De meeste DBMS systemen zijn hier flexibel in: je kan wat we noemen isolation levels instellen, dat meestal bestaat uit de volgende vier opties (van low naar high isolation):\nRead uncommited\u0026mdash;Dit laat toe om \u0026ldquo;uncommited\u0026rdquo; data te lezen (dat problemen geeft, zie boven), en wordt meestal gebruikt in combinatie met read-only transacties. Read committed\u0026mdash;Gebruikt short-term read locks en long-term write locks, en lost zo het inconsistent analysis/lost update probleem op, maar is niet krachtig genoeg om phantom reads tegen te houden. Repeatable read\u0026mdash;Gebruikt long-term read \u0026amp; write locks. Een transactie kan zo dezelfde rij verschillende keren opnieuw lezen zonder conflicten van insert/updates van andere transacties. Het phantom read probleem is echter nog steeds niet opgelost. Serializable\u0026mdash;het krachtigste isolation level dat in theorie aansluit met serial scheduling. Een short-term lock wordt enkel vastgehouden gedurende het interval dat nodig is om een operatie af te ronden, terwijl long-term locks een protocol volgen en meestal tot de transactie commited/rollbacked is, vastgelegd zijn.\nVolgende tabel geeft een overzicht over wat elk Isolation level weer juist doet en welke read phenomena dit voorkomt. In de tabel gaat de snelheid omlaag hoe lager je gaat, maar stijgt wel de veiligheid!\nIsolation level\rDirty reads\rLost updates\rNon-repeatable reads\rPhantoms\rRead Uncommitted\nNo Isolation, any change from the outside is visible to the transaction\rmay occur\rmay occur\rmay occur\rmay occur\rRead Committed\nEach query in a transaction only sees committed stuff\rdon't occur\rmay occur\rmay occur\rmay occur\rRepeatable Read\nEach query in a transaction only sees committed updates at the beginning of transaction\rdon't occur\rdon't occur\rdon't occur\rmay occur\rSerializable\nTransactions are serialized\rdon't occur\rdon't occur\rdon't occur\rdon't occur\rMerk op dat voor elke database implementatie de selectie van een isolation level andere gevolgen kan hebben! Zie de tabellen in https://github.com/changemyminds/Transaction-Isolation-Level-Issue. Het is dus van belang de documentatie van je DB te raadplegen, zoals deze van Oracle, waarin de volgende beschrijving staat voor TRANSACTION_SERIALIZABLE: \u0026ldquo;Dirty reads, non-repeatable reads and phantom reads are prevented.\u0026rdquo;.\nWe komen hier nog later op terug in concurrency in practice wanneer we bijvoorbeeld bij JPA of Hibernate aangeven welk isolation level gewenst is.\nPessimistic Scheduling VS Isolation Levels:- Pessimistic Scheduling is een concrete strategie waarbij resources proactief worden vergrendeld om conflicten te vermijden.- Isolation Levels bepalen via configuratie-instellingen in welke mate transacties elkaars wijzigingen mogen zien en welke anomalieën geaccepteerd worden.\nOefeningen Geef voor onderstaande situaties welk read phenomena optreed. Geef ook twee mogelijke oplossingen waarmee je dit probleem kan oplossen en hoe die twee oplossingen van elkaar verschillen.\nOefening 1: In deze oefening wordt door transactie TX2 de quantity van product 1 aangepast van 10 naar 15 en transactie TX1 wil enerzijds de inkomsten berekenen op elk product apart maar ook in het totaal. Solution:\nread phenomena = Non-repeatable read. De quantity van product 1 wordt door eenzelfde transactie twee keer uitgelezen maar krijgt hier 2 verschillende waarden voor. oplossing: Twee mogelijke oplossingen hiervoor zijn de Repeatable Read en Serializable isolation levels. Bij Repeatable Read wordt ervoor gezorgd dat als een transactie een rij leest, geen andere transactie die rij kan wijzigen totdat de eerste transactie is voltooid. Dit voorkomt dat de waarde verandert tussen opeenvolgende leesoperaties binnen dezelfde transactie. Bij Serializable wordt een nog striktere controle toegepast, waarbij transacties volledig geïsoleerd worden uitgevoerd alsof ze sequentieel plaatsvinden, wat niet alleen non-repeatable reads voorkomt, maar ook andere problemen zoals phantom reads. Het belangrijkste verschil is dat Serializable een hogere mate van isolatie biedt, wat leidt tot betere gegevensintegriteit, maar mogelijk ook tot lagere prestaties door verhoogde kans op blocking en wachttijden. Oefening 2: In deze oefening wordt door transactie TX2 een product toegevoegd aan de SALES tabel en transactie TX1 wil enerzijds de inkomsten berekenen op elk product apart maar ook in het totaal.\nSolution:\nread phenomena = Phantom read. Het aantal producten waarvan de inkomsten worden berekend apart wordt uitgevoerd en daarna wordt de totaal inkomsten berekend op alle producten maar hiertussen is een extra product toegevoegd waardoor de waarden nu niet meer overeenkomen wat zou moeten aangezien de aparte inkomsten en totale inkomsten in 1 transactie berekend werden. (Waarom gebruik je nu niet gewoon de eerder berekende inkomsten om de totale inkomsten te berekenen. Als je aparte functies in je applicatie hebt geschreven om de aparte inkomsten en totale inkomsten queries uit te voeren zou dit voor meer werk zorgen als je nog een 3e functie moet schrijven die de totale inkomsten berekend op basis van de aparte queries. Dit is zot en moet het DBMS voor ons oplossen en moeten wij ons zo weinig mogelijk mee bezig houden bij het schrijven van onze applicatie: scheiding van verantwoordelijkheden) oplossing: Twee mogelijke oplossingen hiervoor zijn het gebruik van het Serializable isolatieniveau en het toepassen van range locks: Bij Serializable wordt strikte controle toegepast, waarbij transacties volledig geïsoleerd worden uitgevoerd alsof ze sequentieel plaatsvinden, wat non-repeatable reads voorkomt. Range locks daarentegen vergrendelen expliciet een specifiek bereik van rijen (bijv. WHERE id \u0026gt; 100), waardoor concurrente transacties worden geblokkeerd om binnen dat bereik in te voegen of te wijzigen, maar dit vereist handmatige implementatie en kan deadlocks bevorderen als niet nauwkeurig afgestemd. Het verschil ligt in de automatische vs. handmatige scope-beheersing en de balans tussen databasebrede consistentie (Serializable) versus gerichte controle (range locks) met bijbehorende prestatieafwegingen zoals throughput (het minste met Serializable). Oefening 3: In deze oefening wordt door transactie TX2 de quantity van product 1 aangepast van 10 naar 15 en transactie TX1 wil enerzijds de inkomsten berekenen op elk product apart maar ook in het totaal.\nSolution:\nread phenomena = Dirty read. De quantity van product 1 wordt geupdate tijdens transactie 1 maar is nog niet gecommit. De berekening van de totale inkomsten kan dus fout zijn in het geval dat TX2 wordt gererolled. oplossing: Twee mogelijke oplossingen hiervoor zijn de Read Committed en Repeatable Read isolation levels. Bij Read Committed worden dirty reads voorkomen door alleen gelezen data toe te staan die al is gecommit, wat automatisch werkt en transacties beperkt tot het lezen van stabiele gegevens, maar geen bescherming biedt tegen non-repeatable reads of phantom reads. Bij Repeatable Read wordt ervoor gezorgd dat als een transactie een rij leest, geen andere transactie die rij kan wijzigen totdat de eerste transactie is voltooid. Dit voorkomt dat een waarde wordt uitgelezen van een andere transactie die misschien gerollebacked wordt, aangezien die transactie moet wachten tot de eerste klaar is. Het belangrijkste verschil is dat Repeatable Read een hogere mate van isolatie biedt, wat leidt tot betere gegevensintegriteit, maar mogelijk ook tot lagere prestaties door verhoogde kans op blocking en wachttijden. Denkvragen Waarom is het phantom read probleem niet opgelost bij isolation level 3 (repeatable read)?\nSolution:\nHoewel het voorkomt dat rijen worden gewijzigd of verwijderd, staat het nog steeds toe dat nieuwe rijen worden toegevoegd die voldoen aan de zoekcriteria van een query. Als gevolg hiervan kan een transactie die herhaaldelijk een query uitvoert, verschillende resultaten zien vanwege toegevoegde rijen tussen de verschillende uitvoeringen van de query. Dit is het \u0026ldquo;phantom read\u0026rdquo; probleem. Hoe weet je wanneer je best optimistic vs pessimistic scheduling gebruikt?\nSolution:\nStart optimistisch door te vertrouwen op lage conflicten en minimale locks, maar als je merkt dat er frequent errors (zoals rollbacks of conflicten) optreden die de prestatie of data-integriteit schaden, schakel dan over naar een pessimistische aanpak door vooraf locks te plaatsen om concurrentie te beheersen en betere voorspelbaarheid te garanderen. Deze verschuiving is logisch bij schrijf-zware workloads of kritieke systemen waar preventie van conflicten prioriteit heeft boven flexibiliteit. Is het cruciaal voor je programma echter dat data-integriteit niet geschaad mag worden dan kies je best initieel al voor een pessimistische aanpak. Kan je nog andere situaties verzinnen waarin een deadlock kan voorkomen? Welk isolation level of scheduling algoritme lost dit op?\nSolution:\nStel, twee banktransacties gebeuren tegelijk: Transactie A wil €100 van rekening X naar Y overmaken (vergrendelt eerst X, dan Y). Transactie B wil €50 van Y naar X overmaken (vergrendelt eerst Y, dan X). Als beide transacties hun eerste lock hebben maar wachten op de tweede, ontstaat een deadlock: beide kunnen niet verder. Om deadlocks op te lossen, kunnen verschillende isolation levels en scheduling-algoritmen worden gebruikt zoals \u0026ldquo;Serializable\u0026rdquo;. Dit kan deadlocks helpen voorkomen door transacties te dwingen in een serialiseerbare volgorde te worden uitgevoerd, waarbij conflicten worden voorkomen en de kans op deadlocks wordt verminderd. Het nadeel is natuurlijk performantie. Algemeen: Deadlocks los je op met preventie (voorkom inconsistente lock-volgordes via isolation levels) of detectie (forceer een rollback op basis van time-outs). Wat is de verantwoordelijkheid van de DBMS\u0026rsquo;s transactie management systeem in verhouding tot de ACID eigenschappen?\nSolution:\nDoor het implementeren van transactiemanagementfunctionaliteiten zoals concurrency control, logging en herstelmechanismen, zorgt het systeem ervoor dat transacties consistent, geïsoleerd en duurzaam worden uitgevoerd, waardoor de betrouwbaarheid en integriteit van de database worden gewaarborgd. Atomicity en Durability: Transaction logs, herstelprocedures na crashes. Isolation: Pessimistische locks (bijv. SELECT FOR UPDATE), Isolation levels. (Consistency: Constraints (UNIQUE, FOREIGN KEY), triggers.) Welke impact heeft lock granulariteit op transactie throughput?\nSolution:\nThroughput is afhankelijk van het type fouten dat je gaat tegenkomen VS de isolation levels die rollbacks preventen maar ook wel traag kunnen zijn. Je moet dus steeds proberen de beste keuze te maken voor jouw specifieke geval. Bedenk en teken zelf een situatie waar een Lost Update voorkomt.\n"
},
{
	"uri": "http://localhost:1313/db-course/apis/jdbi/",
	"title": "JDBI",
	"tags": [],
	"description": "",
	"content": "Queries/Objecten in Java DataBase Interface v3 (JDBI) Jdbi (Java DataBase Interface v3) is een lightweight library geschreven bovenop JDBC. Het gebruikt dus de interne Java API om te communiceren tussen de database en de Java applicatie. Echter, het maakt het leven voor ons als ontwikkelaar op heel wat vlakken véél aangenamer: waar JDBC eerder database-driven en dialect-afhankelijk is, is Jdbi eerder user-driven en met behulp van plugins dialect-onafhankelijk.\nJDBI3 is opgedeeld in modules, waarvan wij de volgende drie gaan gebruiken:\njdbi3-core (altijd nodig) - voor JDBC zit dit in de JDK. jdb3-sqlobject - voor de eenvoudige mapping naar Plain Old Java Objects (POJOs) Nog steeds je mysql driver: implementation 'mysql:mysql-connector-java:8.0.33' Voor SQLite heb je ook nog volgende implementation nodig jdbi3-sqlite.\nimplementation \u0026#39;mysql:mysql-connector-java:8.0.33\u0026#39; // JDBI implementation \u0026#39;org.jdbi:jdbi3-core:3.45.0\u0026#39; implementation \u0026#39;org.jdbi:jdbi3-sqlobject:3.45.0\u0026#39; Met JDBI3 wordt op de volgende manier Java met de DB verbonden:\ngraph LR;\rJava[Java]\rJdbi[Jdbi3-core]\rJDBC[JDBC]\rJMYSQL[Jdbi3-MySQL]\rMYSQL[MySQL-JDBC]\rDB[(MyuSQL Database)]\rsubgraph Java space\rJava --\u003e Jdbi\rJdbi --\u003e JDBC\rJdbi --\u003e JMYSQL\rJMYSQL -.-\u003e MYSQL\rJDBC -.-\u003e MYSQL\rend\rsubgraph DB space\rMYSQL --\u003e DB\rend\rEr komt dus één blokje bij tussen Java en JDBC: we gebruiken niet langer de ingebouwde JDK interfaces maar rechtstreeks de jdbi-core dependency die via JDBC de MySQL connectie maakt.\nOm voorgaand JDBC demo (met enkel een simpele student) te implementeren in Jdbi3 hebben we eerst een extractie van een interface nodig voor de repository acties, zodat we snel kunnen switchen tussen onze verschillende implementaties:\npublic interface StudentRepository { List\u0026lt;Student\u0026gt; getStudentsByName(String student); void saveNewStudent(Student student); void updateStudent(Student student); } Nu kan StudentRepositoryJdbcImpl (hernoem bovenstaande) en onze nieuwe StudentRepositoryJdbi3Impl de interface implements-en. Afhankelijk van een instelling bijvoorbeeld kunnen we switchen van SQL leverancier, zolang de code overal de interface gebruikt.\ngraph LR;\rMain[Controlller]\rInterface{StudentRepository}\rJdbc[StudentRepositoryJdbcImpl]\rJdbi[StudentRepositoryJdbi3Impl]\rMain --\u003e Interface\rInterface --\u003e Jdbc\rInterface --\u003e Jdbi\rJDBC vs Jdbi3 Geen idee waar te beginnen? Hier: http://jdbi.org/\n1. Connection openen In plaats van JDBC\u0026rsquo;s DriverManager.getConnection() om de Connection instance te bootstrappen, gebruiken wij gewoon Jdbi.create() met ook drie parameters, namelijk dezelfde ConnectionString, user en password.\nJdbi jdbi = Jdbi.create(connectionString, username, password); 2. Query uitvoeren In plaats van de vervelende checked SQLExceptions en de createStatement() code, heb je nu de keuze om ofwel de Fluent API te gebruiken:\nreturn jdbi.withHandle(handle -\u0026gt; { return handle.createQuery(\u0026#34;SELECT * FROM student WHERE naam = :naam\u0026#34;) .bind(\u0026#34;naam\u0026#34;, student) .mapToBean(Student.class) .list(); }); Merk op dat Jdbi3 er voor kan zorgen dat de resultaten van je query automatisch worden vertaald naar een Student instantie door middel van bean mapping: de mapToBean() methode. Die gaat via reflectie alle kolomnamen 1-op-1 mappen op properties van je object dat je wenst te mappen (Je kolomnamen moeten dan wel overeenkomen met de attribuut namen van je Java Klasse). Er zijn ook nog andere mogelijkheden, zoals mappen op een HashMap, ea:\nOm ervoor te zorgen dat de beans correct werken is het belangrijk dat je een goed werkende equals(Object o) methode hebt voor je Java Klassen die je met .bindBean of .mapToBean wil gebruiken.\nEen query manueel behandelen Voorbeeld:\njdbi.withHandle(handle -\u0026gt; { // Create a Query object with a named parameter for the student Query query = handle.createQuery( \u0026#34;SELECT v.* FROM student_volgt_vak svv JOIN vak v ON svv.vak = v.vaknr WHERE svv.student = :student\u0026#34;) .bind(\u0026#34;student\u0026#34;, studentId); // Iterate over each row in the result set manually for (Row row : query) { // Manually extract values from the row int vaknr = row.getInt(\u0026#34;vaknr\u0026#34;); String vakNaam = row.getString(\u0026#34;naam\u0026#34;); // Process other columns as needed... // You can now handle the output manually (e.g., print, collect, or process data) System.out.println(\u0026#34;Vak: \u0026#34; + vaknr + \u0026#34; - \u0026#34; + vakNaam); } return null; // or any other appropriate return value }); Enkele methoden met uitleg Voorbeeld 1: een create jdbi.withHandle(handle -\u0026gt; { return handle.createUpdate(\u0026#34;INSERT INTO student (studnr, naam, voornaam, goedBezig) VALUES (:studnr, :naam, :voornaam, :goedBezig)\u0026#34;) .bindBean(student) .execute(); }); withHandle\njdbi.withHandle(handle -\u0026gt; { ... }) De withHandle methode zorgt voor het openen van een databaseverbinding (handle). Je geeft een lambda (anonieme functie -\u0026gt;) door waarin je met de handle jouw SQL-statements bouwt en uitvoert.. Na afloop wordt de handle/connectie automatisch gesloten. Dit zorgt voor goed resource management.\nhandle.createUpdate(...) Hiermee maak je een SQL INSERT-statement aan. De SQL bevat benoemde parameters (bijv. :studnr, :naam, etc.) die later gekoppeld worden aan waarden.\n.bindBean(student) Hiermee koppel je alle eigenschappen van het student object aan de benoemde parameters in je SQL. De velden in student (zoals studnr, naam, enz.) worden automatisch gekoppeld aan de overeenkomstige parameters.\n.execute() Dit voert de INSERT-operatie daadwerkelijk uit. Het retourneert meestal het aantal rijen dat is ingevoegd in de database.\nVoorbeeld 2: een meer manuele create public void addStudentToDb(Student student) { jdbi.withHandle(handle -\u0026gt; { return handle.execute(\u0026#34;INSERT INTO student (studnr, naam, voornaam, goedBezig) VALUES (?, ?, ?, ?)\u0026#34;, student.getStudnr(), student.getNaam(), student.getVoornaam(), student.isGoedBezig()); }); } Vergelijkbaar met een prepared statement\nVoorbeeld 3: een read (Student) jdbi.withHandle(handle -\u0026gt; { return handle.createQuery(\u0026#34;SELECT * FROM student WHERE studnr = :nummer\u0026#34;) .bind(\u0026#34;nummer\u0026#34;, stud_nr) .mapToBean(Student.class) .first(); }); handle.createQuery(...) Hiermee maak je een SELECT-query aan. De query haalt alle kolommen op van de tabel student waar studnr gelijk is aan een bepaalde waarde (hier aangeduid met :nummer).\n.bind(\u0026#34;nummer\u0026#34;, stud_nr) Hiermee koppel je de waarde van de variabele stud_nr aan de parameter :nummer in de SQL-query.\n.mapToBean(Student.class) Na het uitvoeren van de query zet JDBI de resultaatrij(en) om in een object van de klasse Student. Hierbij worden de kolomnamen in de database vergeleken met de velden in de Student-klasse.\n.first() Deze functie haalt het eerste resultaat op van de query. Als je er zeker van bent dat de query precies één resultaat oplevert, kun je deze methode gebruiken om direct het Student object te verkrijgen.\nNog meer methoden createUpdate: Hiermee maak je een SQL-statement aan dat de database wijzigt (bijvoorbeeld een INSERT, UPDATE of DELETE).\ncreateQuery: Hiermee maak je een SQL SELECT-query aan. Dit gebruik je wanneer je gegevens uit de database wilt ophalen.\nexecute: Deze functie voert het eerder samengestelde SQL-statement (zoals een INSERT of UPDATE) daadwerkelijk uit op de database. Vaak retourneert het een integer met het aantal rijen dat is aangetast.\nbind: Hiermee koppel je een enkele waarde aan een benoemde parameter in je SQL-statement.\nbindBean: Hiermee worden alle eigenschappen van een Java bean (bijvoorbeeld een Student object) automatisch gekoppeld aan de benoemde parameters in je SQL-statement.\nmapToBean: Nadat je een SELECT-query hebt uitgevoerd, zet deze functie elke rij uit het resultaat om in een Java bean (bijvoorbeeld een Student object). JDBI vergelijkt de kolomnamen uit de query met de velden in de bean.\nOefening Hier vind je een zipfolder met een oplossing voor onderstaande oefening: dashboard voor database met enkel studenten tabel\n"
},
{
	"uri": "http://localhost:1313/db-course/transacties/",
	"title": "RDBMS Transacties",
	"tags": [],
	"description": "",
	"content": "Transaction management Zie menu links.\n"
},
{
	"uri": "http://localhost:1313/db-course/xml/xpath/",
	"title": "XPath",
	"tags": [],
	"description": "",
	"content": "XPath XPath is een taal die we gebruiken om specifieke elementen of attributen te vinden in een XML bestand. Zoals je in de voorbije oefeningen al hebt gemerkt zijn XML bestanden nogal groot en niet zo makkelijk in een oogopslag om alle informatie uit te halen.\nLaten we ons voorbeeldbestand nemen:\n\u0026lt;boeken\u0026gt; \u0026lt;boek genre=\u0026#34;Non-Fiction\u0026#34;\u0026gt; \u0026lt;titel\u0026gt;Mythos\u0026lt;/titel\u0026gt; \u0026lt;auteur\u0026gt;Stephen Fry\u0026lt;/auteur\u0026gt; \u0026lt;jaar\u0026gt;2017\u0026lt;/jaar\u0026gt; \u0026lt;/boek\u0026gt; \u0026lt;boek genre=\u0026#34;biography\u0026#34;\u0026gt; \u0026lt;titel\u0026gt;Scar Tissue\u0026lt;/titel\u0026gt; \u0026lt;auteur\u0026gt;Anthony Kiedis\u0026lt;/auteur\u0026gt; \u0026lt;jaar\u0026gt;2004\u0026lt;/jaar\u0026gt; \u0026lt;/boek\u0026gt; \u0026lt;boek genre=\u0026#34;fantasy\u0026#34;\u0026gt; \u0026lt;titel\u0026gt;The Lost Metal\u0026lt;/titel\u0026gt; \u0026lt;auteur\u0026gt;Brandon Sanderson\u0026lt;/auteur\u0026gt; \u0026lt;jaar\u0026gt;2022\u0026lt;/jaar\u0026gt; \u0026lt;/boek\u0026gt; \u0026lt;/boeken\u0026gt; XPath voorbeelden /\nDit selecteert het meest top level object. In de praktijk geeft dit dus het hele document terug.\n/boeken\nDit zou het boeken element teruggeven. Wat dus ook het hele bestand is.\n/boeken/boek/titel\nGeef alle titels die vallen in de hiërarchie boeken -\u0026gt; boek.\n//titel\nGeef alle titels, eender waar in het document.\n//boek/@genre\nGeef alle genre attributen die gekoppeld zijn aan een boek element.\n//boek[@genre='fantasy']\nGeef alle boek elementen terug die in het genre fantasy vallen.\n//boek/*\nGeef alle subelementen van het boek element.\nOefeningen Hieronder vind je een xml bestand waarop we de XPath oefeningen gaan uitvoeren. Je kan je XPath valideren op deze online XPath evaluator.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;catalog\u0026gt; \u0026lt;book id=\u0026#34;bk101\u0026#34;\u0026gt; \u0026lt;author\u0026gt;Gambardella, Matthew\u0026lt;/author\u0026gt; \u0026lt;title\u0026gt;XML Developer\u0026#39;s Guide\u0026lt;/title\u0026gt; \u0026lt;genre fiction=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;maingenre\u0026gt;Computer\u0026lt;/maingenre\u0026gt; \u0026lt;subgenre\u0026gt;XML\u0026lt;/subgenre\u0026gt; \u0026lt;/genre\u0026gt; \u0026lt;price\u0026gt;44.95\u0026lt;/price\u0026gt; \u0026lt;publish_date\u0026gt;2000-10-01\u0026lt;/publish_date\u0026gt; \u0026lt;description\u0026gt;An in-depth look at creating applications with XML.\u0026lt;/description\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;book id=\u0026#34;bk102\u0026#34;\u0026gt; \u0026lt;author\u0026gt;Ralls, Kim\u0026lt;/author\u0026gt; \u0026lt;title\u0026gt;Midnight Rain\u0026lt;/title\u0026gt; \u0026lt;genre fiction=\u0026#34;1\u0026#34;\u0026gt; \u0026lt;maingenre\u0026gt;Fantasy\u0026lt;/maingenre\u0026gt; \u0026lt;subgenre\u0026gt;Low Fantasy\u0026lt;/subgenre\u0026gt; \u0026lt;/genre\u0026gt; \u0026lt;price\u0026gt;5.95\u0026lt;/price\u0026gt; \u0026lt;publish_date\u0026gt;2000-12-16\u0026lt;/publish_date\u0026gt; \u0026lt;description\u0026gt;A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.\u0026lt;/description\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;book id=\u0026#34;bk103\u0026#34;\u0026gt; \u0026lt;author\u0026gt;Corets, Eva\u0026lt;/author\u0026gt; \u0026lt;title\u0026gt;Maeve Ascendant\u0026lt;/title\u0026gt; \u0026lt;genre fiction=\u0026#34;1\u0026#34;\u0026gt; \u0026lt;maingenre\u0026gt;Fantasy\u0026lt;/maingenre\u0026gt; \u0026lt;subgenre\u0026gt;Dystopian Fantasy\u0026lt;/subgenre\u0026gt; \u0026lt;/genre\u0026gt; \u0026lt;price\u0026gt;5.95\u0026lt;/price\u0026gt; \u0026lt;publish_date\u0026gt;2000-11-17\u0026lt;/publish_date\u0026gt; \u0026lt;description\u0026gt;After the collapse of a nanotechnology society in England, the young survivors lay the foundation for a new society.\u0026lt;/description\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;book id=\u0026#34;bk104\u0026#34;\u0026gt; \u0026lt;author\u0026gt;Corets, Eva\u0026lt;/author\u0026gt; \u0026lt;title\u0026gt;Oberon\u0026#39;s Legacy\u0026lt;/title\u0026gt; \u0026lt;genre fiction=\u0026#34;1\u0026#34;\u0026gt; \u0026lt;maingenre\u0026gt;Fantasy\u0026lt;/maingenre\u0026gt; \u0026lt;subgenre\u0026gt;Dystopian Fantasy\u0026lt;/subgenre\u0026gt; \u0026lt;/genre\u0026gt; \u0026lt;price\u0026gt;5.95\u0026lt;/price\u0026gt; \u0026lt;publish_date\u0026gt;2001-03-10\u0026lt;/publish_date\u0026gt; \u0026lt;description\u0026gt;In post-apocalypse England, the mysterious agent known only as Oberon helps to create a new life for the inhabitants of London. Sequel to Maeve Ascendant.\u0026lt;/description\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;book id=\u0026#34;bk105\u0026#34;\u0026gt; \u0026lt;author\u0026gt;Corets, Eva\u0026lt;/author\u0026gt; \u0026lt;title\u0026gt;The Sundered Grail\u0026lt;/title\u0026gt; \u0026lt;genre fiction=\u0026#34;1\u0026#34;\u0026gt; \u0026lt;maingenre\u0026gt;Fantasy\u0026lt;/maingenre\u0026gt; \u0026lt;subgenre\u0026gt;Dystopian Fantasy\u0026lt;/subgenre\u0026gt; \u0026lt;/genre\u0026gt; \u0026lt;price\u0026gt;5.95\u0026lt;/price\u0026gt; \u0026lt;publish_date\u0026gt;2001-09-10\u0026lt;/publish_date\u0026gt; \u0026lt;description\u0026gt;The two daughters of Maeve, half-sisters, battle one another for control of England. Sequel to Oberon\u0026#39;s Legacy.\u0026lt;/description\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;book id=\u0026#34;bk106\u0026#34;\u0026gt; \u0026lt;author\u0026gt;Randall, Cynthia\u0026lt;/author\u0026gt; \u0026lt;title\u0026gt;Lover Birds\u0026lt;/title\u0026gt; \u0026lt;genre fiction=\u0026#34;1\u0026#34;\u0026gt; \u0026lt;maingenre\u0026gt;Romance\u0026lt;/maingenre\u0026gt; \u0026lt;/genre\u0026gt; \u0026lt;price\u0026gt;4.95\u0026lt;/price\u0026gt; \u0026lt;publish_date\u0026gt;2000-09-02\u0026lt;/publish_date\u0026gt; \u0026lt;description\u0026gt;When Carla meets Paul at an ornithology conference, tempers fly as feathers get ruffled.\u0026lt;/description\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;book id=\u0026#34;bk107\u0026#34;\u0026gt; \u0026lt;author\u0026gt;Thurman, Paula\u0026lt;/author\u0026gt; \u0026lt;title\u0026gt;Splish Splash\u0026lt;/title\u0026gt; \u0026lt;genre fiction=\u0026#34;1\u0026#34;\u0026gt; \u0026lt;maingenre\u0026gt;Romance\u0026lt;/maingenre\u0026gt; \u0026lt;/genre\u0026gt; \u0026lt;price\u0026gt;4.95\u0026lt;/price\u0026gt; \u0026lt;publish_date\u0026gt;2000-11-02\u0026lt;/publish_date\u0026gt; \u0026lt;description\u0026gt;A deep sea diver finds true love twenty thousand leagues beneath the sea.\u0026lt;/description\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;book id=\u0026#34;bk108\u0026#34;\u0026gt; \u0026lt;author\u0026gt;Knorr, Stefan\u0026lt;/author\u0026gt; \u0026lt;title\u0026gt;Creepy Crawlies\u0026lt;/title\u0026gt; \u0026lt;genre fiction=\u0026#34;1\u0026#34;\u0026gt; \u0026lt;maingenre\u0026gt;Horror\u0026lt;/maingenre\u0026gt; \u0026lt;subgenre\u0026gt;Short Stories\u0026lt;/subgenre\u0026gt; \u0026lt;/genre\u0026gt; \u0026lt;price\u0026gt;4.95\u0026lt;/price\u0026gt; \u0026lt;publish_date\u0026gt;2000-12-06\u0026lt;/publish_date\u0026gt; \u0026lt;description\u0026gt;An anthology of horror stories about roaches, centipedes, scorpions and other insects.\u0026lt;/description\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;book id=\u0026#34;bk109\u0026#34;\u0026gt; \u0026lt;author\u0026gt;Kress, Peter\u0026lt;/author\u0026gt; \u0026lt;title\u0026gt;Paradox Lost\u0026lt;/title\u0026gt; \u0026lt;genre fiction=\u0026#34;1\u0026#34;\u0026gt; \u0026lt;maingenre\u0026gt;Science Fiction\u0026lt;/maingenre\u0026gt; \u0026lt;/genre\u0026gt; \u0026lt;price\u0026gt;6.95\u0026lt;/price\u0026gt; \u0026lt;publish_date\u0026gt;2000-11-02\u0026lt;/publish_date\u0026gt; \u0026lt;description\u0026gt;After an inadvertant trip through a Heisenberg Uncertainty Device, James Salway discovers the problems of being quantum.\u0026lt;/description\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;book id=\u0026#34;bk110\u0026#34;\u0026gt; \u0026lt;author\u0026gt;O\u0026#39;Brien, Tim\u0026lt;/author\u0026gt; \u0026lt;title\u0026gt;Microsoft .NET: The Programming Bible\u0026lt;/title\u0026gt; \u0026lt;genre fiction=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;maingenre\u0026gt;Computer\u0026lt;/maingenre\u0026gt; \u0026lt;subgenre\u0026gt;.NET\u0026lt;/subgenre\u0026gt; \u0026lt;/genre\u0026gt; \u0026lt;price\u0026gt;36.95\u0026lt;/price\u0026gt; \u0026lt;publish_date\u0026gt;2000-12-09\u0026lt;/publish_date\u0026gt; \u0026lt;description\u0026gt;Microsoft\u0026#39;s .NET initiative is explored in detail in this deep programmer\u0026#39;s reference.\u0026lt;/description\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;book id=\u0026#34;bk111\u0026#34;\u0026gt; \u0026lt;author\u0026gt;O\u0026#39;Brien, Tim\u0026lt;/author\u0026gt; \u0026lt;title\u0026gt;MSXML3: A Comprehensive Guide\u0026lt;/title\u0026gt; \u0026lt;genre fiction=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;maingenre\u0026gt;Computer\u0026lt;/maingenre\u0026gt; \u0026lt;subgenre\u0026gt;XML\u0026lt;/subgenre\u0026gt; \u0026lt;/genre\u0026gt; \u0026lt;price\u0026gt;36.95\u0026lt;/price\u0026gt; \u0026lt;publish_date\u0026gt;2000-12-01\u0026lt;/publish_date\u0026gt; \u0026lt;description\u0026gt;The Microsoft MSXML3 parser is covered in detail, with attention to XML DOM interfaces, XSLT processing, SAX and more.\u0026lt;/description\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;book id=\u0026#34;bk112\u0026#34;\u0026gt; \u0026lt;author\u0026gt;Galos, Mike\u0026lt;/author\u0026gt; \u0026lt;title\u0026gt;Visual Studio 7: A Comprehensive Guide\u0026lt;/title\u0026gt; \u0026lt;genre fiction=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;maingenre\u0026gt;Computer\u0026lt;/maingenre\u0026gt; \u0026lt;subgenre\u0026gt;Visual Studio\u0026lt;/subgenre\u0026gt; \u0026lt;/genre\u0026gt; \u0026lt;price\u0026gt;49.95\u0026lt;/price\u0026gt; \u0026lt;publish_date\u0026gt;2001-04-16\u0026lt;/publish_date\u0026gt; \u0026lt;description\u0026gt;Microsoft Visual Studio 7 is explored in depth, looking at how Visual Basic, Visual C++, C#, and ASP+ are integrated into a comprehensive development environment.\u0026lt;/description\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;/catalog\u0026gt; Geef alle prijzen weer. Schrijf hiervoor 2 verschillende XPath queries die hetzelfde resultaat geven. Geef de titel van het boek met id bk110. Geef de description van alle boeken. Geef alle non-fictie boeken Lijst alle subgenres van de fictie boeken op. Solution: /catalog/book[genre/@fiction='1']/genre/subgenre/text()\n"
},
{
	"uri": "http://localhost:1313/db-course/apis/",
	"title": "Database APIs",
	"tags": [],
	"description": "",
	"content": "Casus: Tornooisysteem Tennis Tennis Vlaanderen organiseert tennistornooien over heel Vlaanderen. Deze casus werkt een voorbeeld uit van een soort systeembeheer voor die tornooien. Wat dat systeembeheer software pakket juist moet kunnen staat hieronder beschreven. (Dit is natuurlijk maar een beperkte versie van hoe zo een werkelijk systeem er zou uitzien bv. een datum en uur voor de wedstrijden ontbreken \u0026hellip;)\nMet de software moet je tennisspelers kunnen aanmaken. Elke tennisspeler heeft een globale ID bij Tennis Vlaanderen die gebruikt wordt als identifier wanneer een persoon zich inschrijft voor een toernooi. Een speler moet op zijn Tennis Vlaanderen-dashboard een globaal overzicht kunnen opvragen van het aantal gespeelde matchen, het aantal gewonnen matchen, verloren matchen en de hoogst bereikte plaats in een toernooi (bv. Finalist T.C.Ham reeks enkel Heren t.e.m. 5 punten OF Winnaar G.T.Tessenderlo reeks enkel Dames t.e.m. 10 punten).\nNaast de fysieke parameters van een tennisspeler zoals leeftijd, geslacht, \u0026hellip;, moet de applicatie ook de ranking van elke tennisspeler bijhouden. Deze ranking is een waarde van 1 tot en met 10 waarbij 1 de laagste ranking is en 10 de hoogste.\nDaarnaast kan een tennisspeler ook een rol als wedstrijdleider en/of scheidsrechter opnemen.\nEen wedstrijdleider regelt de planning van het toernooi, is het aanspreekpunt voor vragen en doet op de wedstrijddagen de inschrijvingen van de spelers. Aan elk toernooi moet minstens één wedstrijdleider gekoppeld zijn. Aan elke finalewedstrijd van een toernooi (= halve finales en grote + kleine finale) moet één scheidsrechter toegewezen worden. Een wedstrijdleider en scheidsrechter moeten ook wel een tennisspeler zijn! Je kan niet wedstrijdleider en scheidsrechter tegelijk zijn\nElk toernooi vindt plaats op een bepaalde Tennisclub (met een bepaald adres, telefoonnummer(s), email adres, \u0026hellip;) en bestaat uit minstens 4 reeksen waarvoor je kan inschrijven (\u0026ldquo;Enkel Heren t.e.m. 5 punten\u0026rdquo;, \u0026ldquo;enkel Heren t.e.m. 10 punten\u0026rdquo;, \u0026ldquo;enkel Dames t.e.m. 5 punten\u0026rdquo;, \u0026ldquo;enkel Dames t.e.m. 10 punten\u0026rdquo;). Een speler kan altijd voor een hogere reeks inschrijven maar niet voor een lagere (bv. speler met 3 punten kan inschrijven in de reeks t.e.m. 10 punten, maar een speler met 6 punten kan niet inschrijven voor de reeks t.e.m. 5 punten).\nEen toernooi wordt georganiseerd volgens het principe van rechtstreekse uitschakeling: de winnaar gaat door naar de volgende ronde (ronde wil zeggen: ronde 4 is 1/4 finale, ronde 16 is 1/16 finale en ronde 1 is finale \u0026hellip;); voor de verliezer eindigt het toernooi. Dit proces loopt door tot er per wedstrijdreeks één winnaar overblijft. Indien het aantal spelers in een wedstrijdreeks geen match van twee is, worden in de eerste ronde zoveel spelers vrijgesteld als nodig. Zij kwalificeren zich rechtstreeks voor de tweede ronde. Dit wordt per loting bepaald (random dus).\nEER-schema (My)SQL database Klik hier om de mysql code te zien/verbergen om de schema's aan te maken voor de 'tennisvlaanderen' database 🔽\rDROP TABLE IF EXISTS tennisspeler_speelt_wedstrijd; DROP TABLE IF EXISTS tennisspeler_inschrijving_wedstrijdreeks; DROP TABLE IF EXISTS wedstrijd; DROP TABLE IF EXISTS wedstrijdreeks; DROP TABLE IF EXISTS tornooi; DROP TABLE IF EXISTS tennisspeler; DROP TABLE IF EXISTS tennisclub_email; DROP TABLE IF EXISTS tennisclub; CREATE TABLE tennisclub ( clubnummer INT PRIMARY KEY, nummer INT, straatnaam VARCHAR(255), gemeente VARCHAR(255) ); CREATE TABLE tennisclub_email ( id INT AUTO_INCREMENT PRIMARY KEY, clubnummer INT, email_address VARCHAR(255), FOREIGN KEY (clubnummer) REFERENCES tennisclub(clubnummer) ); CREATE TABLE tennisspeler ( tennisvlaanderenid INT AUTO_INCREMENT PRIMARY KEY, geslacht ENUM(\u0026#39;M\u0026#39;, \u0026#39;V\u0026#39;), geboortedatum DATE, ranking INT, rol ENUM(\u0026#39;Wedstrijdleider\u0026#39;, \u0026#39;Scheidsrechter\u0026#39;) DEFAULT NULL, diploma TEXT, naam TEXT, CHECK (ranking BETWEEN 0 AND 10) ); CREATE TABLE tornooi ( id INT AUTO_INCREMENT PRIMARY KEY, startdatum DATE, einddatum DATE, wedstrijdleider INT, tennisclub INT, FOREIGN KEY (wedstrijdleider) REFERENCES tennisspeler(tennisvlaanderenid), FOREIGN KEY (tennisclub) REFERENCES tennisclub(clubnummer) ); CREATE TABLE wedstrijdreeks ( id INT AUTO_INCREMENT PRIMARY KEY, max_punten INT, geslacht ENUM(\u0026#39;M\u0026#39;, \u0026#39;V\u0026#39;), tornooi_id INT, FOREIGN KEY (tornooi_id) REFERENCES tornooi(id), CHECK (max_punten BETWEEN 0 AND 10) ); CREATE TABLE wedstrijd ( partial_id INT, wedstrijdreeks INT, scheidsrechter INT DEFAULT NULL, ronde INT, winnaar INT DEFAULT NULL, score TEXT DEFAULT NULL, PRIMARY KEY (partial_id, wedstrijdreeks), FOREIGN KEY (wedstrijdreeks) REFERENCES wedstrijdreeks(id), FOREIGN KEY (scheidsrechter) REFERENCES tennisspeler(tennisvlaanderenid), FOREIGN KEY (winnaar) REFERENCES tennisspeler(tennisvlaanderenid) ); CREATE TABLE tennisspeler_inschrijving_wedstrijdreeks ( id INT AUTO_INCREMENT PRIMARY KEY, tennisspeler INT, wedstrijdreeks INT, inschrijvingsdatum DATE, FOREIGN KEY (tennisspeler) REFERENCES tennisspeler(tennisvlaanderenid), FOREIGN KEY (wedstrijdreeks) REFERENCES wedstrijdreeks(id) ); CREATE TABLE tennisspeler_speelt_wedstrijd ( id INT AUTO_INCREMENT PRIMARY KEY, speler1 INT, speler2 INT, partial_id INT, wedstrijdreeks INT, FOREIGN KEY (speler1) REFERENCES tennisspeler(tennisvlaanderenid), FOREIGN KEY (speler2) REFERENCES tennisspeler(tennisvlaanderenid), FOREIGN KEY (partial_id) REFERENCES wedstrijd(partial_id), FOREIGN KEY (wedstrijdreeks) REFERENCES wedstrijd(wedstrijdreeks) ); Klik hier om de mysql code te zien/verbergen om de 'tennisvlaanderen' database te in te vullen met wat test/dummy data 🔽\rDELETE FROM tennisspeler_speelt_wedstrijd; DELETE FROM tennisspeler_inschrijving_wedstrijdreeks; DELETE FROM wedstrijd; DELETE FROM wedstrijdreeks; DELETE FROM tornooi; DELETE FROM tennisspeler; DELETE FROM tennisclub_email; DELETE FROM tennisclub; -- Insert tennis clubs INSERT INTO tennisclub (clubnummer, nummer, straatnaam, gemeente) VALUES (1, 12, \u0026#39;Tennis Straat\u0026#39;, \u0026#39;Antwerpen\u0026#39;), (2, 5, \u0026#39;Racketweg\u0026#39;, \u0026#39;Gent\u0026#39;); -- Insert club emails INSERT INTO tennisclub_email (clubnummer, email_address) VALUES (1, \u0026#39;club1@example.com\u0026#39;), (1, \u0026#39;contact@club1.com\u0026#39;), (2, \u0026#39;info@club2.be\u0026#39;); -- Insert tennis players INSERT INTO tennisspeler (tennisvlaanderenid, geslacht, geboortedatum, ranking, rol, diploma, naam) VALUES (1, \u0026#39;M\u0026#39;, \u0026#39;1990-05-15\u0026#39;, 5, \u0026#39;Wedstrijdleider\u0026#39;, \u0026#39;Diploma X\u0026#39;, \u0026#39;Test Wedstrijdleider\u0026#39;), (2, \u0026#39;V\u0026#39;, \u0026#39;1995-08-20\u0026#39;, 8, NULL, NULL, \u0026#39;Testspeler A\u0026#39;), (3, \u0026#39;M\u0026#39;, \u0026#39;1985-03-10\u0026#39;, 3, \u0026#39;Scheidsrechter\u0026#39;, \u0026#39;Diploma Y\u0026#39;, \u0026#39;Test Scheidsrechter\u0026#39;), (4, \u0026#39;V\u0026#39;, \u0026#39;2000-12-05\u0026#39;, 2, NULL, NULL, \u0026#39;Testspeler B\u0026#39;), (5, \u0026#39;M\u0026#39;, \u0026#39;1996-02-18\u0026#39;, 2, NULL, NULL, \u0026#39;Testspeler C\u0026#39;), (6, \u0026#39;M\u0026#39;, \u0026#39;1992-07-14\u0026#39;, 7, NULL, NULL, \u0026#39;Testspeler D\u0026#39;), (7, \u0026#39;V\u0026#39;, \u0026#39;1998-11-23\u0026#39;, 4, NULL, NULL, \u0026#39;Testspeler E\u0026#39;), (8, \u0026#39;M\u0026#39;, \u0026#39;1989-01-30\u0026#39;, 5, NULL, NULL, \u0026#39;Testspeler F\u0026#39;), (9, \u0026#39;V\u0026#39;, \u0026#39;1993-04-18\u0026#39;, 5, NULL, NULL, \u0026#39;Testspeler G\u0026#39;), (10, \u0026#39;M\u0026#39;, \u0026#39;1997-09-05\u0026#39;, 3, NULL, NULL, \u0026#39;Testspeler H\u0026#39;), (11, \u0026#39;V\u0026#39;, \u0026#39;2001-06-12\u0026#39;, 2, NULL, NULL, \u0026#39;Testspeler I\u0026#39;), (12, \u0026#39;M\u0026#39;, \u0026#39;1994-03-22\u0026#39;, 8, NULL, NULL, \u0026#39;Testspeler J\u0026#39;), (13, \u0026#39;V\u0026#39;, \u0026#39;1990-12-01\u0026#39;, 1, NULL, NULL, \u0026#39;Testspeler K\u0026#39;), (14, \u0026#39;M\u0026#39;, \u0026#39;1987-05-09\u0026#39;, 4, NULL, NULL, \u0026#39;Testspeler L\u0026#39;), (15, \u0026#39;V\u0026#39;, \u0026#39;1999-08-15\u0026#39;, 6, NULL, NULL, \u0026#39;Testspeler M\u0026#39;); -- Insert tournaments INSERT INTO tornooi (id, startdatum, einddatum, wedstrijdleider, tennisclub) VALUES (1, \u0026#39;2023-10-01\u0026#39;, \u0026#39;2023-10-07\u0026#39;, 1, 1), (2, \u0026#39;2023-11-01\u0026#39;, \u0026#39;2023-11-15\u0026#39;, 1, 2), (3, \u0026#39;2024-03-05\u0026#39;, \u0026#39;2024-03-12\u0026#39;, NULL, 2); -- Insert competition series INSERT INTO wedstrijdreeks (id, max_punten, geslacht, tornooi_id) VALUES (1, 5, \u0026#39;M\u0026#39;, 1), (2, 5, \u0026#39;V\u0026#39;, 1), (3, 10, \u0026#39;M\u0026#39;, 1), (4, 10, \u0026#39;V\u0026#39;, 1), (5, 5, \u0026#39;M\u0026#39;, 2), (7, 10, \u0026#39;M\u0026#39;, 2), (8, 10, \u0026#39;V\u0026#39;, 2); -- Insert player registrations INSERT INTO tennisspeler_inschrijving_wedstrijdreeks (tennisspeler, wedstrijdreeks, inschrijvingsdatum) VALUES (5, 1, \u0026#39;2023-09-20\u0026#39;), (8, 1, \u0026#39;2023-09-20\u0026#39;), (14, 1, \u0026#39;2023-09-21\u0026#39;), (10, 1, \u0026#39;2023-09-21\u0026#39;), (4, 2, \u0026#39;2023-09-19\u0026#39;), (11, 2, \u0026#39;2023-09-20\u0026#39;), (13, 2, \u0026#39;2023-09-20\u0026#39;), (9, 2, \u0026#39;2023-09-21\u0026#39;), (7, 2, \u0026#39;2023-09-21\u0026#39;), (6, 3, \u0026#39;2023-09-18\u0026#39;), (8, 3, \u0026#39;2023-09-18\u0026#39;), (12, 3, \u0026#39;2023-09-18\u0026#39;), (1, 3, \u0026#39;2023-09-20\u0026#39;), (3, 3, \u0026#39;2023-09-20\u0026#39;), (10, 3, \u0026#39;2023-09-20\u0026#39;), (5, 3, \u0026#39;2023-09-20\u0026#39;), (14, 3, \u0026#39;2023-09-21\u0026#39;), (2, 4, \u0026#39;2023-09-17\u0026#39;), (9, 4, \u0026#39;2023-09-18\u0026#39;), (15, 4, \u0026#39;2023-09-18\u0026#39;), (7, 4, \u0026#39;2023-09-18\u0026#39;), (11, 4, \u0026#39;2023-09-20\u0026#39;), (6, 7, \u0026#39;2023-09-18\u0026#39;), (8, 7, \u0026#39;2023-09-18\u0026#39;), (12, 7, \u0026#39;2023-09-18\u0026#39;); -- Insert matches INSERT INTO wedstrijd (partial_id, wedstrijdreeks, scheidsrechter, ronde, winnaar, score) VALUES (1, 1, NULL, 2, 5, \u0026#39;6-4, 6-2\u0026#39;), (2, 1, NULL, 2, 14, \u0026#39;6-4, 5-7, 6-0\u0026#39;), (3, 1, 3, 1, 14, \u0026#39;6-1, 6-4\u0026#39;), (1, 2, NULL, 4, 4, \u0026#39;6-0, 6-1\u0026#39;), (2, 2, NULL, 2, 9, \u0026#39;7-6(3), 6-4\u0026#39;), (3, 2, NULL, 2, 7, \u0026#39;6-0, 6-0\u0026#39;), (4, 2, 3, 1, 9, \u0026#39;6-2, 7-5\u0026#39;), (1, 3, NULL, 4, 1, \u0026#39;6-2, 6-2\u0026#39;), (2, 3, NULL, 4, 1, \u0026#39;5-7, 6-2, 6-0\u0026#39;), (3, 3, NULL, 4, 1, \u0026#39;6-3, 6-2\u0026#39;), (4, 3, NULL, 4, 1, \u0026#39;7-5, 6-4\u0026#39;), (5, 3, NULL, 2, 1, \u0026#39;6-2, 6-3\u0026#39;), (6, 3, NULL, 2, 1, \u0026#39;6-2, 6-4\u0026#39;), (7, 3, 3, 1, 1, \u0026#39;6-4, 2-6, 7-5\u0026#39;), (1, 4, NULL, 4, 7, \u0026#39;walkover\u0026#39;), (2, 4, NULL, 2, NULL, NULL), (3, 4, NULL, 2, NULL, NULL), (1, 7, NULL, 2, NULL, NULL), (2, 7, NULL, 1, NULL, NULL); -- Insert player-match participation INSERT INTO tennisspeler_speelt_wedstrijd (speler1, speler2, partial_id, wedstrijdreeks) VALUES (5, 10, 1, 1), (8, 14, 2, 1), (5, 14, 3, 1), (4, 11, 1, 2), (4, 9, 2, 2), (13, 7, 3, 2), (7, 9, 4, 2), (1, 3, 1, 3), (6, 5, 2, 3), (14, 8, 3, 3), (10, 12, 4, 3), (1, 6, 5, 3), (8, 12, 6, 3), (6, 12, 7, 3), (7, 11, 1, 4), (7, 2, 2, 4), (15, 9, 3, 4), (6, 8, 1, 7), (12, NULL, 2, 7); Je kan bovenstaande code kopiëren en opslaan onder filenames zoals create_tables.sql en populate_tables_with_testdata.sql. Door deze onder ./resources-directory van Gradle projecten bijvoorbeeld op te slaan zal het heel simpel zijn om tijdens de oefeningen steeds snel en repeatable testdatabases aan te maken die we kunnen gebruiken om onze code te testen.\nDe dummy data Enkele opmerkingen bij de dummy data die weergeeft welke verschillende scenario\u0026rsquo;s allemaal ingevuld zijn die dan in software getest kunnen worden:\nTornooi 1, wedstrijdreeks M5 heeft 4 inschrijvingen (dus perfecte halve finales) en is volledig uitgespeeld. Tornooi 1, wedstrijdreeks V5 heeft 5 inschrijvingen (dus gedeeltelijk kwartfinale) en is volledig uitgespeeld. Tornooi 1, wedstrijdreeks M10 heeft 8 inschrijvingen (dus perfecte kwart finales) en is volledig uitgespeeld. Speler met id 5, doet ook al in tornooi 1 reeks 1 mee. (net als een heel deel andere spelers) Tornooi 1, wedstrijdreeks V10 5 inschrijvingen (dus gedeeltelijk kwartfinale) en is gedeeltelijk al gespeeld maar is nog niet volledig uitgespeeld. bevat een \u0026lsquo;walkover\u0026rsquo; als resultaat. (iemand die opgeeft) Tornooi 2, wedstrijdreeks M5 heeft geen inschrijvingen. Tornooi 2, wedstrijdreeks V5 bestaat (nog) niet. Tornooi 2, wedstrijdreeks M10 heeft inschrijvingen en wedstrijden gepland, maar er zijn nog geen wedstrijden gespeeld. Tornooi 2, wedstrijdreeks V10 heeft inschrijvingen maar er zijn nog geen wedstrijden gepland. Tornooi 3, wat ook een tornooi van clubnummer 1 is bevat nog geen inschrijvingen, en heeft nog geen wedstrijdleider. Nadenken over de structuur van de database en de interactie met je applicatie Je zal wellicht al gemerkt hebben dat de inhoud van de database niet al te leesbaar is. Zo zie je voor matchen enkel tennisvlaanderenid\u0026rsquo;s wat natuurlijk super onhandig is. Als ik mijn wedstrijd opzoek wil ik de naam van mijn tegenstander zien en niet enkel zijn/haar nummer. Daarvoor dienen dus goede SQL-queries gecombineerd met wat programmeermagic om dit zo weer te geven in de UI.\nBovendien missen er ook nog wat logische elementen die je niet rechtstreeks uit de database kan uitlezen maar die je er wel kan van afleiden. Zo weet je dat als er een wedstrijd van een speler voor een bepaalde reeks te zien is in de wedstrijd-tabel maar de winnaar nog NULL is, dat die wedstrijd dan wel ingepland is, maar nog niet gespeeld is.\nWaarom is het nog belangrijk om na te denken over eigen software? Sommige constraints kunnen niet toegevoegd worden in de definitie van de MySQL-database bijvoorbeeld:\nzorgen dat een speler met meer dan 5 punten niet kan inschrijven in een reeks tot en met 5 punten; zorgen dat een man niet kan inschrijven in een vrouwenreeks en vice versa; zorgen dat een scheidsrechter niet zijn eigen match kan beoordelen; \u0026hellip; Hoe bouw je dit nu in? Dit kan over het algemeen op 2 manieren: voorkomen of error messaging:\nJe kan simpelweg voorkomen dat een vrouw in een mannenreeks kan inschrijven door het onmogelijk te maken in de UI dat een vrouw een inschrijvingsknop heeft voor een mannenreeks. Je kan een error geven wanneer je bijvoorbeeld voor een te lage reeks wil inschrijven. Over het algemeen is het dus belangrijk dat je op zoveel mogelijk plaatsen en in zoveel mogelijk lagen restricties controleert zodat er geen fouten kunnen gebeuren. Dat doe je door constraints mee te geven in je database, error handling te doen, je UI zo te voorzien dat fouten maken bijna onmogelijk wordt \u0026hellip; Dit vraagt natuurlijk heel wat denkwerk voor programmeur, maar dit hoort zeker in deze gevallen tot zijn/haar job. Een werkende database en UI voorzien kan 20% van het werk zijn en ervoor zorgen dat je enkel de juiste dingen kan doen in de UI zal wellicht uit 80% van het werk in beslag nemen. (Daarom is het ook belangrijk om goede en voldoende testen te schrijven!)\nTips Test queries die je wil gebruiken in je programma eerst uit in een GUI tool zoals PHPMyAdmin of Adminer om te testen of ze werken en al een idee te hebben van hoe de output eruit ziet zodat je de returns correct kan processen.\n"
},
{
	"uri": "http://localhost:1313/db-course/transacties/failures-rollbacks/",
	"title": "Failures-Rollbacks",
	"tags": [],
	"description": "",
	"content": "Voorbereidende CREATE statements (Dit is SQLite syntax!) Zie SQLite manual:\nDROP TABLE IF EXISTS student; CREATE TABLE student( studnr INT NOT NULL PRIMARY KEY, naam VARCHAR(200) NOT NULL, voornaam VARCHAR(200), goedbezig BOOL ); DROP TABLE IF EXISTS log; CREATE TABLE log( id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT, date DATETIME DEFAULT CURRENT_TIMESTAMP, foreign_id INT NOT NULL, msg TEXT ); INSERT INTO student(studnr, naam, voornaam, goedbezig) VALUES (123, \u0026#39;Trekhaak\u0026#39;, \u0026#39;Jaak\u0026#39;, 0); INSERT INTO student(studnr, naam, voornaam, goedbezig) VALUES (456, \u0026#39;Peeters\u0026#39;, \u0026#39;Jos\u0026#39;, 0); INSERT INTO student(studnr, naam, voornaam, goedbezig) VALUES (890, \u0026#39;Dongmans\u0026#39;, \u0026#39;Ding\u0026#39;, 1); System failure simulatie In SQLite met DB Browser Gegeven een aantal SQL statements, waarvan niet alle statements kloppen, maar die wel allemaal bij elkaar horen als één atomaire transactie. Dat betekent dat als er één van die statements misloopt, de rest teruggedraait zou moeten worden. Het spreekt voor zich dat zonder speciale handelingen, zoals het beheren van transacties, dit niet gebeurt. Een eenvoudig voorbeeld demonstreert dit.\nUPDATE student SET voornaam = \u0026#39;Jaqueline\u0026#39; WHERE studnr = 123; INSERT INTO oeitiskapot; INSERT INTO log(foreign_id, msg) VALUES (123, \u0026#39;Voornaam vergissing\u0026#39;); INSERT INTO student(studnr, naam, voornaam, goedbezig) VALUES (445, \u0026#39;Klakmans\u0026#39;, \u0026#39;Jef\u0026#39;, 1); INSERT INTO log(foreign_id, msg) VALUES (445, \u0026#39;Nieuwe student registratie\u0026#39;); Plak dit in de \u0026ldquo;Execute SQL\u0026rdquo; tab van de SQLite DB Browser. Het resultaat is een foutboodschap:\nnear \u0026#34;;\u0026#34;: syntax error: INSERT INTO oeitiskapot; Maar: het eerste UPDATE statement, voor de foute regel, is wel uitgevoerd:\nOefeningen Probeer bovenstaande voorbeeld zelf uit in de SQLite DB Browser. Als je jezelf ervan verzekerd hebt dat inderdaad het eerste UPDATE statement wordt uitgevoerd, terwijl wij dat in één ACID blok willen, ga dan over naar de volgende oefening. In SQLite is het starten van een transactie erg eenvoudig: zie SQLite transaction tutorials van tutorialspoint.com. BEGIN; en COMMIT; zijn voldoende. Probeer dit uit in bovenstaande voorbeeld om er voor te zorgen dat de voornaam van Jaak niet wordt gewijzigd. Om met een \u0026ldquo;clean slate\u0026rdquo; te herbeginnen kan je gewoon de voorbereidende SQL code copy/pasten en opnieuw uitvoeren. Merk op dat dit nog steeds het ongewenst effect heeft dat de student zijn/haar naam wordt gewijzigd. We moeten expliciet zelf ROLLBACK; aanroepen. Probeer een nieuwe student toe te voegen: eentje met studentennummer, en eentje zonder. Dat tweede kan in principe niet door de NOT NULL constraint. Wrap beide statements in een transactie. Let Op: Het zou kunnen dat SQLite de volgende fout geeft: cannot start a transaction within a transaction: BEGIN;. Queries die geplakt worden in het \u0026ldquo;execute SQL\u0026rdquo; scherm worden meestal (onzichtbaar, achter de schermen) gewrapped in transacties. Stop de huidige transactie door COMMIT; uit te voeren met de knop \u0026ldquo;execute single SQL line\u0026rdquo;.\nLet Op: Het zou kunnen dat BEGIN TRANSACTION; de transactie niet goed encapsuleert, maar simpelweg BEGIN; wel. Het TRANSACTION keyword is optioneel volgens de SQLite docs en lijkt, afhankelijk van de geïnstalleerde SQLite versie, ander gedrag te vertonen.\nIn SQLite met Java/JDBC SQLite/JDBC uitleg: zie APIs - JDBC.\nGebruik nu connection.setAutoCommit(false). Deze regel is nodig omdat in JDBC standaard elke SQL statement aanschouwd wordt als een onafhankelijke transactie, die automatisch wordt gecommit:\nWhen a connection is created, it is in auto-commit mode. This means that each individual SQL statement is treated as a transaction and is automatically committed right after it is executed. (To be more precise, the default is for a SQL statement to be committed when it is completed, not when it is executed. A statement is completed when all of its result sets and update counts have been retrieved. In almost all cases, however, a statement is completed, and therefore committed, right after it is executed.)\nJe kan nu manueel committen met connection.commit();.\nZie JDBC Basics in Oracle docs: https://docs.oracle.com/javase/tutorial/jdbc/basics/transactions.html\nOefeningen Maak een nieuw Gradle project aan en connecteer naar je SQLite database. Merk op dat, bij connectionstring \u0026quot;jdbc:sqlite:sample.db\u0026quot;, automatisch een lege .db file wordt aangemaakt indien de database niet bestaat. Probeer met behulp van executeUpdate() en executeQuery() bovenstaande system failure te veroorzaken. Je kan de \u0026ldquo;foute SQL\u0026rdquo; (met \u0026ldquo;oeitiskapot\u0026rdquo;) gewoon in een string in java copy/pasten. executeUpdate() kan verschillende statements tegelijkertijd verwerken. Verifieer dat de naam foutief toch wordt gewijzigd met een SELECT() nadat je de fout hebt opgevangen in een try { } block. Je moet nu dan ook wel de juiste aanpassingen doen om met een SQLite database te connecteren zie hier\nHet probleem is op te lossen met één welgeplaatste regel: connection.rollback(). De vraag is echter: waar plaatsen we die? En ja, rollback() throwt ook de checked SQLException\u0026hellip; Verifieer of je oplossing werkt door de naam na de rollback terug op te halen en te vergelijken met de juiste waarde: \u0026ldquo;Jaak\u0026rdquo;.\nAangezien de API als zeer slim geworden is zal er zelfs een rollback plaatsvinden bij het opkomen van een SQLException zelfs als je zelf die rollback niet specifiek oproept in de catch-clausule. Daarom gaan we deze oefening samen even als demo bekijken.\nDe DROP TABLE IF EXISTS statements kan je in je project in een aparte SQL file bewaren en als een String inlezen, om in één keer te laten uitvoeren na het openen van de connectie:\nprivate void initTables() throws Exception { URI path = Objects.requireNonNull(App.class.getClassLoader().getResource(\u0026#34;create_db.sql\u0026#34;)).toURI(); var create_db_sql = new String(Files.readAllBytes(Paths.get(path))); System.out.println(create_db_sql); var s = connection.createStatement(); s.executeUpdate(create_db_sql); s.close(); } De verwachte fout (met de ongeldige SQL regel) die SQLite doorgeeft aan Java genereert de volgende stacktrace:\norg.sqlite.SQLiteException: [SQLITE_ERROR] SQL error or missing database (near \u0026#34;;\u0026#34;: syntax error)\rat org.sqlite.core.DB.newSQLException(DB.java:1010)\rat org.sqlite.core.DB.newSQLException(DB.java:1022)\rat org.sqlite.core.DB.throwex(DB.java:987)\rat org.sqlite.core.NativeDB._exec_utf8(Native Method)\rat org.sqlite.core.NativeDB._exec(NativeDB.java:94)\rat org.sqlite.jdbc3.JDBC3Statement.executeUpdate(JDBC3Statement.java:109)\rat SQLiteTransactionTest.doStuff(SQLiteTransactionTest.java:54)\rat SQLiteMain.main(SQLiteMain.java:7) Denkvragen De SQLite website beschrijft in detail hoe ze omgaan met \u0026ldquo;atomic commits\u0026rdquo; om aan de ACID regels te voldoen. Lees dit na op https://sqlite.org/atomiccommit.html Op welke manier gebruiken zij een rollback journal? Hoe is dat gelinkt aan de logfile van 14.2.3 op p.435? "
},
{
	"uri": "http://localhost:1313/db-course/apis/jpa/",
	"title": "JPA en Hibernate",
	"tags": [],
	"description": "",
	"content": "Wat is JPA? JPA of de Jakarta Persistence API (vroeger de Java Persistence API genoemd) is een deel van Java EE (Java Enterprise Platform), een set van specificaties die initieel de JDK SE 8 versie uitbreidden met \u0026ldquo;enterprise\u0026rdquo; features zoals distributed computing en web services. J2EE wordt vooral ingezet als het gaat over grote applicaties die bedrijven ontwikkelen voor andere bedrijven (zogenaamde \u0026ldquo;B2B\u0026rdquo;, Business 2 Business, of Enterprise Software Development).\nOndertussen is J2EE omgevormd tot Jakarta EE. Dat betekent ook dat JPA recent officieel werd vervangen door de Jakarta Persistence API. Je zal merken dat de javax.persistence dependency die wij gebruiken niet meer wordt geupdate. Pas dus op met recente Stack Overflow links!\nMaar wat is de JPA API nu precies? De API, levend in de package javax.persistence, is een manier om relationele data voor te stellen in enterprise Java applicaties, door middel van Objecten. Het is dus een mapping tool die object/relationale (meta)data bewerkt en verwerkt. JPA heeft ook zijn eigen query language, JPQL, die het eenvoudiger moet maken om queries te schrijven in Java code zelf in plaats van in SQL, die vertaald worden naar SQL. Dit vereenvoudigt refactoring en vermindert mogelijke fouten in de SQL string die te laat naar boven komen (nu compiletime ipv runtime, aangezien Java statisch getypeerd is).\nJPA is niet meer dan een specificatie die de interfaces en annotaties voorziet. De implementatie, en de klasses, worden overgelaten aan vendors, waarvan voor JPA 2.2 de volgende vendors beschikbaar zijn:\nDataNucleus EclipseLink Hibernate OpenJPA JPA 2.2 Gradle dependency: implementation 'javax.persistence:javax.persistence-api:2.2'\nWat is Hibernate ORM (Object Relational Mapper)? Volgens de hibernate.org website:\nYour relational data. Objectively.\nKort door de bocht uitgelegd is het een manier om Java Klassen te annoteren, zodat objecten automatisch in een database kunnen opgeslagen, opgehaald, geupdated of gedeleted kunnen worden zonder zelf nog queries te moeten schrijven. Je blijft dus eigenlijk steeds in Java land werken en moet je na een initiële configuratie niets meer aantrekken van hoe de data in de database zit of opgehaald wordt. Voor degene die het vak Full Stack Web Development opnemen, is dit al een tweede ORM die ze tegenkomen. In PHP/Laravel hebben hebben we met de PHP ORM gewerkt.\nHibernate is dé populairste object-relational mapper in Java die de JPA standaard implementeert. Hibernate heeft zowel een eigen API als een JPA specificatie, en kan dus overal waar JPA nodig is ingeschakeld worden. Het wordt vaak in omgevingen gebruikt waar performantie belangrijk is, en waar enorm veel data en gebruikers data transfereren.\nBelangrijk startpunt: Hibernate getting started guide Ook Hibernate werkt met modules, zoals Jdbi3. We gebruiken hibernate-core; via Gradle: compile group: 'org.hibernate', name: 'hibernate-core', version: '5.4.23.Final'\nHet gebruik van Hibernate geeft meestal een aantal mogelijkheden:\nGebruik de Native Hibernate API en hbm.xml mapping (Zie \u0026ldquo;2. Tutorial Using Native Hibernate APIs and hbm.xml Mapping\u0026rdquo;) Gebruik de Native Hibernate API en annotaties (Zie \u0026ldquo;3. Tutorial Using Native Hibernate APIs and Annotation Mappings\u0026rdquo;) Gebruik de JPA interface (Zie \u0026ldquo;4. Tutorial Using the Java Persistence API (JPA)\u0026rdquo;) Waarvan wij #3 gaan hanteren!!!\nHibernate/JPA Bootstrapping JPA bootstrappen kan - net zoals JDBC en JDBI - vrij eenvoudig met een statische klasse Persistence die een sessionFactory object aanmaakt. Elke session factory stelt een verbinding voor tussen de Java code en de Database zelf. Om te kunnen werken met objecten moet je vanuit de session factory de entity manager creëren. Vanaf dan kan er worden gewerkt met de database via de entity manager instantie.\nvar sessionFactory = Persistence.createEntityManagerFactory(\u0026#34;be.kuleuven.studenthibernate\u0026#34;); var entityManager = sessionFactory.createEntityManager(); // do stuff with it! // entityManager.createQuery(\u0026#34;SELECT s FROM Student s\u0026#34;, Student.class); // entityManager.getTransaction(); // CREATE // entityManager.persist(student); // READ // entityManager.find(Student.class, studnr); // UPDATE // entityManager.merge(student); // DELETE // entityManager.remove(student); javax.persistence.Persistence gaat op zoek naar een persistence.xml bestand in de map src/main/resources/META-INF. Die bevat alle connectiegegevens en instellingen. De persistence XML file is de belangrijkste file van je hele applicatie, waar caching strategie, driver management, table autocreation, \u0026hellip; allemaal in wordt bepaald!\nEen voorbeeld XML file voor onze Studenten demo\u0026rsquo;s:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;persistence version=\u0026#34;2.1\u0026#34; xmlns=\u0026#34;http://xmlns.jcp.org/xml/ns/persistence\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://xmlns.jcp.org/xml/ns/persistence http://xmlns.jcp.org/xml/ns/persistence/persistence_2_1.xsd\u0026#34;\u0026gt; \u0026lt;persistence-unit name=\u0026#34;be.kuleuven.studenthibernate\u0026#34;\u0026gt; \u0026lt;description\u0026gt;Studenten JPA Test\u0026lt;/description\u0026gt; \u0026lt;provider\u0026gt;org.hibernate.jpa.HibernatePersistenceProvider\u0026lt;/provider\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;!-- MySQL driver --\u0026gt; \u0026lt;property name=\u0026#34;javax.persistence.jdbc.driver\u0026#34; value=\u0026#34;com.mysql.jdbc.Driver\u0026#34;/\u0026gt; \u0026lt;!-- Pas de URL, user en password aan naar jouw XAMPP configuratie --\u0026gt; \u0026lt;property name=\u0026#34;javax.persistence.jdbc.url\u0026#34; value=\u0026#34;jdbc:mysql://localhost:3306/school\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;javax.persistence.jdbc.user\u0026#34; value=\u0026#34;root\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;javax.persistence.jdbc.password\u0026#34; value=\u0026#34;\u0026#34;/\u0026gt; \u0026lt;!-- Schema generatie: drop en create bij elke run --\u0026gt; \u0026lt;!-- UNCOMMENT HIERONDER ALS JE MET import.sql WIL WERKEN --\u0026gt; \u0026lt;!-- \u0026lt;property name=\u0026#34;javax.persistence.schema-generation.database.action\u0026#34; value=\u0026#34;drop-and-create\u0026#34;/\u0026gt; --\u0026gt; \u0026lt;!-- Hibernate specifieke properties --\u0026gt; \u0026lt;property name=\u0026#34;hibernate.dialect\u0026#34; value=\u0026#34;org.hibernate.dialect.MariaDBDialect\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;hibernate.connection.autocommit\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;hibernate.show_sql\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;hibernate.flushMode\u0026#34; value=\u0026#34;ALWAYS\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;hibernate.cache.use_second_level_cache\u0026#34; value=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/persistence-unit\u0026gt; \u0026lt;/persistence\u0026gt; Bevat onder andere de volgende belangrijke properties:\njavax.persistence JDBC driver/url. Merk op dat achterliggend dus nog steeds JDBC wordt gebruikt! Dat betekent ook dat we de MYSQL dependency 'mysql:mysql-connector-java:5.1.6' nog steeds nodig hebben. schema-generation properties: drop-and-create betekent dat tabellen die niet bestaan automatisch worden aangemaakt. Geen CREATE TABLE statements meer nodig, dus. Kan je ook weglaten hibernate.dialect: voor vendor-specifieke queries te genereren moet Hibernate weten welke database wij hanteren. Dit staat los van de jdbc driver! Hiervoor gebruiken we het dialect van MYSQL \u0026quot;org.hibernate.dialect.MariaDBDialect\u0026quot;. Flush modes, auto-commit instellingen, caching, e.a. Dit gaat ver buiten de scope van deze cursus. show_sql print de gegenereerde queries af in de console, handig om te zien hoe Hibernate intern werkt, en om te debuggen. Er ontbreekt hierboven nog een belangrijk gegeven: elke entity (domein object dat een tabel voorstelt in de code) moet met fully qualified name in een \u0026lt;class/\u0026gt; tag onder \u0026lt;persistence-unit/\u0026gt; worden toegevoegd. Anders herkent JPA het object niet, en heeft hij geen idee welke kolommen te mappen op welke properties. Die metadata zit namelijk in de entity klasse zelf. Je moet die klassen vlak boven de \u0026lt;properties\u0026gt;-tag toevoegen:\n... \u0026lt;provider\u0026gt;org.hibernate.jpa.HibernatePersistenceProvider\u0026lt;/provider\u0026gt; \u0026lt;!-- HIER KLASSEN TOEVOEGEN --\u0026gt; \u0026lt;class\u0026gt;be.kuleuven.Student\u0026lt;/class\u0026gt; \u0026lt;class\u0026gt;be.kuleuven.Vak\u0026lt;/class\u0026gt; \u0026lt;class\u0026gt;be.kuleuven.Opleiding\u0026lt;/class\u0026gt; \u0026lt;properties\u0026gt; ... Meer informatie: zie hibernate.org documentatie en A beginners guide to JPA persistence xml.\nHibernate/JPA Peristence/querying Nu de verbinding tussen de DB en Hibernate/JPA tot stand werd gebracht, is het tijd om gebruik te maken van de kracht van de library.\nOm kolommen te kunnen mappen op properties voorziet JPA een aantal annotaties als meta-data op het domeinobject zelf. Dat betekent dat DB beschrijving netjes bij het object waar het hoort wordt geplaatst. Bijvoorbeeld:\n@Entity @Table(name = \u0026#34;student\u0026#34;) public class Student { @Id private int studnr; @Column(name = \u0026#34;voornaam\u0026#34;) private String voornaam; @Column(name = \u0026#34;naam\u0026#34;) private String naam; @Column(name = \u0026#34;goedbezig\u0026#34;) private boolean goedBezig; } Het datatype kan ook worden ingesteld met @Column (merk op dat de kolomnaam van de tabel in de DB kan en mag wijzigen van de property name in Java), bijvoorbeeld voor temporele waardes waar enkel de tijd of datum wordt bijgehouden op DB niveau. Merk op dat @Id nodig is op een @Entity - zonder primary key kan JPA geen object persisteren. @GeneratedValue bestaat wanneer wij niet telkens de ID willen verhogen, maar dat willen overlaten aan de database vanwege de AUTOINCREMENT. Bij elke persist() gaat Hibernate de juiste volgende ID ophalen, dat zich vertaalt in de volgende queries in sysout:\nHibernate: select next_val as id_val from hibernate_sequence Hibernate: update hibernate_sequence set next_val= ? where next_val=? Hibernate: insert into Student (goedBezig, naam, voornaam, studnr) values (?, ?, ?, ?) De tabelnaam kan je wijzigen met de @Table annotatie op klasse niveau.\nInserts/updates Hoe bewaar ik een entity? entityManager.persist(object). That\u0026rsquo;s it!\nHoe update ik een entity, als properties zijn gewijzigd? .merge(object)\nMerk op dat in de Sysout output geen query wordt gegenereerd. Hibernate houdt alles in zijn interne cache bij, en zal pas flushen naar de database wanneer hij acht dat dat nodig is. Dat kan je zelf triggeren door entityManager.flush() te gebruiken (kan alleen in een transactie) - of het commando te wrappen met een transactie:\nentityManager.getTransaction().begin(); entityManager.persist(dingdong); entityManager.getTransaction().commit(); Zonder dit, en met herbruik van dezelfde entity manager in SQLite, is er een kans dat je SELECT query niets teruggeeft, omdat de INSERT nog niet werd geflushed. De interne werking van combinatie JDBC+SQLite+JPA+Hibernate is zeer complex en zou een cursus van 20 studiepunten vereisen\u0026hellip;\nQueries Hoe query ik in JPA? Dit kan op verschillende manieren. We beperken ons hier tot de JPA Criteria API. Een voorbeeld. Gegeven een studentenlijst, waarvan we studenten willen teruggeven die als studnr \u0026lt; 200. In SQL zou dit SELECT * FROM student WHERE studnr \u0026lt; 200 zijn. In Criteria API:\nvar criteriaBuilder = entityManager.getCriteriaBuilder(); var query = criteriaBuilder.createQuery(Student.class); var root = query.from(Student.class); query.where(criteriaBuilder.lt(root.get(\u0026#34;studnr\u0026#34;), 200)); return entityManager.createQuery(query).getResultList(); Voor simpele queries zoals deze is dat inderdaad omslachtig, maar de API is zeer krachtig en kan automatisch complexe queries genereren zonder dat wij ons moe moeten maken. Merk op dat wij geen enkele letter SQL zelf schrijven. Alles is java code, wat het eenvoudig maakt om te refactoren, redesignen, statische code analyse op te doen, unit testen, \u0026hellip; Lees meer over criteria API:\nVoorbeeld 2 Programmatic Criteria Queries Voorbeeld 3 TutorialsPoint Criteria API Voorbeeld 4 Using \u0026ldquo;In\u0026rdquo; in Criteria API Controleer in de sysout output welke query Hibernate uiteindelijk genereert. Dat ziet er zo uit bijvoorbeeld:\nselect student0_.studnr as studnr1_0_, student0_.goedBezig as goedbezi2_0_, student0_.naam as naam3_0_, student0_.voornaam as voornaam4_0_ from Student student0_ where student0_.naam=? Jdbc met SQLite: ideaal voor testing Willen we JPA met SQLite gebruiken moeten we natuurlijk onze persistence.xml aanpassen, maar ook nog een dependency toevoegen voor het juiste hibernate.dialect. Om vendor-specifieke queries te genereren moet Hibernate weten welke database wij hanteren. Dit staat los van de jdbc driver! Hiervoor gebruiken we voor SQLite dialect van dependency implementation \u0026quot;com.github.gwenn:sqlite-dialect:0.1.1\u0026quot;.\nDit komt dan overeen met volgende persistence.xml-file:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;persistence version=\u0026#34;2.1\u0026#34; xmlns=\u0026#34;http://xmlns.jcp.org/xml/ns/persistence\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://xmlns.jcp.org/xml/ns/persistence http://xmlns.jcp.org/xml/ns/persistence/persistence_2_1.xsd\u0026#34;\u0026gt; \u0026lt;persistence-unit name=\u0026#34;be.kuleuven.studenthibernate\u0026#34;\u0026gt; \u0026lt;description\u0026gt;Studenten JPA Test\u0026lt;/description\u0026gt; \u0026lt;provider\u0026gt;org.hibernate.jpa.HibernatePersistenceProvider\u0026lt;/provider\u0026gt; \u0026lt;class\u0026gt;be.kuleuven.Student\u0026lt;/class\u0026gt; \u0026lt;class\u0026gt;be.kuleuven.Vak\u0026lt;/class\u0026gt; \u0026lt;class\u0026gt;be.kuleuven.Opleiding\u0026lt;/class\u0026gt; \u0026lt;exclude-unlisted-classes\u0026gt;false\u0026lt;/exclude-unlisted-classes\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;!-- SQLite driver --\u0026gt; \u0026lt;property name=\u0026#34;javax.persistence.jdbc.driver\u0026#34; value=\u0026#34;org.sqlite.JDBC\u0026#34;/\u0026gt; \u0026lt;!-- SQLite database file path --\u0026gt; \u0026lt;property name=\u0026#34;javax.persistence.jdbc.url\u0026#34; value=\u0026#34;jdbc:sqlite:mydatabase.db\u0026#34;/\u0026gt; \u0026lt;!-- SQLite doesn\u0026#39;t require username/password --\u0026gt; \u0026lt;!-- Schema generatie: drop en create bij elke run --\u0026gt; \u0026lt;!-- UNCOMMENT HIERONDER ALS JE MET import.sql WIL WERKEN --\u0026gt; \u0026lt;!-- \u0026lt;property name=\u0026#34;javax.persistence.schema-generation.database.action\u0026#34; value=\u0026#34;drop-and-create\u0026#34;/\u0026gt; --\u0026gt; \u0026lt;!-- Hibernate specifieke properties --\u0026gt; \u0026lt;!-- \u0026lt;property name=\u0026#34;hibernate.dialect\u0026#34; value=\u0026#34;org.hibernate.community.dialect.SQLiteDialect\u0026#34;/\u0026gt; --\u0026gt; \u0026lt;property name=\u0026#34;hibernate.dialect\u0026#34; value=\u0026#34;org.sqlite.hibernate.dialect.SQLiteDialect\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;hibernate.connection.autocommit\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;hibernate.show_sql\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;hibernate.flushMode\u0026#34; value=\u0026#34;ALWAYS\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;hibernate.cache.use_second_level_cache\u0026#34; value=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;!-- Needed for SQLite foreign key support --\u0026gt; \u0026lt;property name=\u0026#34;hibernate.connection.foreign_keys\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/persistence-unit\u0026gt; \u0026lt;/persistence\u0026gt; Waar de grootste verschillen liggen bij de:\npersistence 'driver', 'url' ('user' en 'password'). hibernate dialect en SQLite heeft voor \u0026lsquo;foreign key\u0026rsquo; support nog volgende property extra nodig: \u0026lt;property name=\u0026quot;hibernate.connection.foreign_keys\u0026quot; value=\u0026quot;true\u0026quot;/\u0026gt; OPMERKING: wil je in je main een configuratie gebruiken voor JPA en in de test een andere, dan moet je ook verschillende persistence-unit names voorzien bv: \u0026lt;persistence-unit name=\u0026quot;be.kuleuven.studenthibernateTest\u0026quot;\u0026gt;\nDEMO: JPA met relations Het is het simpelste om die in een applicatie even te bekijken. Het belangrijkste zijn de juiste annotaties bij de verschillende klassen. Daarna zou alles automatisch moeten verlopen. Dat is toch wel handig.\nHier vind je een zipfolder met een oplossing voor JPA demo Student met relations to Vak en Opleiding\nOPDRACHT Werk verder aan de verplichte opdracht waar je nu ook een JPA repository voor voorziet met een correct werkende persistance.xml en met de juiste dependencies.\n"
},
{
	"uri": "http://localhost:1313/db-course/nosql/opdracht/",
	"title": "VERPLICHTE opdracht",
	"tags": [],
	"description": "",
	"content": "Opdracht MongoDb met als deadline vrijdag 9 mei 2025 23u59 Voor de verplichte opdracht meld je je aan onder de correcte naam bij volgende Github Classroom. En pull je de repository van Opdracht rond MongoDb. Deze repository bevat een Java Gradle project met een aantal TODO\u0026rsquo;s die je moet oplossen. Hieronder staat de opdracht nog beschreven:\nOpdracht: Je krijgt alweer een startproject met de Speler en Club klasse gegeven, je krijgt ook al de start van de SpelerRepository klasse waarvan je weer zelf de implementaties van de gevraagde methoden moet programmeren volgens de TODOs zodat alle testen slagen. Alle dependencies en imports zijn al ingevoegd en correct, het kan echter zijn dat voor jouw manier een extra import nodig is. Je moet er ook wel voor zorgen dat je op je lokale MongoDb server al een database hebt met de naam \u0026ldquo;Tennisvlaanderen\u0026rdquo; en een collectie \u0026ldquo;spelers\u0026rdquo;. Dit kan je eenvoudig doen via de MongoDb Compass GUI zoals we in de les gezien hebben.\nMaak voor de twee laatste methoden getFilteredSpelers en getAveragePointsOfAllPlayers gebruik van aggregaties.\nEen speler ziet er als volgt uit (en je mag ervan uitgaan dat elk json object altijd de nodige properties heeft om het om te zetten naar een Java Speler object):\n{ \u0026#34;tennisvlaanderenId\u0026#34;: 1, \u0026#34;naam\u0026#34;: \u0026#34;Tom Janssens\u0026#34;, \u0026#34;punten\u0026#34;: 10, \u0026#34;prof\u0026#34;: true, \u0026#34;club\u0026#34;: { \u0026#34;naam\u0026#34;: \u0026#34;T.P.Tessenderlo\u0026#34;, \u0026#34;gemeente\u0026#34;: \u0026#34;Tessenderlo-Ham\u0026#34; }, \u0026#34;likedTennisvlaanderenIds\u0026#34;: [ 2, 3 ] } Push voor de deadline van vrijdag 9 mei 2025 23u59 je oplossingen naar je repository\nVeel succes! Je mag me altijd contacteren via arne.duyver@kuleuven.be voor vragen of in de les aanspreken.\n"
},
{
	"uri": "http://localhost:1313/db-course/apis/repository_interface/",
	"title": "Interface",
	"tags": [],
	"description": "",
	"content": "Wat is een interface? Een Java interface is een referentietype dat een contract definieert voor klassen door een set methodsignatures (en optioneel constante waarden) vast te leggen, zonder daarbij een concrete implementatie te leveren. Klassen die deze interface implementeren, moeten alle gedeclareerde methoden voorzien van een concrete implementatie, waardoor consistent gedrag wordt gegarandeerd en polymorfisme mogelijk wordt. Interfaces maken het ook mogelijk om meerdere gedragingen te combineren, aangezien een klasse meerdere interfaces kan implementeren.\nWat betekend dit voor onze StudentRepository We hebben nu verschillende methoden gezien waarmee we zo een StudentRepository kunnen implementeren. Het is dan een goed idee om een Interface voor StudentRepository te voorzien zodat we de specifieke implementaties (JDBC, JDBI, JPA) die interface dan kunnen laten implementeren zodat je later in je main programma gewoon kan kiezen met welke methode je wil werken, want elke methode heeft wel zijn voor en nadelen.\nOnze StudentRepository interface ziet er dan als volgt uit:\npublic interface StudentRepository { public void addStudentToDb(Student student); public Student getStudentsByStudnr(int stud_nr); public List\u0026lt;Student\u0026gt; getAllStudents(); public void updateStudentInDb(Student student); public void deleteStudentInDb(int studnr); } En onze JDBC implementatie kan er dan als volgt uitzien: (waarbij de StudentRepositoryJDBCimpl-klasse de interface implementeert)\npublic class StudentRepositoryJDBCimpl implements StudentRepository { ... } In de main van je programma kan je dus overal waar je een StudentRepository nodig hebt een van de implementatie klassen gebruiken! Bijvoorbeeld: StudentRepository sr = new StudentRepositoryJDBCimpl(...);\nOefening Hier vind je een zipfolder met een oplossing voor onderstaande oefening: dashboard voor database met enkel studenten tabel\n"
},
{
	"uri": "http://localhost:1313/db-course/nosql/",
	"title": "NoSQL",
	"tags": [],
	"description": "",
	"content": "NoSQL Databases Zie menu links.\n"
},
{
	"uri": "http://localhost:1313/db-course/timeseries/",
	"title": "Timeseries database",
	"tags": [],
	"description": "",
	"content": "Timeseries Databases Zie menu links.\n"
},
{
	"uri": "http://localhost:1313/db-course/apis/opdracht/",
	"title": "VERPLICHTE opdracht",
	"tags": [],
	"description": "",
	"content": "Opdracht JDBC en JDBI met als deadline vrijdag 2 mei 2025 23u59 Voor de verplichte opdracht meld je je aan onder de correcte naam bij volgende Github Classroom. En pull je de repository van Opdracht rond Database API\u0026rsquo;s. Deze repository bevat een Java Gradle project met een aantal TODO\u0026rsquo;s die je moet oplossen. Hieronder staat de opdracht nog beschreven:\nOpdracht: tennisspelers, tornooien en wedstrijden Voor de verplichte opdrachten rond Database API\u0026rsquo;s in Java (met Gradle) gaan jullie een aantal TODO\u0026rsquo;s in dit project moeten oplossen. De probleemstelling is een zeer beperkte versie van de casus. De beperkte versie werkt als volgt:\nJe hebt enkel Spelers, Tornooien en Wedstrijden Een Speler heeft een unieke tennisvlaanderenId, een naam en een aantal punten. Een Tornooi heeft enkel een id en een naam van de tennisclub die dat tornooi organiseert. Een Wedstrijd bevat 2 spelers, 1 winnaar, de tornooiId, een score string en een finale nummer De finale nummer geeft aan in de hoeveelste ronde de wedstrijd gespeeld werd: 1 = finale, 2 = halve finale, 4 = kwart finale \u0026hellip; Je kan je als Speler ook inschrijven voor een Tornooi Het EER-schema van het project zie je hieronder:\nIn het project zijn alle klassen die je nodig hebt al aangemaakt op de correcte manier. Het enige wat je nog moet doen is de TODO\u0026rsquo;s oplossen bij de SpelerRespositoryJDBCimpl-klasse en de SpelerRespositoryJDBIimpl-klasse. Waarbij je natuurlijk de juiste technologieën toepast. Er is geen App.java code, je kan jezelf wel testen door de Testen die toegevoegd zijn te runnen. Je mag extra testen schrijven als die nodig is. (De testen werken met een in memory database met SQLite, maar dit zou geen probleem mogen geven bij jullie implementatie)\nBELANGRIJK wanneer je SQL specifieke imports moet doen, gebruik dan de generieke java.sql imports en geen com.mysql imports aangezien de testen dan niet zullen werken !!!\nPush voor de deadline van vrijdag 2 mei 2025 23u59 je oplossingen naar je repository\nVeel succes! Je mag me altijd contacteren via arne.duyver@kuleuven.be voor vragen of in de les aanspreken.\nEr is ook een video met deze uitleg voorzien die je op Toledo bij de opdracht kan terugvinden.\nOpdracht JPA met als deadline vrijdag 2 mei 2025 23u59 Vul het bovenstaande project verder aan met een werkende SpelerRespositoryJPAimpl-klasse en bijhorende dependencies en persistence.xml-file op de correcte manier. De testen bijhorend bij dit deel volgen ASAP.\nDenk eraan dat je in de persistence.xml die bij de testen hoort ook de juiste DATABASE url gebruikt\nBELANGRIJKE VERBETERINGEN en EXTRA TIPS Tijdens vorige les kwamen er nog een aantal fouten/moeilijkheden naar boven. Daarom kan je hieronder de verbeterde files terugvinden die je kan/MOET verwisselen met je eigen klassen. Per klasse staat er ook bij wat juist aangepast is.\nAangezien de opdrachtenlast samen met de andere vakken wat druk is (en aangezien de decoratoren iets complexer zijn dan wat we in de les gezien hebben), krijg je voor het deel van JPA al de decorators gegeven voor de volledige Tornooi en Wedstrijd klassen. Ook in de Speler klasse werden de decorators die je nodig hebt voor de relaties al toegevoegd. Je moet in Speler dus enkel nog de basis decoratoren voor JPA toevoegen.\nVERBETERDE KLASSEN (rechterklik op de naam en kies \u0026lsquo;save link as \u0026hellip;\u0026rsquo; om de file te downloaden):\nbuild.gradle: Hier werden de correcte dependecies voor JPA toegevoegd. Voor de main folder: Speler: Hier zijn decorators toegevoegd voor de relaties in JPA, de setTennisvlaanderenId-methode werd toegevoegd. ArrayLists werden naar Lists omgevormd in de type vermelding van de datamembers om compatibel te zijn met JPA/Hibernate. De spelfout in getTennisvlaanderenid is verbeterd naar getTennisvlaanderenId (Let op dit kan in je eigen code errors geven die je dan simpel kan oplossen) Wedstrijd: Hier zijn decorators toegevoegd voor JPA, de setId-methode werd toegevoegd. Tornooi: Hier zijn decorators toegevoegd voor JPA, en werd een naamconventie correct van snakecase naar camelcase omgevormd. De setId-methode werd toegevoegd. ArrayLists werden naar Lists omgevormd in de type vermelding van de datamembers om compatibel te zijn met JPA/Hibernate. SpelerRepository: Hier werden de parameters van de methoden addSpelerToTornooi en removeSpelerFromTornooi correct aangepast zodat je ook een tennisvlaanderenId meegeeft. NIEUW: SpelerRepositoryJPAimpl: Template file voor JPA implementatie, met al oplossing voor addSpelerToTornooi en removeSpelerFromTornooi. (want anders dan in de les gezien) initTableWithDummyData.sql: NOT NULL werd weggehaald bij Wedstrijd voor speler1 en speler2 dit overcompliceerde de JPA implementatie LET OP: deze file heeft dezelfde naam als de SQL file in de test folder maar is niet dezelfde!!! Voor de test folder:\nSpelerRepositoryTest: De test whenGetAllSpelers_assertThat8correctSpelersPresent werd aangepast om compatibel te zijn met JDBI en JPA. De database url werd aangepast van \u0026ldquo;jdbc:sqlite::memory:\u0026rdquo; naar \u0026ldquo;jdbc:sqlite:testdatabase.db\u0026rdquo; om compatibel te zijn met JDBI en JPA (Let erbij op dat je dezelfde url gebruikt in je persistence.xml-file). De laatste 2 testen voor addSpelerToTornooi en removeSpelerFromTornooi zijn nu niet meer leeg. NIEUW: SpelerRepositoryJPAimplTest: Testfile voor JPA, toe te voegen in dezelfde folder als de andere testen. initTableWithDummyData.sql: NOT NULL werd weggehaald bij Wedstrijd voor speler1 en speler2 dit overcompliceerde de JPA implementatie LET OP: deze file heeft dezelfde naam als de SQL file in de main folder maar is niet dezelfde!!! Vergeet niet dat je zelf nog wel de juiste persistence.xml-files moet toevoegen!\n"
},
{
	"uri": "http://localhost:1313/db-course/xml/",
	"title": "XML Data Storage",
	"tags": [],
	"description": "",
	"content": "XML Zie menu links.\n"
},
{
	"uri": "http://localhost:1313/db-course/bigdata/",
	"title": "Big Data &amp; Analytics",
	"tags": [],
	"description": "",
	"content": "Big Data \u0026amp; Analytics Zie menu links.\n"
},
{
	"uri": "http://localhost:1313/db-course/extra/",
	"title": "Extras",
	"tags": [],
	"description": "",
	"content": "Extra informatie Zie menu links.\n"
},
{
	"uri": "http://localhost:1313/db-course/nosql/4oef3/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "\rPrint docs\r...\r"
},
{
	"uri": "http://localhost:1313/db-course/",
	"title": "Index",
	"tags": [],
	"description": "",
	"content": " Databases Syllabus Lesgevers: Coördinerend Verantwoordelijke: prof. dr. Kris Aerts (kris.aerts@kuleuven.be) assistent lesgever: ing. Arne Duyver (arne.duyver@kuleuven.be) Kantoor: Technologiecentrum Diepenbeek, Groep ACRO. Cursusbeschrijving Dit opleidingsonderdeel focust enerzijds op drie soorten databases:\nrelationele databases de NoSQL-alternatieven XML databases En anderzijds op twee toepassingen:\nprogrammeren van database-gestuurde applicaties via API\u0026rsquo;s een inleiding in Big Data Vereiste voorkennis Basiskennis van een Object-Geörienteerde programmeertaal als Java of C# Basiskennis van het UNIX systeem, werken met commandline Doelstellingen Zie ook Studiegids UHasselt\nDe context en het overzicht worden aangereikt in de eerste lessen van dit vak.\nAls practicum wordt een grotere probleemstelling als project uitgewerkt. Alle aan te leren aspecten van databases komen in dit project aan bod. Studenten kunnen facultatief buiten het practicum extra thematische oefeningen oplossen.\nKalender Zie Mytimetable UHasselt.\n"
},
{
	"uri": "http://localhost:1313/db-course/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]